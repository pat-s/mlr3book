[
["index.html", "Quickstart", " Quickstart As a 30-second introductory example, we will train a decision tree model on the first 120 rows of iris data set and make predictions on the final 30, measuring the accuracy of the trained model. library(mlr3) task = tsk(&quot;iris&quot;) learner = lrn(&quot;classif.rpart&quot;) # train a model of this learner for a subset of the task learner$train(task, row_ids = 1:120) # this is what the decision tree looks like learner$model ## n= 120 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 120 70 setosa (0.41667 0.41667 0.16667) ## 2) Petal.Length&lt; 2.45 50 0 setosa (1.00000 0.00000 0.00000) * ## 3) Petal.Length&gt;=2.45 70 20 versicolor (0.00000 0.71429 0.28571) ## 6) Petal.Length&lt; 4.95 49 1 versicolor (0.00000 0.97959 0.02041) * ## 7) Petal.Length&gt;=4.95 21 2 virginica (0.00000 0.09524 0.90476) * predictions = learner$predict(task, row_ids = 121:150) predictions ## &lt;PredictionClassif&gt; for 30 observations: ## row_id truth response ## 121 virginica virginica ## 122 virginica versicolor ## 123 virginica virginica ## --- ## 148 virginica virginica ## 149 virginica virginica ## 150 virginica virginica # accuracy of our model on the test set of the final 30 rows predictions$score(msr(&quot;classif.acc&quot;)) ## classif.acc ## 0.8333 More examples can be found in the mlr3gallery, a collection of use cases and examples. "],
["introduction-and-overview.html", "1 Introduction and Overview", " 1 Introduction and Overview The mlr3 (Lang et al. 2019) package and ecosystem provide a generic, object-oriented, and extensible framework for classification, regression, survival analysis, and other machine learning tasks for the R language (R Core Team 2019). We do not implement any learners ourselves, but provide a unified interface to many existing learners in R. This unified interface provides functionality to extend and combine existing learners, intelligently select and tune the most appropriate technique for a task, and perform large-scale comparisons that enable meta-learning. Examples of this advanced functionality include hyperparameter tuning, feature selection, and ensemble construction. Parallelization of many operations is natively supported. Target Audience mlr3 provides a domain-specific language for machine learning in R. We target both practitioners who want to quickly apply machine learning algorithms and researchers who want to implement, benchmark, and compare their new methods in a structured environment. The package is a complete rewrite of an earlier version of mlr that leverages many years of experience to provide a state-of-the-art system that is easy to use and extend. It is intended for users who have basic knowledge of machine learning and R and who are interested in complex projects that use advanced functionality as well as one-liners to quickly prototype specific tasks. Why a Rewrite? mlr (Bischl et al. 2016) was first released to CRAN in 2013, with the core design and architecture dating back much further. Over time, the addition of many features has led to a considerably more complex design that made it harder to build, maintain, and extend than we had hoped for. With hindsight, we saw that some of the design and architecture choices in mlr made it difficult to support new features, in particular with respect to pipelines. Furthermore, the R ecosystem as well as helpful packages such as data.table have undergone major changes in the meantime. It would have been nearly impossible to integrate all of these changes into the original design of mlr. Instead, we decided to start working on a reimplementation in 2018, which resulted in the first release of mlr3 on CRAN in July 2019. The new design and the integration of further and newly developed R packages (R6, future, data.table) makes mlr3 much easier to use, maintain, and more efficient compared to mlr. Design Principles We follow these general design principles in the mlr3 package and ecosystem. Backend over frontend. Most packages of the mlr3 ecosystem focus on processing and transforming data, applying machine learning algorithms, and computing results. We do not provide graphical user interfaces (GUIs); visualizations of data and results are provided in extra packages. Embrace R6 for a clean, object-oriented design, object state-changes, and reference semantics. Embrace data.table for fast and convenient data frame computations. Unify container and result classes as much as possible and provide result data in data.tables. This considerably simplifies the API and allows easy selection and “split-apply-combine” (aggregation) operations. We combine data.table and R6 to place references to non-atomic and compound objects in tables and make heavy use of list columns. Defensive programming and type safety. All user input is checked with checkmate (Lang 2017). Return types are documented, and mechanisms popular in base R which “simplify” the result unpredictably (e.g., sapply() or the drop argument in [.data.frame) are avoided. Be light on dependencies. One of the main maintenance burdens for mlr was to keep up with changing learner interfaces and behavior of the many packages it depended on. We require far fewer packages in mlr3 to make installation and maintenance easier. mlr3 requires the following packages: backports: Ensures backward compatibility with older R releases. Developed by members of the mlr3 team. checkmate: Fast argument checks. Developed by members of the mlr3 team. mlr3misc: Miscellaneous functions used in multiple mlr3 extension packages. Developed by the mlr3 team. mlr3measures: Performance measures for classification and regression. Developed by members of the mlr3 team. paradox: Descriptions of parameters and parameter sets. Developed by the mlr3 team. R6: Reference class objects. data.table: Extension of R’s data.frame. digest: Hash digests. uuid: Unique string identifiers. lgr: Logging facility. mlbench: A collection of machine learning data sets. None of these packages adds any extra recursive dependencies to mlr3. mlr3 provides additional functionality through extra packages: For parallelization, mlr3 utilizes the future and future.apply packages. To capture output, warnings, and exceptions, evaluate and callr can be used. References "],
["basics.html", "2 Basics", " 2 Basics This chapter will teach you the essential building blocks, R6 classes, and operations of mlr3 for machine learning. A typical machine learning workflow looks like this: The data, which mlr3 encapsulates in tasks, is split into non-overlapping training and test sets. We are interested in models that generalize to new data rather than just memorizing the training data, and separate test data allows to objectively evaluate models with respect to that. The training data is given to a machine learning algorithm, which we call a learner in mlr3. The learner uses the training data to build a model of the relationship of the input features to the output target values. This model is then used to produce predictions on the test data, which are compared to the ground truth values to assess the quality of the model. mlr3 offers a number of different measures to quantify how well a model performs based on the difference between predicted and actual values. Usually this measure is a numeric score. The process of splitting up data into training and test sets, building a model, and evaluating it may be repeated several times, resampling different training and test sets from the original data each time. Multiple resampling iterations allow us to get a better, more generalizable performance estimate for a particular type of model as it is tested under different conditions and less likely to get lucky or unlucky because of a particular way the data was resampled. In many cases, this simple workflow is not sufficient to deal with real-world data, which may require normalization, imputation of missing values, or feature selection. We will cover more complex workflows that allow to do this and even more later in the book. This chapter covers the following subtopics: Tasks Tasks encapsulate the data with meta-information, such as the name of the prediction target column. We cover how to: access predefined tasks, specify a task type, create a task, work with a task’s API, assign roles to rows and columns of a task, implement task mutators, and retrieve the data that is stored in a task. Learners Learners encapsulate machine learning algorithms to train models and make predictions for a task. They are provided by R and other packages. We cover how to: access the set of classification and regression learners that come with mlr3 and retrieve a specific learner, access the set of hyperparameter values of a learner and modify them. How to modify and extend learners is covered in a supplemental advanced technical section. Train and predict The section on the train and predict methods illustrates how to use tasks and learners to train a model and make predictions on a new data set. In particular, we cover how to: set up tasks and learners properly, set up train and test splits for a task, train the learner on the training set to produce a model, generate predictions on the test set, and assess the performance of the model by comparing predicted and actual values. Resampling A resampling is a method to create training and test splits. We cover how to access and select resampling strategies, instantiate the split into training and test sets by applying the resampling, and execute the resampling to obtain results. Additional information on resampling can be found in the section about nested resampling and in the chapter on model optimization. Benchmarking Benchmarking is used to compare the performance of different models, for example models trained with different learners, on different tasks, or with different resampling methods. We cover how to create a benchmarking design, execute a design and aggregate results, and convert benchmarking objects to resample objects. Binary classification Binary classification is a special case of classification where the target variable to predict has only two possible values. In this case, additional considerations apply; in particular: ROC curves and the threshold where to predict one class versus the other, and threshold tuning (WIP). Before we get into the details of how to use mlr3 for machine learning, we give a brief introduction to R6 as it is a relatively new part of R. mlr3 heavily relies on R6 and all basic building blocks it provides are R6 classes: tasks, learners, measures, and resamplings. "],
["r6.html", "2.1 Quick R6 Intro for Beginners", " 2.1 Quick R6 Intro for Beginners R6 is one of R’s more recent dialects for object-oriented programming (OO). It addresses shortcomings of earlier OO implementations in R, such as S3, which we used in mlr. If you have done any object-oriented programming before, R6 should feel familiar. We focus on the parts of R6 that you need to know to use mlr3 here. Objects are created by calling the constructor of an R6Class() object, specifically the $new() method. For example, foo = Foo$new(bar = 1) creates a new object of class Foo, setting the bar argument of the constructor to 1. Classes have mutable state, which is encapsulated in their fields, which can be accessed through the dollar operator. We can access the bar value in the Foo class through foo$bar and set its value by assigning the field, e.g. foo$bar = 2. In addition to fields, objects expose methods that may allow to inspect the object’s state, retrieve information, or perform an action that may change the internal state of the object. For example, the $train method of a learner changes the internal state of the learner by building and storing a trained model, which can then be used to make predictions given data. Objects can have public and private fields and methods. In mlr3, you can only access the public variables and methods. Private fields and methods are only relevant to change or extend mlr3. R6 variables are references to objects rather then the actual objects, which are stored in an environment. For example foo2 = foo does not create a copy of foo in foo2, but another reference to the same actual object. Setting foo$bar = 3 will also change foo2$bar to 3 and vice versa. To copy an object, use the $clone() method and the deep = TRUE argument for nested objects, for example foo2 = foo$clone(deep = TRUE). Here is an overview of the extension packages of mlr: For further information see wiki for short descriptions and links to the respective repositories. For more details on R6, have a look at the R6 vignettes. "],
["tasks.html", "2.2 Tasks", " 2.2 Tasks Tasks are objects that contain the data and additional meta-data for a machine learning problem. The meta-data is for example the name of the target variable (the prediction) for supervised machine learning problems, or the type of the dataset (e.g. a spatial or survival). This information is used for specific operations that can be performed on a task. 2.2.1 Task Types To create a task from a data.frame() or data.table() object, the task type needs to be specified: Classification Task: The target is a label (stored as character()orfactor()) with only few distinct values. → mlr3::TaskClassif Regression Task: The target is a numeric quantity (stored as integer() or double()). → mlr3::TaskRegr Survival Task: The target is the (right-censored) time to an event. → mlr3proba::TaskSurv in add-on package mlr3proba Ordinal Regression Task: The target is ordinal. → TaskOrdinal in add-on package mlr3ordinal Cluster Task: An unsupervised task type; there is no target and the aim is to identify similar groups within the feature space. → Not yet implemented Spatial Task: Observations in the task have spatio-temporal information (e.g. coordinates). → Not yet implemented, but started in add-on package mlr3spatiotemporal 2.2.2 Task Creation As an example, we will create a regression task using the mtcars data set from the package datasets and predict the target &quot;mpg&quot; (miles per gallon). We only consider the first two features in the dataset for brevity. First, we load and prepare the data. data(&quot;mtcars&quot;, package = &quot;datasets&quot;) data = mtcars[, 1:3] str(data) ## &#39;data.frame&#39;: 32 obs. of 3 variables: ## $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... ## $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... ## $ disp: num 160 160 108 258 360 ... Next, we create the task using the constructor for a regression task object (TaskRegr$new) and give the following information: id: An arbitrary identifier for the task, used in plots and summaries. backend: This parameter allows fine-grained control over how data is accessed. Here, we simply provide the dataset which is automatically converted to a DataBackendDataTable. Alternatively, we could also construct a DataBackend manually. target: The name of the target column for the regression problem. library(&quot;mlr3&quot;) task_mtcars = TaskRegr$new(id = &quot;cars&quot;, backend = data, target = &quot;mpg&quot;) print(task_mtcars) ## &lt;TaskRegr:cars&gt; (32 x 3) ## * Target: mpg ## * Properties: - ## * Features (2): ## - dbl (2): cyl, disp The print() method gives a short summary of the task: It has 32 observations and 3 columns, of which 2 are features. We can also plot the task using the mlr3viz package, which gives a graphical summary of its properties: library(&quot;mlr3viz&quot;) autoplot(task_mtcars, type = &quot;pairs&quot;) ## Registered S3 method overwritten by &#39;GGally&#39;: ## method from ## +.gg ggplot2 2.2.3 Predefined tasks mlr3 ships with a few predefined machine learning tasks. All tasks are stored in an R6 Dictionary (a key-value store) named mlr_tasks. Printing it gives the keys (the names of the datasets): mlr_tasks ## &lt;DictionaryTask&gt; with 9 stored values ## Keys: boston_housing, german_credit, iris, mtcars, pima, sonar, spam, ## wine, zoo We can get a more informative summary of the example tasks by converting the dictionary to a data.table() object: library(&quot;data.table&quot;) as.data.table(mlr_tasks) ## # A tibble: 9 x 11 ## key task_type nrow ncol lgl int dbl chr fct ord pxc ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 boston_housing regr 506 19 0 3 13 0 2 0 0 ## 2 german_credit classif 1000 21 0 0 7 0 12 1 0 ## 3 iris classif 150 5 0 0 4 0 0 0 0 ## 4 mtcars regr 32 11 0 0 10 0 0 0 0 ## 5 pima classif 768 9 0 0 8 0 0 0 0 ## 6 sonar classif 208 61 0 0 60 0 0 0 0 ## 7 spam classif 4601 58 0 0 57 0 0 0 0 ## 8 wine classif 178 14 0 2 11 0 0 0 0 ## 9 zoo classif 101 17 15 1 0 0 0 0 0 In the above display, the columns “lgl” (logical), “int” (integer), “dbl” (double), “chr” (character), “fct” (factor) and “ord” (ordinal) display the number of features (or columns) in the dataset with the corresponding datatype. To get a task from the dictionary, one can use the $get() method from the mlr_tasks class and assign the return value to a new object. For example, to use the iris data set for classification: task_iris = mlr_tasks$get(&quot;iris&quot;) print(task_iris) ## &lt;TaskClassif:iris&gt; (150 x 5) ## * Target: Species ## * Properties: multiclass ## * Features (4): ## - dbl (4): Petal.Length, Petal.Width, Sepal.Length, Sepal.Width Alternatively, you can also use the convenience function tsk(), which also constructs a task from the dictionary. tsk(&quot;iris&quot;) ## &lt;TaskClassif:iris&gt; (150 x 5) ## * Target: Species ## * Properties: multiclass ## * Features (4): ## - dbl (4): Petal.Length, Petal.Width, Sepal.Length, Sepal.Width 2.2.4 Task API All task properties and characteristics can be queried using the task’s public fields and methods (see Task). Methods are also used to change the behavior of the task. 2.2.4.1 Retrieving Data The data stored in a task can be retrieved directly from fields, for example: task_iris$nrow ## [1] 150 task_iris$ncol ## [1] 5 More information can be obtained through methods of the object, for example: task_iris$data() ## # A tibble: 150 x 5 ## Species Petal.Length Petal.Width Sepal.Length Sepal.Width ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 1.4 0.2 5.1 3.5 ## 2 setosa 1.4 0.2 4.9 3 ## 3 setosa 1.3 0.2 4.7 3.2 ## 4 setosa 1.5 0.2 4.6 3.1 ## 5 setosa 1.4 0.2 5 3.6 ## 6 setosa 1.7 0.4 5.4 3.9 ## 7 setosa 1.4 0.3 4.6 3.4 ## 8 setosa 1.5 0.2 5 3.4 ## 9 setosa 1.4 0.2 4.4 2.9 ## 10 setosa 1.5 0.1 4.9 3.1 ## # … with 140 more rows In mlr3, each row (observation) has a unique identifier, stored as an integer(). These can be passed as arguments to the $data() method to select specific rows: head(task_iris$row_ids) ## [1] 1 2 3 4 5 6 # retrieve data for rows with ids 1, 51, and 101 task_iris$data(rows = c(1, 51, 101)) ## # A tibble: 3 x 5 ## Species Petal.Length Petal.Width Sepal.Length Sepal.Width ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 1.4 0.2 5.1 3.5 ## 2 versicolor 4.7 1.4 7 3.2 ## 3 virginica 6 2.5 6.3 3.3 Similarly, target and feature columns also have unique identifiers, i.e. names. These names are stored in the public slots $feature_names and $target_names. Here “target” refers to the variable we want to predict and “feature” to the predictors for the task. task_iris$feature_names ## [1] &quot;Petal.Length&quot; &quot;Petal.Width&quot; &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; task_iris$target_names ## [1] &quot;Species&quot; The row_ids and column names can be combined when selecting a subset of the data: # retrieve data for rows 1, 51, and 101 and only select column &quot;Species&quot; task_iris$data(rows = c(1, 51, 101), cols = &quot;Species&quot;) ## # A tibble: 3 x 1 ## Species ## &lt;fct&gt; ## 1 setosa ## 2 versicolor ## 3 virginica To extract the complete data from the task, one can simply convert it to a data.table: summary(as.data.table(task_iris)) ## Species Petal.Length Petal.Width Sepal.Length Sepal.Width ## setosa :50 Min. :1.00 Min. :0.1 Min. :4.30 Min. :2.00 ## versicolor:50 1st Qu.:1.60 1st Qu.:0.3 1st Qu.:5.10 1st Qu.:2.80 ## virginica :50 Median :4.35 Median :1.3 Median :5.80 Median :3.00 ## Mean :3.76 Mean :1.2 Mean :5.84 Mean :3.06 ## 3rd Qu.:5.10 3rd Qu.:1.8 3rd Qu.:6.40 3rd Qu.:3.30 ## Max. :6.90 Max. :2.5 Max. :7.90 Max. :4.40 2.2.4.2 Roles (Rows and Columns) It is possible to assign roles to rows and columns. These roles affect the behavior of the task for different operations. Furthermore, these roles provide additional meta-data for it. For example, the previously-constructed mtcars task has the following column roles: print(task_mtcars$col_roles) ## $feature ## [1] &quot;cyl&quot; &quot;disp&quot; ## ## $target ## [1] &quot;mpg&quot; ## ## $name ## character(0) ## ## $order ## character(0) ## ## $stratum ## character(0) ## ## $group ## character(0) ## ## $weight ## character(0) To add the row names of mtcars as an additional feature, we first add them to the data table and then recreate the task. # with `keep.rownames`, data.table stores the row names in an extra column &quot;rn&quot; data = as.data.table(mtcars[, 1:3], keep.rownames = TRUE) task = TaskRegr$new(id = &quot;cars&quot;, backend = data, target = &quot;mpg&quot;) # there is a new feature called &quot;rn&quot; task$feature_names ## [1] &quot;cyl&quot; &quot;disp&quot; &quot;rn&quot; The row names are now a feature whose values are stored in the column &quot;rn&quot;. We include this column here for educational purposes only. Generally speaking, there is no point in having a feature that uniquely identifies each row. Furthermore, the character data type will cause problems with many types of machine learning algorithms. The identifier may be useful to label points in plots and identify outliers however. To use the new column for only this purpose, we will change the role of the &quot;rn&quot; column and remove it from the set of active features. This is done by simply modifying the field $col_roles, which is a named list of vectors of column names. Each vector in this list corresponds to a column role, and the column names contained in that vector are designated as having that role. Supported column roles can be found in the manual of Task. # supported column roles, see ?Task names(task$col_roles) ## [1] &quot;feature&quot; &quot;target&quot; &quot;name&quot; &quot;order&quot; &quot;stratum&quot; &quot;group&quot; &quot;weight&quot; # assign column &quot;rn&quot; the role &quot;name&quot; task$col_roles$name = &quot;rn&quot; # remove &quot;rn&quot; from role &quot;feature&quot; task$col_roles$feature = setdiff(task$col_roles$feature, &quot;rn&quot;) # &quot;rn&quot; not listed as feature anymore task$feature_names ## [1] &quot;cyl&quot; &quot;disp&quot; # &quot;rn&quot; also does not appear anymore when we access the data task$data(rows = 1:2) ## # A tibble: 2 x 3 ## mpg cyl disp ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 21 6 160 ## 2 21 6 160 task$head(2) ## # A tibble: 2 x 3 ## mpg cyl disp ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 21 6 160 ## 2 21 6 160 Changing the role does not change the underlying data. Changing the role only changes the view on it. The data is not copied in the code above. The view is changed in-place though, i.e. the task object itself is modified. Just like columns, it is also possible to assign different roles to rows. Rows can have two different roles: Role use: Rows that are generally available for model fitting (although they may also be used as test set in resampling). This role is the default role. Role validation: Rows that are not used for training. Rows that have missing values in the target column during task creation are automatically set to the validation role. There are several reasons to hold some observations back or treat them differently: It is often good practice to validate the final model on an external validation set to identify possible overfitting. Some observations may be unlabeled, e.g. in competitions like Kaggle. These observations cannot be used for training a model, but can be used to get predictions. 2.2.4.3 Task Mutators As shown above, modifying $col_roles or $row_roles changes the view on the data. The additional convenience method $filter() subsets the current view based on row ids and $select() subsets the view based on feature names. task = tsk(&quot;iris&quot;) task$select(c(&quot;Sepal.Width&quot;, &quot;Sepal.Length&quot;)) # keep only these features task$filter(1:3) # keep only these rows task$head() ## # A tibble: 3 x 3 ## Species Sepal.Length Sepal.Width ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 5.1 3.5 ## 2 setosa 4.9 3 ## 3 setosa 4.7 3.2 While the methods discussed above allow to subset the data, the methods $rbind() and $cbind() allow to add extra rows and columns to a task. Again, the original data is not changed. The additional rows or columns are only added to the view of the data. task$cbind(data.table(foo = letters[1:3])) # add column foo task$head() ## # A tibble: 3 x 4 ## Species Sepal.Length Sepal.Width foo ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 setosa 5.1 3.5 a ## 2 setosa 4.9 3 b ## 3 setosa 4.7 3.2 c 2.2.5 Plotting Tasks The mlr3viz package provides plotting facilities for many classes implemented in mlr3. The available plot types depend on the inherited class, but all plots are returned as ggplot2 objects which can be easily customized. For classification tasks (inheriting from TaskClassif), see the documentation of mlr3viz::autoplot.TaskClassif for the implemented plot types. Here are some examples to get an impression: library(&quot;mlr3viz&quot;) # get the pima indians task task = tsk(&quot;pima&quot;) # subset task to only use the 3 first features task$select(head(task$feature_names, 3)) # default plot: class frequencies autoplot(task) # pairs plot (requires package GGally) autoplot(task, type = &quot;pairs&quot;) # duo plot (requires package GGally) autoplot(task, type = &quot;duo&quot;) Of course, you can do the same for regression tasks (inheriting from TaskRegr) as documented in mlr3viz::autoplot.TaskRegr: library(&quot;mlr3viz&quot;) # get the mtcars task task = tsk(&quot;mtcars&quot;) # subset task to only use the 3 first features task$select(head(task$feature_names, 3)) # default plot: boxplot of target variable autoplot(task) # pairs plot (requires package GGally) autoplot(task, type = &quot;pairs&quot;) "],
["learners.html", "2.3 Learners", " 2.3 Learners Objects of class mlr3::Learner provide a unified interface to many popular machine learning algorithms in R. They consist of methods to train and predict a model for a mlr3::Task and provide meta-information about the learners, such as the hyperparameters you can set. The package ships with a minimal set of classification and regression learners to avoid lots of dependencies: mlr_learners_classif.featureless: Simple baseline classification learner, constantly predicts the label that is most frequent in the training set. mlr_learners_classif.rpart: Single classification tree from rpart. mlr_learners_regr.featureless: Simple baseline regression learner, constantly predicts with the mean. mlr_learners_regr.rpart: Single regression tree from rpart. Some of the most popular learners are connected via the mlr3learners package: (penalized) linear and logistic regression \\(k\\)-Nearest Neighbors regression and classification Linear and Quadratic Discriminant Analysis Naive Bayes Support-Vector machines Gradient Boosting Random Regression Forests and Random Classification Forests Kriging More learners are collected on GitHub in the mlr3learners organization. Their state is also listed on the wiki of the mlr3learners repository. Below a graphical illustration of the role of a learner: The base class of each learner is Learner, specialized for regression as LearnerRegr and for classification as LearnerClassif. In contrast to the Task, the creation of a custom Learner is usually not required and a more advanced topic. Hence, we refer the reader to Section 6.1 and proceed with an overview of the interface of already implemented learners. 2.3.1 Predefined Learners Similar to mlr_tasks, the Dictionary mlr_learners can be queried for available learners: library(&quot;mlr3learners&quot;) mlr_learners ## &lt;DictionaryLearner&gt; with 24 stored values ## Keys: classif.cv_glmnet, classif.debug, classif.featureless, ## classif.glmnet, classif.kknn, classif.lda, classif.log_reg, ## classif.multinom, classif.naive_bayes, classif.qda, classif.ranger, ## classif.rpart, classif.svm, classif.xgboost, regr.cv_glmnet, ## regr.featureless, regr.glmnet, regr.kknn, regr.km, regr.lm, ## regr.ranger, regr.rpart, regr.svm, regr.xgboost Each learner has the following information: feature_types: the type of features the learner can deal with. packages: the packages required to train a model with this learner and make predictions. properties: additional properties and capabilities. For example, a learner has the property “missings” if it is able to handle missing feature values, and “importance” if it computes and allows to extract data on the relative importance of the features. A complete list of these is available in the mlr3 reference on regression learners and classification learners. predict_types: possible prediction types. For example, a classification learner can predict labels (“response”) or probabilities (“prob”). For a complete list of possible predict types see the mlr3 reference. For a tabular overview of integrated learners, see Section 9.1. You can get a specific learner using its id, listed under key in the dictionary: learner = mlr_learners$get(&quot;classif.rpart&quot;) print(learner) ## &lt;LearnerClassifRpart:classif.rpart&gt; ## * Model: - ## * Parameters: xval=0 ## * Packages: rpart ## * Predict Type: response ## * Feature types: logical, integer, numeric, factor, ordered ## * Properties: importance, missings, multiclass, selected_features, ## twoclass, weights The field param_set stores a description of the hyperparameters the learner has, their ranges, defaults, and current values: learner$param_set ## ParamSet: ## id class lower upper levels default value ## 1: minsplit ParamInt 1 Inf 20 ## 2: minbucket ParamInt 1 Inf &lt;NoDefault&gt; ## 3: cp ParamDbl 0 1 0.01 ## 4: maxcompete ParamInt 0 Inf 4 ## 5: maxsurrogate ParamInt 0 Inf 5 ## 6: maxdepth ParamInt 1 30 30 ## 7: usesurrogate ParamInt 0 2 2 ## 8: surrogatestyle ParamInt 0 1 0 ## 9: xval ParamInt 0 Inf 10 0 The set of current hyperparameter values is stored in the values field of the param_set field. You can change the current hyperparameter values by assigning a named list to this field: learner$param_set$values = list(cp = 0.01, xval = 0) learner ## &lt;LearnerClassifRpart:classif.rpart&gt; ## * Model: - ## * Parameters: cp=0.01, xval=0 ## * Packages: rpart ## * Predict Type: response ## * Feature types: logical, integer, numeric, factor, ordered ## * Properties: importance, missings, multiclass, selected_features, ## twoclass, weights Note that this operation just overwrites all previously set parameters. If you just want to add or update hyperparameters, you can use mlr3misc::insert_named(): learner$param_set$values = mlr3misc::insert_named( learner$param_set$values, list(cp = 0.02, minsplit = 2) ) learner ## &lt;LearnerClassifRpart:classif.rpart&gt; ## * Model: - ## * Parameters: cp=0.02, xval=0, minsplit=2 ## * Packages: rpart ## * Predict Type: response ## * Feature types: logical, integer, numeric, factor, ordered ## * Properties: importance, missings, multiclass, selected_features, ## twoclass, weights This updates cp to 0.02, sets minsplit to 2 and keeps the previously set parameter xval. Again, there is an alternative to writing down the lengthy mlr_learners$get() part: lrn(). This function additionally allows to construct learners with specific hyperparameters or settings of a different identifier in one go: lrn(&quot;classif.rpart&quot;, id = &quot;rp&quot;, cp = 0.001) ## &lt;LearnerClassifRpart:rp&gt; ## * Model: - ## * Parameters: xval=0, cp=0.001 ## * Packages: rpart ## * Predict Type: response ## * Feature types: logical, integer, numeric, factor, ordered ## * Properties: importance, missings, multiclass, selected_features, ## twoclass, weights If you pass hyperparameters here, they are added to the parameters in a insert_named()-fashion. "],
["train-predict.html", "2.4 Train and Predict", " 2.4 Train and Predict In this section, we explain how tasks and learners can be used to train a model and predict to a new dataset. The concept is demonstrated on a supervised classification using the iris dataset and the rpart learner, which builds a singe classification tree. Training a learner means fitting a model to a given data set. Subsequently, we want to predict the label for new observations. These predictions are compared to the ground truth values in order to assess the predictive performance of the model. 2.4.1 Creating Task and Learner Objects The first step is to generate the following mlr3 objects from the task dictionary and the learner dictionary, respectively: The classification task: task = tsk(&quot;sonar&quot;) A learner for the classification tree: learner = lrn(&quot;classif.rpart&quot;) 2.4.2 Setting up the train/test splits of the data It is common to train on a majority of the data. Here we use 80% of all available observations and predict on the remaining 20%. For this purpose, we create two index vectors: train_set = sample(task$nrow, 0.8 * task$nrow) test_set = setdiff(seq_len(task$nrow), train_set) In Section 2.5 we will learn how mlr3 can automatically create training and test sets based on different resampling strategies. 2.4.3 Training the learner The field $model stores the model that is produced in the training step. Before the $train() method is called on a learner object, this field is NULL: learner$model ## NULL Next, the classification tree is trained using the train set of the iris task by calling the $train() method of the Learner: learner$train(task, row_ids = train_set) This operation modifies the learner in-place. We can now access the stored model via the field $model: print(learner$model) ## n= 166 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 166 80 M (0.51807 0.48193) ## 2) V11&gt;=0.1609 109 33 M (0.69725 0.30275) ## 4) V27&gt;=0.8167 51 4 M (0.92157 0.07843) * ## 5) V27&lt; 0.8167 58 29 M (0.50000 0.50000) ## 10) V45&gt;=0.2383 12 0 M (1.00000 0.00000) * ## 11) V45&lt; 0.2383 46 17 R (0.36957 0.63043) ## 22) V31&lt; 0.3743 19 6 M (0.68421 0.31579) * ## 23) V31&gt;=0.3743 27 4 R (0.14815 0.85185) * ## 3) V11&lt; 0.1609 57 10 R (0.17544 0.82456) ## 6) V4&gt;=0.0515 11 4 M (0.63636 0.36364) * ## 7) V4&lt; 0.0515 46 3 R (0.06522 0.93478) * 2.4.4 Predicting After the model has been trained, we use the remaining part of the data for prediction. Remember that we initially split the data in train_set and test_set. prediction = learner$predict(task, row_ids = test_set) print(prediction) ## &lt;PredictionClassif&gt; for 42 observations: ## row_id truth response ## 10 R R ## 12 R R ## 15 R M ## --- ## 182 M M ## 192 M M ## 208 M M The $predict() method of the Learner returns a Prediction object. More precisely, a LearnerClassif returns a PredictionClassif object. A prediction objects holds the row ids of the test data, the respective true label of the target column and the respective predictions. The simplest way to extract this information is by converting the Prediction object to a data.table(): head(as.data.table(prediction)) ## # A tibble: 6 x 3 ## row_id truth response ## &lt;int&gt; &lt;fct&gt; &lt;fct&gt; ## 1 10 R R ## 2 12 R R ## 3 15 R M ## 4 18 R M ## 5 19 R M ## 6 28 R R For classification, you can also extract the confusion matrix: prediction$confusion ## truth ## response M R ## M 19 7 ## R 6 10 2.4.5 Changing the Predict Type Classification learners default to predicting the class label. However, many classifiers additionally also tell you how sure they are about the predicted label by providing posterior probabilities. To switch to predicting these probabilities, the predict_type field of a LearnerClassif must be changed from &quot;response&quot; to &quot;prob&quot; before training: learner$predict_type = &quot;prob&quot; # re-fit the model learner$train(task, row_ids = train_set) # rebuild prediction object prediction = learner$predict(task, row_ids = test_set) The prediction object now contains probabilities for all class labels: # data.table conversion head(as.data.table(prediction)) ## # A tibble: 6 x 5 ## row_id truth response prob.M prob.R ## &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 10 R R 0.0652 0.935 ## 2 12 R R 0.0652 0.935 ## 3 15 R M 0.922 0.0784 ## 4 18 R M 0.636 0.364 ## 5 19 R M 0.922 0.0784 ## 6 28 R R 0.148 0.852 # directly access the predicted labels: head(prediction$response) ## [1] R R M M M R ## Levels: M R # directly access the matrix of probabilities: head(prediction$prob) ## M R ## [1,] 0.06522 0.93478 ## [2,] 0.06522 0.93478 ## [3,] 0.92157 0.07843 ## [4,] 0.63636 0.36364 ## [5,] 0.92157 0.07843 ## [6,] 0.14815 0.85185 Analogously to predicting probabilities, many regression learners support the extraction of standard error estimates by setting the predict type to &quot;se&quot;. 2.4.6 Plotting Predictions Analogously to plotting tasks, mlr3viz provides a autoplot() method for Prediction objects. All available types are listed on the manual page of autoplot.PredictionClassif() or autoplot.PredictionClassif(), respectively. library(&quot;mlr3viz&quot;) task = tsk(&quot;sonar&quot;) learner = lrn(&quot;classif.rpart&quot;, predict_type = &quot;prob&quot;) learner$train(task) prediction = learner$predict(task) autoplot(prediction) autoplot(prediction, type = &quot;roc&quot;) library(&quot;mlr3viz&quot;) library(&quot;mlr3learners&quot;) local({ # we do this locally to not overwrite the objects from previous chunks task = tsk(&quot;mtcars&quot;) learner = lrn(&quot;regr.lm&quot;) learner$train(task) prediction = learner$predict(task) autoplot(prediction) }) 2.4.7 Performance assessment The last step of modeling is usually the performance assessment. To assess the quality of the predictions, the predicted labels are compared with the true labels. How this comparison is calculated is defined by a measure, which is given by a Measure object. Note that if the prediction was made on a dataset without the target column, i.e. without true labels, then no performance can be calculated. Predefined available measures are stored in mlr_measures (with convenience getter msr()): mlr_measures ## &lt;DictionaryMeasure&gt; with 53 stored values ## Keys: classif.acc, classif.auc, classif.bacc, classif.bbrier, ## classif.ce, classif.costs, classif.dor, classif.fbeta, classif.fdr, ## classif.fn, classif.fnr, classif.fomr, classif.fp, classif.fpr, ## classif.logloss, classif.mbrier, classif.mcc, classif.npv, ## classif.ppv, classif.precision, classif.recall, classif.sensitivity, ## classif.specificity, classif.tn, classif.tnr, classif.tp, ## classif.tpr, debug, oob_error, regr.bias, regr.ktau, regr.mae, ## regr.mape, regr.maxae, regr.medae, regr.medse, regr.mse, regr.msle, ## regr.pbias, regr.rae, regr.rmse, regr.rmsle, regr.rrse, regr.rse, ## regr.rsq, regr.sae, regr.smape, regr.srho, regr.sse, ## selected_features, time_both, time_predict, time_train We choose accuracy (classif.acc) as a specific performance measure and call the method $score() of the Prediction object to quantify the predictive performance. measure = msr(&quot;classif.acc&quot;) prediction$score(measure) ## classif.acc ## 0.875 Note that, if no measure is specified, classification defaults to classification error (classif.ce) and regression defaults to the mean squared error (regr.mse). "],
["resampling.html", "2.5 Resampling", " 2.5 Resampling Resampling strategies are usually used to assess the performance of a learning algorithm. mlr3 entails 6 predefined resampling strategies: Cross-validation, Leave-one-out cross validation, Repeated cross-validation, Out-of-bag bootstrap and other variants (e.g. b632), Monte-Carlo cross-validation and Holdout. The following sections provide guidance on how to set and select a resampling strategy and how to subsequently instantiate the resampling process. Below you can find a graphical illustration of the resampling process: 2.5.1 Settings In this example we use the iris task and a simple classification tree from the rpart package. task = tsk(&quot;iris&quot;) learner = lrn(&quot;classif.rpart&quot;) When performing resampling with a dataset, we first need to define which approach should be used. mlr3 resampling strategies and their parameters can be queried by looking at the data.table output of the mlr_resamplings dictionary: as.data.table(mlr_resamplings) ## # A tibble: 7 x 3 ## key params iters ## &lt;chr&gt; &lt;list&gt; &lt;int&gt; ## 1 bootstrap &lt;chr [2]&gt; 30 ## 2 custom &lt;chr [0]&gt; 0 ## 3 cv &lt;chr [1]&gt; 10 ## 4 holdout &lt;chr [1]&gt; 1 ## 5 insample &lt;chr [0]&gt; 1 ## 6 repeated_cv &lt;chr [2]&gt; 100 ## 7 subsampling &lt;chr [2]&gt; 30 Additional resampling methods for special use cases will be available via extension packages, such as mlr3spatiotemporal for spatial data (still in development). The model fit conducted in the train/predict/score chapter is equivalent to a “holdout resampling”, so let’s consider this one first. Again, we can retrieve elements from the dictionary mlr_resamplings via $get() or with the convenience functionrsmp(): resampling = rsmp(&quot;holdout&quot;) print(resampling) ## &lt;ResamplingHoldout&gt; with 1 iterations ## * Instantiated: FALSE ## * Parameters: ratio=0.6667 Note that the $is_instantiated field is set to FALSE. This means we did not actually apply the strategy on a dataset yet. Applying the strategy on a dataset is done in the next section Instantiation. By default we get a .66/.33 split of the data. There are two ways in which the ratio can be changed: Overwriting the slot in $param_set$values using a named list: resampling$param_set$values = list(ratio = 0.8) Specifying the resampling parameters directly during construction: rsmp(&quot;holdout&quot;, ratio = 0.8) ## &lt;ResamplingHoldout&gt; with 1 iterations ## * Instantiated: FALSE ## * Parameters: ratio=0.8 2.5.2 Instantiation So far we just set the stage and selected the resampling strategy. To actually perform the splitting and obtain indices for the training and the test split the resampling needs a Task. By calling the method instantiate(), we split the indices of the data into indices for training and test sets. These resulting indices are stored in the Resampling object: resampling = rsmp(&quot;cv&quot;, folds = 3L) resampling$instantiate(task) resampling$iters ## [1] 3 str(resampling$train_set(1)) ## int [1:100] 1 5 9 16 19 20 22 23 27 31 ... str(resampling$test_set(1)) ## int [1:50] 3 4 7 8 11 13 17 18 21 24 ... 2.5.3 Execution With a Task, a Learner and a Resampling object we can call resample(), which fits the learner to the task at hand according to the given resampling strategy. This in turn creates a ResampleResult object. Before we go into more detail, let’s change the resampling to a “3-fold cross-validation” to better illustrate what operations are possible with a ResampleResult. Additionally, when actually fitting the models, we tell resample() to keep the fitted models by setting the store_models option to TRUE: task = tsk(&quot;pima&quot;) learner = lrn(&quot;classif.rpart&quot;, maxdepth = 3, predict_type = &quot;prob&quot;) resampling = rsmp(&quot;cv&quot;, folds = 3L) rr = resample(task, learner, resampling, store_models = TRUE) print(rr) ## &lt;ResampleResult&gt; of 3 iterations ## * Task: pima ## * Learner: classif.rpart ## * Warnings: 0 in 0 iterations ## * Errors: 0 in 0 iterations The following operations are supported with ResampleResult objects: Calculate the average performance across all resampling iterations: rr$aggregate(msr(&quot;classif.ce&quot;)) ## classif.ce ## 0.25 Extract the performance for the individual resampling iterations: rr$score(msr(&quot;classif.ce&quot;)) ## # A tibble: 3 x 9 ## task task_id learner learner_id resampling resampling_id iteration prediction ## &lt;lis&gt; &lt;chr&gt; &lt;list&gt; &lt;chr&gt; &lt;list&gt; &lt;chr&gt; &lt;int&gt; &lt;list&gt; ## 1 &lt;Tsk… pima &lt;LrnrC… classif.r… &lt;RsmplnCV&gt; cv 1 &lt;named li… ## 2 &lt;Tsk… pima &lt;LrnrC… classif.r… &lt;RsmplnCV&gt; cv 2 &lt;named li… ## 3 &lt;Tsk… pima &lt;LrnrC… classif.r… &lt;RsmplnCV&gt; cv 3 &lt;named li… ## # … with 1 more variable: classif.ce &lt;dbl&gt; Check for warnings or errors: rr$warnings ## # A tibble: 0 x 2 ## # … with 2 variables: iteration &lt;int&gt;, msg &lt;chr&gt; rr$errors ## # A tibble: 0 x 2 ## # … with 2 variables: iteration &lt;int&gt;, msg &lt;chr&gt; Extract and inspect the resampling splits: rr$resampling ## &lt;ResamplingCV&gt; with 3 iterations ## * Instantiated: TRUE ## * Parameters: folds=3 rr$resampling$iters ## [1] 3 str(rr$resampling$test_set(1)) ## int [1:256] 3 7 8 9 11 12 14 15 23 25 ... str(rr$resampling$train_set(1)) ## int [1:512] 1 2 4 5 13 16 18 20 24 27 ... Retrieve the learner of a specific iteration and inspect it: lrn = rr$learners[[1]] lrn$model ## n= 512 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 512 165 neg (0.3223 0.6777) ## 2) glucose&gt;=143.5 112 33 pos (0.7054 0.2946) * ## 3) glucose&lt; 143.5 400 86 neg (0.2150 0.7850) * Extract the predictions: rr$prediction() # all predictions merged into a single Prediction ## &lt;PredictionClassif&gt; for 768 observations: ## row_id truth response prob.pos prob.neg ## 3 pos pos 0.7054 0.2946 ## 7 pos neg 0.2150 0.7850 ## 8 neg neg 0.2150 0.7850 ## --- ## 764 neg neg 0.2129 0.7871 ## 766 neg neg 0.2129 0.7871 ## 768 neg neg 0.2129 0.7871 rr$predictions()[[1]] # prediction of first resampling iteration ## &lt;PredictionClassif&gt; for 256 observations: ## row_id truth response prob.pos prob.neg ## 3 pos pos 0.7054 0.2946 ## 7 pos neg 0.2150 0.7850 ## 8 neg neg 0.2150 0.7850 ## --- ## 758 pos neg 0.2150 0.7850 ## 765 neg neg 0.2150 0.7850 ## 767 pos neg 0.2150 0.7850 Note that if you want to compare multiple Learners in a fair manner, it is important to ensure that each learner operates on the same resampling instance. This can be achieved by manually instantiating the instance before fitting model(s) on it. Hint: If your aim is to compare different Task, Learner or Resampling, you are better off using the benchmark() function which is covered in the next section on benchmarking. It is a wrapper around resample(), simplifying the handling of large comparison grids. If you discover this only after you’ve run multiple resample() calls, don’t worry. You can combine multiple ResampleResult objects into a BenchmarkResult (also explained in the section benchmarking). 2.5.4 Custom resampling Sometimes it is necessary to perform resampling with custom splits. If you want to do that because you are coming from a specific modeling field, first take a look at the mlr3 extension packages, to check wheter your resampling method has been implemented already. If this is not the case, feel welcome to extend an existing package or create your own extension package. A manual resampling instance can be created using the &quot;custom&quot; template. resampling = rsmp(&quot;custom&quot;) resampling$instantiate(task, train = list(c(1:10, 51:60, 101:110)), test = list(c(11:20, 61:70, 111:120)) ) resampling$iters ## [1] 1 resampling$train_set(1) ## [1] 1 2 3 4 5 6 7 8 9 10 51 52 53 54 55 56 57 58 59 ## [20] 60 101 102 103 104 105 106 107 108 109 110 resampling$test_set(1) ## [1] 11 12 13 14 15 16 17 18 19 20 61 62 63 64 65 66 67 68 69 ## [20] 70 111 112 113 114 115 116 117 118 119 120 2.5.5 Plotting Resample Results Again, mlr3viz provides a autoplot() method. library(&quot;mlr3viz&quot;) autoplot(rr) autoplot(rr, type = &quot;roc&quot;) All available plot types are listed on the manual page of autoplot.ResampleResult(). "],
["benchmarking.html", "2.6 Benchmarking", " 2.6 Benchmarking Comparing the performance of different learners on multiple tasks and/or different resampling schemes is a common task. This operation is usually referred to as “benchmarking” in the field of machine-learning. The mlr3 package offers the benchmark() function for convenience. 2.6.1 Design Creation In mlr3 we require you to supply a “design” of your benchmark experiment. By “design” we essentially mean the matrix of settings you want to execute. A “design” consists of unique combinations of Task, Learner and Resampling triplets. Here, we call benchmark() to perform a single holdout split on a single task and two learners. We use the benchmark_grid() function to create an exhaustive design and instantiate the resampling properly, so that all learners are executed on the same train/test split for each task: library(&quot;data.table&quot;) design = benchmark_grid( tasks = tsk(&quot;iris&quot;), learners = list(lrn(&quot;classif.rpart&quot;), lrn(&quot;classif.featureless&quot;)), resamplings = rsmp(&quot;holdout&quot;) ) print(design) ## task learner resampling ## 1: &lt;TaskClassif&gt; &lt;LearnerClassifRpart&gt; &lt;ResamplingHoldout&gt; ## 2: &lt;TaskClassif&gt; &lt;LearnerClassifFeatureless&gt; &lt;ResamplingHoldout&gt; bmr = benchmark(design) Instead of using benchmark_grid() you could also create the design manually as a data.table and use the full flexibility of the benchmark() function. The design does not have to be exhaustive, e.g. it can also contain a different learner for each task. However, you should note that benchmark_grid() makes sure to instantiate the resamplings for each task. If you create the design manually, even if the same task is used multiple times, the train/test splits will be different for each row of the design if you do not manually instantiate the resampling before creating the design. Let’s construct a more complex design to show the full capabilities of the benchmark() function. # get some example tasks tasks = lapply(c(&quot;german_credit&quot;, &quot;sonar&quot;), tsk) # get some learners and for all learners ... # * predict probabilities # * predict also on the training set library(&quot;mlr3learners&quot;) learners = c(&quot;classif.featureless&quot;, &quot;classif.rpart&quot;, &quot;classif.ranger&quot;, &quot;classif.kknn&quot;) learners = lapply(learners, lrn, predict_type = &quot;prob&quot;, predict_sets = c(&quot;train&quot;, &quot;test&quot;)) # compare via 3-fold cross validation resamplings = rsmp(&quot;cv&quot;, folds = 3) # create a BenchmarkDesign object design = benchmark_grid(tasks, learners, resamplings) print(design) ## task learner resampling ## 1: &lt;TaskClassif&gt; &lt;LearnerClassifFeatureless&gt; &lt;ResamplingCV&gt; ## 2: &lt;TaskClassif&gt; &lt;LearnerClassifRpart&gt; &lt;ResamplingCV&gt; ## 3: &lt;TaskClassif&gt; &lt;LearnerClassifRanger&gt; &lt;ResamplingCV&gt; ## 4: &lt;TaskClassif&gt; &lt;LearnerClassifKKNN&gt; &lt;ResamplingCV&gt; ## 5: &lt;TaskClassif&gt; &lt;LearnerClassifFeatureless&gt; &lt;ResamplingCV&gt; ## 6: &lt;TaskClassif&gt; &lt;LearnerClassifRpart&gt; &lt;ResamplingCV&gt; ## 7: &lt;TaskClassif&gt; &lt;LearnerClassifRanger&gt; &lt;ResamplingCV&gt; ## 8: &lt;TaskClassif&gt; &lt;LearnerClassifKKNN&gt; &lt;ResamplingCV&gt; 2.6.2 Execution and Aggregation of Results After the benchmark design is ready, we can directly call benchmark(): # execute the benchmark bmr = benchmark(design) Note that we did not instantiate the resampling instance manually. benchmark_grid() took care of it for us: Each resampling strategy is instantiated once for each task during the construction of the exhaustive grid. After the benchmark, we can calculate and aggregate the performance with $aggregate(): # measures: # * area under the curve (auc) on training # * area under the curve (auc) on test measures = list( msr(&quot;classif.auc&quot;, id = &quot;auc_train&quot;, predict_sets = &quot;train&quot;), msr(&quot;classif.auc&quot;, id = &quot;auc_test&quot;) ) bmr$aggregate(measures) ## # A tibble: 8 x 8 ## nr resample_result task_id learner_id resampling_id iters auc_train ## &lt;int&gt; &lt;list&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 &lt;RsmplRsl&gt; german… classif.f… cv 3 0.5 ## 2 2 &lt;RsmplRsl&gt; german… classif.r… cv 3 0.803 ## 3 3 &lt;RsmplRsl&gt; german… classif.r… cv 3 0.999 ## 4 4 &lt;RsmplRsl&gt; german… classif.k… cv 3 0.989 ## 5 5 &lt;RsmplRsl&gt; sonar classif.f… cv 3 0.5 ## 6 6 &lt;RsmplRsl&gt; sonar classif.r… cv 3 0.920 ## 7 7 &lt;RsmplRsl&gt; sonar classif.r… cv 3 1 ## 8 8 &lt;RsmplRsl&gt; sonar classif.k… cv 3 0.999 ## # … with 1 more variable: auc_test &lt;dbl&gt; Subsequently, we can aggregate the results further. For example, we might be interested which learner performed best over all tasks simultaneously. Simply aggregating the performances with the mean is usually not statistically sound. Instead, we calculate the rank statistic for each learner grouped by task. Then the calculated ranks grouped by learner are aggregated with data.table. Since the AUC needs to be maximized, we multiply with \\(-1\\) so that the best learner gets a rank of \\(1\\). tab = bmr$aggregate(measures) print(tab) ## nr resample_result task_id learner_id resampling_id iters ## 1: 1 &lt;ResampleResult&gt; german_credit classif.featureless cv 3 ## 2: 2 &lt;ResampleResult&gt; german_credit classif.rpart cv 3 ## 3: 3 &lt;ResampleResult&gt; german_credit classif.ranger cv 3 ## 4: 4 &lt;ResampleResult&gt; german_credit classif.kknn cv 3 ## 5: 5 &lt;ResampleResult&gt; sonar classif.featureless cv 3 ## 6: 6 &lt;ResampleResult&gt; sonar classif.rpart cv 3 ## 7: 7 &lt;ResampleResult&gt; sonar classif.ranger cv 3 ## 8: 8 &lt;ResampleResult&gt; sonar classif.kknn cv 3 ## auc_train auc_test ## 1: 0.5000 0.5000 ## 2: 0.8028 0.6961 ## 3: 0.9986 0.7956 ## 4: 0.9892 0.6968 ## 5: 0.5000 0.5000 ## 6: 0.9196 0.7531 ## 7: 1.0000 0.9093 ## 8: 0.9985 0.9011 # group by levels of task_id, return columns: # - learner_id # - rank of col &#39;-auc_train&#39; (per level of learner_id) # - rank of col &#39;-auc_test&#39; (per level of learner_id) ranks = tab[, .(learner_id, rank_train = rank(-auc_train), rank_test = rank(-auc_test)), by = task_id] print(ranks) ## task_id learner_id rank_train rank_test ## 1: german_credit classif.featureless 4 4 ## 2: german_credit classif.rpart 3 3 ## 3: german_credit classif.ranger 1 1 ## 4: german_credit classif.kknn 2 2 ## 5: sonar classif.featureless 4 4 ## 6: sonar classif.rpart 3 3 ## 7: sonar classif.ranger 1 1 ## 8: sonar classif.kknn 2 2 # group by levels of learner_id, return columns: # - mean rank of col &#39;rank_train&#39; (per level of learner_id) # - mean rank of col &#39;rank_test&#39; (per level of learner_id) ranks = ranks[, .(mrank_train = mean(rank_train), mrank_test = mean(rank_test)), by = learner_id] # print the final table, ordered by mean rank of AUC test ranks[order(mrank_test)] ## # A tibble: 4 x 3 ## learner_id mrank_train mrank_test ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 classif.ranger 1 1 ## 2 classif.kknn 2 2 ## 3 classif.rpart 3 3 ## 4 classif.featureless 4 4 Unsurprisingly, the featureless learner is outperformed on both training and test set. 2.6.3 Plotting Benchmark Results Analogously to plotting tasks, predictions or resample results, mlr3viz also provides a autoplot() method for benchmark results. library(&quot;mlr3viz&quot;) library(&quot;ggplot2&quot;) autoplot(bmr) + theme(axis.text.x = element_text(angle = 45, hjust = 1)) We can also plot ROC curves. To do so, we first need to filter the BenchmarkResult to only contain a single Task: autoplot(bmr$clone()$filter(task_id = &quot;german_credit&quot;), type = &quot;roc&quot;) All available types are listed on the manual page of autoplot.BenchmarkResult(). 2.6.4 Extracting ResampleResults A BenchmarkResult object is essentially a collection of multiple ResampleResult objects. As these are stored in a column of the aggregated data.table(), we can easily extract them: tab = bmr$aggregate(measures) rr = tab[task_id == &quot;sonar&quot; &amp; learner_id == &quot;classif.ranger&quot;]$resample_result[[1]] print(rr) ## &lt;ResampleResult&gt; of 3 iterations ## * Task: sonar ## * Learner: classif.ranger ## * Warnings: 0 in 0 iterations ## * Errors: 0 in 0 iterations We can now investigate this resampling and even single resampling iterations using one of the approaches shown in the previous section: measure = msr(&quot;classif.auc&quot;) rr$aggregate(measure) ## classif.auc ## 0.9093 # get the iteration with worst AUC perf = rr$score(measure) i = which.min(perf$classif.auc) # get the corresponding learner and train set print(rr$learners[[i]]) ## &lt;LearnerClassifRanger:classif.ranger&gt; ## * Model: - ## * Parameters: list() ## * Packages: ranger ## * Predict Type: prob ## * Feature types: logical, integer, numeric, character, factor, ordered ## * Properties: importance, multiclass, oob_error, twoclass, weights head(rr$resampling$train_set(i)) ## [1] 1 2 3 13 17 26 2.6.5 Converting and Merging ResampleResults It is also possible to cast a single ResampleResult to a BenchmarkResult using the converter as_benchmark_result(). task = tsk(&quot;iris&quot;) resampling = rsmp(&quot;holdout&quot;)$instantiate(task) rr1 = resample(task, lrn(&quot;classif.rpart&quot;), resampling) rr2 = resample(task, lrn(&quot;classif.featureless&quot;), resampling) # Cast both ResampleResults to BenchmarkResults bmr1 = as_benchmark_result(rr1) bmr2 = as_benchmark_result(rr2) # Merge 2nd BMR into the first BMR bmr1$combine(bmr2) bmr1 ## &lt;BenchmarkResult&gt; of 2 rows with 2 resampling runs ## nr task_id learner_id resampling_id iters warnings errors ## 1 iris classif.rpart holdout 1 0 0 ## 2 iris classif.featureless holdout 1 0 0 "],
["binary.html", "2.7 Binary classification", " 2.7 Binary classification Classification problems with a target variable containing only two classes are called “binary”. For such binary target variables, you can specify the positive class within the classification task object during task creation. If not explicitly set during construction, the positive class defaults to the first level of the target variable. # during construction data(&quot;Sonar&quot;, package = &quot;mlbench&quot;) task = TaskClassif$new(id = &quot;Sonar&quot;, Sonar, target = &quot;Class&quot;, positive = &quot;R&quot;) # switch positive class to level &#39;M&#39; task$positive = &quot;M&quot; 2.7.1 ROC Curve and Thresholds ROC Analysis, which stands for “receiver operating characteristics”, is a subfield of machine learning which studies the evaluation of binary prediction systems. We saw earlier that one can retrieve the confusion matrix of a Prediction by accessing the $confusion field: learner = lrn(&quot;classif.rpart&quot;, predict_type = &quot;prob&quot;) pred = learner$train(task)$predict(task) C = pred$confusion print(C) ## truth ## response M R ## M 95 10 ## R 16 87 The confusion matrix contains the counts of correct and incorrect class assignments, grouped by class labels. The columns illustrate the true (observed) labels and the rows display the predicted labels. The positive is always the first row or column in the confusion matrix. Thus, the element in \\(C_{11}\\) is the number of times our model predicted the positive class and was right about it. Analogously, the element in \\(C_{22}\\) is the number of times our model predicted the negative class and was also right about it. The elements on the diagonal are called True Positives (TP) and True Negatives (TN). The element \\(C_{12}\\) is the number of times we falsely predicted a positive label, and is called False Positives (FP). The element \\(C_{21}\\) is called False Negatives (FN). We can now normalize in rows and columns of the confusion matrix to derive several informative metrics: True Positive Rate (TPR): How many of the true positives did we predict as positive? True Negative Rate (TNR): How many of the true negatives did we predict as negative? Positive Predictive Value PPV: If we predict positive how likely is it a true positive? Negative Predictive Value NPV: If we predict negative how likely is it a true negative? Source: Wikipedia It is difficult to achieve a high TPR and low FPR in conjunction, so one uses them for constructing the ROC Curve. We characterize a classifier by its TPR and FPR values and plot them in a coordinate system. The best classifier lies on the top-left corner. The worst classifier lies at the diagonal. Classifiers lying on the diagonal produce random labels (with different proportions). If each positive \\(x\\) will be randomly classified with 25% as “positive”, we get a TPR of 0.25. If we assign each negative \\(x\\) randomly to “positive” we get a FPR of 0.25. In practice, we should never obtain a classifier below the diagonal, as inverting the predicted labels will result in a reflection at the diagonal. A scoring classifier is a model which produces scores or probabilities, instead of discrete labels. To obtain probabilities from a learner in mlr3, you have to set predict_type = &quot;prob&quot; for a ref(&quot;LearnerClassif&quot;). Whether a classifier can predict probabilities is given in its $predict_types field. Thresholding flexibly converts measured probabilities to labels. Predict \\(1\\) (positive class) if \\(\\hat{f}(x) &gt; \\tau\\) else predict \\(0\\). Normally, one could use \\(\\tau = 0.5\\) to convert probabilities to labels, but for imbalanced or cost-sensitive situations another threshold could be more suitable. After thresholding, any metric defined on labels can be used. For mlr3 prediction objects, the ROC curve can easily be created with mlr3viz which relies on the precrec to calculate and plot ROC curves: # TPR vs FPR / Sensitivity vs (1 - Specificity) ggplot2::autoplot(pred, type = &quot;roc&quot;) # Precision vs Recall ggplot2::autoplot(pred, type = &quot;prc&quot;) 2.7.2 Threshold Tuning "],
["model-optim.html", "3 Model Optimization", " 3 Model Optimization Model Tuning Machine learning algorithms have default values set for their hyperparameters. Irrespective, these hyperparameters need to be changed by the user to achieve optimal performance on the given dataset. A manual selection of hyperparameter values is not recommended as this approach rarely leads to an optimal performance. To substantiate the validity of the selected hyperparameters (= tuning), data-driven optimization is recommended. In order to tune a machine learning algorithm, one has to specify (1) the search space, (2) the optimization algorithm (aka tuning method) and (3) an evaluation method, i.e., a resampling strategy and (4) a performance measure. In summary, the sub-chapter on tuning illustrates how to: undertake empirically sound hyperparameter selection select the optimizing algorithm trigger the tuning automate tuning This sub-chapter also requires the package mlr3tuning, an extension package which supports hyperparameter tuning. Feature Selection The second part of this chapter explains feature selection also known as variable selection. Feature selection is the process of finding a subset of relevant features of the available data. The purpose can be manifold: Enhance the interpretability of the model, speed up model fitting or improve the learner performance by reducing noise in the data. In this book we focus mainly on the last aspect. Different approaches exist to identify the relevant features. In this sub-chapter on feature selection, three approaches are emphasized: Filter algorithms select features independently of the learner according to a score. Variable importance filters select features that are important according to a learner. Wrapper methods iteratively select features to optimize a performance measure. Note, that filters do not require a learner. Variable importance filters require a learner that can calculate feature importance values once it is trained. The obtained importance values can be used to subset the data, which then can be used to train any learner. Wrapper methods can be used with any learner but need to train the learner multiple times. Nested Resampling In order to get a good estimate of generalization performance and avoid data leakage, both an outer (performance) and an inner (tuning/feature selection) resampling process are necessary. Following features are discussed in this chapter: Inner and outer resampling strategies in nested resampling The execution of nested resampling The evaluation of executed resampling iterations This sub-section will provide instructions on how to implement nested resampling, accounting for both inner and outer resampling in mlr3. "],
["tuning.html", "3.1 Hyperparameter Tuning", " 3.1 Hyperparameter Tuning Hyperparameters are second-order parameters of machine learning models that, while often not explicitly optimized during the model estimation process, can have important impacts on the outcome and predictive performance of a model. Typically, hyperparameters are fixed before training a model. However, because the output of a model can be sensitive to the specification of hyperparameters, it is often recommended to make an informed decision about which hyperparameter settings may yield better model performance. In many cases, hyperparameter settings may be chosen a priori, but it can be advantageous to try different settings before fitting your model on the training data. This process is often called ‘tuning’ your model. Hyperparameter tuning is supported via the extension package mlr3tuning. Below you can find an illustration of the process: At the heart of mlr3tuning are the R6 classes: TuningInstance: This class describes the tuning problem and stores results. Tuner: This class is the base class for implementations of tuning algorithms. 3.1.1 The TuningInstance Class The following sub-section examines the optimization of a simple classification tree on the Pima Indian Diabetes data set. task = tsk(&quot;pima&quot;) print(task) ## &lt;TaskClassif:pima&gt; (768 x 9) ## * Target: diabetes ## * Properties: twoclass ## * Features (8): ## - dbl (8): age, glucose, insulin, mass, pedigree, pregnant, pressure, ## triceps We use the classification tree from rpart and choose a subset of the hyperparameters we want to tune. This is often referred to as the “tuning space”. learner = lrn(&quot;classif.rpart&quot;) learner$param_set ## ParamSet: ## id class lower upper levels default value ## 1: minsplit ParamInt 1 Inf 20 ## 2: minbucket ParamInt 1 Inf &lt;NoDefault&gt; ## 3: cp ParamDbl 0 1 0.01 ## 4: maxcompete ParamInt 0 Inf 4 ## 5: maxsurrogate ParamInt 0 Inf 5 ## 6: maxdepth ParamInt 1 30 30 ## 7: usesurrogate ParamInt 0 2 2 ## 8: surrogatestyle ParamInt 0 1 0 ## 9: xval ParamInt 0 Inf 10 0 Here, we opt to tune two parameters: The complexity cp The termination criterion minsplit The tuning space has to be bound, therefore one has to set lower and upper bounds: library(&quot;paradox&quot;) tune_ps = ParamSet$new(list( ParamDbl$new(&quot;cp&quot;, lower = 0.001, upper = 0.1), ParamInt$new(&quot;minsplit&quot;, lower = 1, upper = 10) )) tune_ps ## ParamSet: ## id class lower upper levels default value ## 1: cp ParamDbl 0.001 0.1 &lt;NoDefault&gt; ## 2: minsplit ParamInt 1.000 10.0 &lt;NoDefault&gt; Next, we need to specify how to evaluate the performance. For this, we need to choose a resampling strategy and a performance measure. hout = rsmp(&quot;holdout&quot;) measure = msr(&quot;classif.ce&quot;) Finally, one has to select the budget available, to solve this tuning instance. This is done by selecting one of the available Terminators: Terminate after a given time (TerminatorClockTime) Terminate after a given amount of iterations (TerminatorEvals) Terminate after a specific performance is reached (TerminatorPerfReached) Terminate when tuning does not improve (TerminatorStagnation) A combination of the above in an ALL or ANY fashion (TerminatorCombo) For this short introduction, we specify a budget of 20 evaluations and then put everything together into a TuningInstance: library(&quot;mlr3tuning&quot;) evals20 = term(&quot;evals&quot;, n_evals = 20) instance = TuningInstance$new( task = task, learner = learner, resampling = hout, measures = measure, param_set = tune_ps, terminator = evals20 ) print(instance) ## &lt;TuningInstance&gt; ## * State: Not tuned ## * Task: &lt;TaskClassif:pima&gt; ## * Learner: &lt;LearnerClassifRpart:classif.rpart&gt; ## * Measures: classif.ce ## * Resampling: &lt;ResamplingHoldout&gt; ## * Terminator: &lt;TerminatorEvals&gt; ## * bm_args: list() ## * n_evals: 0 ## ParamSet: ## id class lower upper levels default value ## 1: cp ParamDbl 0.001 0.1 &lt;NoDefault&gt; ## 2: minsplit ParamInt 1.000 10.0 &lt;NoDefault&gt; To start the tuning, we still need to select how the optimization should take place. In other words, we need to choose the optimization algorithm via the Tuner class. 3.1.2 The Tuner Class The following algorithms are currently implemented in mlr3tuning: Grid Search (TunerGridSearch) Random Search (TunerRandomSearch) (Bergstra and Bengio 2012) Generalized Simulated Annealing (TunerGenSA) In this example, we will use a simple grid search with a grid resolution of 10: tuner = tnr(&quot;grid_search&quot;, resolution = 5) Since we have only numeric parameters, TunerGridSearch will create a grid of equally-sized steps between the respective upper and lower bounds. As we have two hyperparameters with a resolution of 5, the two-dimensional grid consists of \\(5^2 = 25\\) configurations. Each configuration serves as hyperparameter setting for the classification tree and triggers a 3-fold cross validation on the task. All configurations will be examined by the tuner (in a random order), until either all configurations are evaluated or the Terminator signals that the budget is exhausted. 3.1.3 Triggering the Tuning To start the tuning, we simply pass the TuningInstance to the $tune() method of the initialized Tuner. The tuner proceeds as follow: The Tuner proposes at least one hyperparameter configuration (the Tuner and may propose multiple points to improve parallelization, which can be controlled via the setting batch_size). For each configuration, a Learner is fitted on Task using the provided Resampling. The results are combined with other results from previous iterations to a single BenchmarkResult. The Terminator is queried if the budget is exhausted. If the budget is not exhausted, restart with 1) until it is. Determine the configuration with the best observed performance. Return a named list with the hyperparameter settings (&quot;values&quot;) and the corresponding measured performance (&quot;performance&quot;). result = tuner$tune(instance) print(result) ## NULL One can investigate all resamplings which were undertaken, using the $archive() method of the TuningInstance. Here, we just extract the performance values and the hyperparameters: instance$archive(unnest = &quot;params&quot;)[, c(&quot;cp&quot;, &quot;minsplit&quot;, &quot;classif.ce&quot;)] ## # A tibble: 20 x 3 ## cp minsplit classif.ce ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.1 8 0.230 ## 2 0.001 8 0.254 ## 3 0.1 10 0.230 ## 4 0.0505 1 0.230 ## 5 0.0258 1 0.230 ## 6 0.0258 8 0.230 ## 7 0.0753 8 0.230 ## 8 0.001 3 0.297 ## 9 0.001 1 0.297 ## 10 0.001 5 0.270 ## 11 0.1 1 0.230 ## 12 0.0753 10 0.230 ## 13 0.0505 5 0.230 ## 14 0.0505 10 0.230 ## 15 0.0258 10 0.230 ## 16 0.0505 3 0.230 ## 17 0.1 5 0.230 ## 18 0.0753 5 0.230 ## 19 0.0505 8 0.230 ## 20 0.0753 3 0.230 In sum, the grid search evaluated 20/25 different configurations of the grid in a random order before the Terminator stopped the tuning. Now the optimized hyperparameters can take the previously created Learner, set the returned hyperparameters and train it on the full dataset. learner$param_set$values = instance$result$params learner$train(task) The trained model can now be used to make a prediction on external data. Note that predicting on observations present in the task, should be avoided. The model has seen these observations already during tuning and therefore results would be statistically biased. Hence, the resulting performance measure would be over-optimistic. Instead, to get statistically unbiased performance estimates for the current task, nested resampling is required. 3.1.4 Automating the Tuning The AutoTuner wraps a learner and augments it with an automatic tuning for a given set of hyperparameters. Because the AutoTuner itself inherits from the Learner base class, it can be used like any other learner. Analogously to the previous subsection, a new classification tree learner is created. This classification tree learner automatically tunes the parameters cp and minsplit using an inner resampling (holdout). We create a terminator which allows 10 evaluations, and use a simple random search as tuning algorithm: library(&quot;paradox&quot;) library(&quot;mlr3tuning&quot;) learner = lrn(&quot;classif.rpart&quot;) resampling = rsmp(&quot;holdout&quot;) measures = msr(&quot;classif.ce&quot;) tune_ps = ParamSet$new(list( ParamDbl$new(&quot;cp&quot;, lower = 0.001, upper = 0.1), ParamInt$new(&quot;minsplit&quot;, lower = 1, upper = 10) )) terminator = term(&quot;evals&quot;, n_evals = 10) tuner = tnr(&quot;random_search&quot;) at = AutoTuner$new( learner = learner, resampling = resampling, measures = measures, tune_ps = tune_ps, terminator = terminator, tuner = tuner ) at ## &lt;AutoTuner:classif.rpart.tuned&gt; ## * Model: - ## * Parameters: xval=0 ## * Packages: rpart ## * Predict Type: response ## * Feature types: logical, integer, numeric, factor, ordered ## * Properties: importance, missings, multiclass, selected_features, ## twoclass, weights We can now use the learner like any other learner, calling the $train() and $predict() method. This time however, we pass it to benchmark() to compare the tuner to a classification tree without tuning. This way, the AutoTuner will do its resampling for tuning on the training set of the respective split of the outer resampling. The learner then undertakes predictions using the test set of the outer resampling. This yields unbiased performance measures, as the observations in the test set have not been used during tuning or fitting of the respective learner. This is called nested resampling. To compare the tuned learner with the learner using its default, we can use benchmark(): grid = benchmark_grid( task = tsk(&quot;pima&quot;), learner = list(at, lrn(&quot;classif.rpart&quot;)), resampling = rsmp(&quot;cv&quot;, folds = 3) ) bmr = benchmark(grid) bmr$aggregate(measures) ## # A tibble: 2 x 7 ## nr resample_result task_id learner_id resampling_id iters classif.ce ## &lt;int&gt; &lt;list&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 &lt;RsmplRsl&gt; pima classif.rpart.tu… cv 3 0.246 ## 2 2 &lt;RsmplRsl&gt; pima classif.rpart cv 3 0.263 Note that we do not expect any differences compared to the non-tuned approach for multiple reasons: the task is too easy the task is rather small, and thus prone to overfitting the tuning budget (10 evaluations) is small rpart does not benefit that much from tuning References "],
["fs.html", "3.2 Feature Selection / Filtering", " 3.2 Feature Selection / Filtering Often, data sets include a large number of features. The technique of extracting a subset of relevant features is called “feature selection”. The objective of feature selection is to fit the sparse dependent of a model on a subset of available data features in the most suitable manner. Feature selection can enhance the interpretability of the model, speed up the learning process and improve the learner performance. Different approaches exist to identify the relevant features. Two different approaches are emphasized in the literature: one is called Filtering and the other approach is often referred to as feature subset selection or wrapper methods. What are the differences (Chandrashekar and Sahin 2014)? Filtering: An external algorithm computes a rank of the variables (e.g. based on the correlation to the response). Then, features are subsetted by a certain criteria, e.g. an absolute number or a percentage of the number of variables. The selected features will then be used to fit a model (with optional hyperparameters selected by tuning). This calculation is usually cheaper than “feature subset selection” in terms of computation time. Wrapper Methods: Here, no ranking of features is done. Features are selected by a (random) subset of the data. Then, we fit a model and subsequently assess the performance. This is done for a lot of feature combinations in a cross-validation (CV) setting and the best combination is reported. This method is very computationally intensive as a lot of models are fitted. Also, strictly speaking all these models would need to be tuned before the performance is estimated. This would require an additional nested level in a CV setting. After undertaken all of these steps, the selected subset of features is again fitted (with optional hyperparameters selected by tuning). There is also a third approach which can be attributed to the “filter” family: The embedded feature-selection methods of some Learner. Read more about how to use these in section embedded feature-selection methods. Ensemble filters built upon the idea of stacking single filter methods. These are not yet implemented. All functionality that is related to feature selection is implemented via the extension package mlr3filters. 3.2.1 Filters Filter methods assign an importance value to each feature. Based on these values the features can be ranked. Thereafter, we are able to select a feature subset. There is a list of all implemented filter methods in the Appendix. 3.2.2 Calculating filter values Currently, only classification and regression tasks are supported. The first step it to create a new R object using the class of the desired filter method. Each object of class Filter has a .$calculate() method which calculates the filter values and ranks them in a descending order. library(&quot;mlr3filters&quot;) filter = FilterJMIM$new() task = tsk(&quot;iris&quot;) filter$calculate(task) as.data.table(filter) ## # A tibble: 4 x 2 ## feature score ## &lt;chr&gt; &lt;dbl&gt; ## 1 Sepal.Length 1.04 ## 2 Petal.Width 0.989 ## 3 Petal.Length 0.988 ## 4 Sepal.Width 0.831 Some filters support changing specific hyperparameters. This is done similar to setting hyperparameters of a Learner using .$param_set$values: filter_cor = FilterCorrelation$new() filter_cor$param_set ## ParamSet: ## id class lower upper ## 1: use ParamFct NA NA ## 2: method ParamFct NA NA ## levels ## 1: everything,all.obs,complete.obs,na.or.complete,pairwise.complete.obs ## 2: pearson,kendall,spearman ## default value ## 1: everything ## 2: pearson # change parameter &#39;method&#39; filter_cor$param_set$values = list(method = &quot;spearman&quot;) filter_cor$param_set ## ParamSet: ## id class lower upper ## 1: use ParamFct NA NA ## 2: method ParamFct NA NA ## levels ## 1: everything,all.obs,complete.obs,na.or.complete,pairwise.complete.obs ## 2: pearson,kendall,spearman ## default value ## 1: everything ## 2: pearson spearman Rather than taking the “long” R6 way to create a filter, there is also a built-in shorthand notation for filter creation: filter = flt(&quot;cmim&quot;) filter ## &lt;FilterCMIM:cmim&gt; ## Task Types: classif, regr ## Task Properties: - ## Packages: praznik ## Feature types: integer, numeric, factor, ordered 3.2.3 Variable Importance Filters All Learner with the property “importance” come with integrated feature selection methods. You can find a list of all learners with this property in the Appendix. For some learners the desired filter method needs to be set during learner creation. For example, learner classif.ranger (in the package mlr3learners) comes with multiple integrated methods. See the help page of ranger::ranger. To use method “impurity”, you need to set the filter method during construction. library(&quot;mlr3learners&quot;) lrn = lrn(&quot;classif.ranger&quot;, importance = &quot;impurity&quot;) Now you can use the mlr3filters::FilterImportance class for algorithm-embedded methods to filter a Task. library(&quot;mlr3learners&quot;) task = tsk(&quot;iris&quot;) filter = flt(&quot;importance&quot;, learner = lrn) filter$calculate(task) head(as.data.table(filter), 3) ## # A tibble: 3 x 2 ## feature score ## &lt;chr&gt; &lt;dbl&gt; ## 1 Petal.Length 43.8 ## 2 Petal.Width 42.6 ## 3 Sepal.Length 10.4 3.2.4 Ensemble Methods Work in progress. 3.2.5 Wrapper Methods Work in progress - via package mlr3fswrap References "],
["nested-resampling.html", "3.3 Nested Resampling", " 3.3 Nested Resampling In order to obtain unbiased performance estimates for learners, all parts of the model building (preprocessing and model selection steps) should be included in the resampling, i.e., repeated for every pair of training/test data. For steps that themselves require resampling like hyperparameter tuning or feature-selection (via the wrapper approach) this results in two nested resampling loops. The graphic above illustrates nested resampling for parameter tuning with 3-fold cross-validation in the outer and 4-fold cross-validation in the inner loop. In the outer resampling loop, we have three pairs of training/test sets. On each of these outer training sets parameter tuning is done, thereby executing the inner resampling loop. This way, we get one set of selected hyperparameters for each outer training set. Then the learner is fitted on each outer training set using the corresponding selected hyperparameters. Subsequently, we can evaluate the performance of the learner on the outer test sets. In mlr3, you can run nested resampling for free without programming any loops by using the mlr3tuning::AutoTuner class. This works as follows: Generate a wrapped Learner via class mlr3tuning::AutoTuner or mlr3filters::AutoSelect (not yet implemented). Specify all required settings - see section “Automating the Tuning” for help. Call function resample() or benchmark() with the created Learner. You can freely combine different inner and outer resampling strategies. A common setup is prediction and performance evaluation on a fixed outer test set. This can be achieved by passing the Resampling strategy (rsmp(&quot;holdout&quot;)) as the outer resampling instance to either resample() or benchmark(). The inner resampling strategy could be a cross-validation one (rsmp(&quot;cv&quot;)) as the sizes of the outer training sets might differ. Per default, the inner resample description is instantiated once for every outer training set. Note that nested resampling is computationally expensive. For this reason we use relatively small search spaces and a low number of resampling iterations in the examples shown below. In practice, you normally have to increase both. As this is computationally intensive you might want to have a look at the section on Parallelization. 3.3.1 Execution To optimize hyperparameters or conduct feature selection in a nested resampling you need to create learners using either: the AutoTuner class, or the mlr3filters::AutoSelect class (not yet implemented) We use the example from section “Automating the Tuning” and pipe the resulting learner into a resample() call. library(&quot;mlr3tuning&quot;) task = tsk(&quot;iris&quot;) learner = lrn(&quot;classif.rpart&quot;) resampling = rsmp(&quot;holdout&quot;) measures = msr(&quot;classif.ce&quot;) param_set = paradox::ParamSet$new( params = list(paradox::ParamDbl$new(&quot;cp&quot;, lower = 0.001, upper = 0.1))) terminator = term(&quot;evals&quot;, n_evals = 5) tuner = tnr(&quot;grid_search&quot;, resolution = 10) at = AutoTuner$new(learner, resampling, measures = measures, param_set, terminator, tuner = tuner) Now construct the resample() call: resampling_outer = rsmp(&quot;cv&quot;, folds = 3) rr = resample(task = task, learner = at, resampling = resampling_outer) 3.3.2 Evaluation With the created ResampleResult we can now inspect the executed resampling iterations more closely. See the section on Resampling for more detailed information about ResampleResult objects. For example, we can query the aggregated performance result: rr$aggregate() ## classif.ce ## 0.05333 Check for any errors in the folds during execution (if there is not output, warnings or errors recorded, this is an empty data.table(): rr$errors ## # A tibble: 0 x 2 ## # … with 2 variables: iteration &lt;int&gt;, msg &lt;chr&gt; Or take a look at the confusion matrix of the joined predictions: rr$prediction()$confusion ## truth ## response setosa versicolor virginica ## setosa 50 0 0 ## versicolor 0 48 6 ## virginica 0 2 44 "],
["pipelines.html", "4 Pipelines", " 4 Pipelines mlr3pipelines is a dataflow programming toolkit. This chapter focuses on the applicant’s side of the package. A more in-depth and technically oriented vignette can be found in the mlr3pipeline vignette. Machine learning workflows can be written as directed “Graphs”/“Pipelines” that represent data flows between preprocessing, model fitting, and ensemble learning units in an expressive and intuitive language. We will most often use the term “Graph” in this manual but it can interchangeably be used with “pipeline” or “workflow”. An example for such a graph can be found below: Single computational steps can be represented as so-called PipeOps, which can then be connected with directed edges in a Graph. The scope of mlr3pipelines is still growing. Currently supported features are: Data manipulation and preprocessing operations, e.g. PCA, feature filtering, imputation Task subsampling for speed and outcome class imbalance handling mlr3 Learner operations for prediction and stacking Ensemble methods and aggregation of predictions Additionally, we implement several meta operators that can be used to construct powerful pipelines: Simultaneous path branching (data going both ways) Alternative path branching (data going one specific way, controlled by hyperparameters) An extensive introduction to creating custom PipeOps (PO’s) can be found in the technical introduction. Using methods from mlr3tuning, it is even possible to simultaneously optimize parameters of multiple processing units. A predecessor to this package is the mlrCPO package, which works with mlr 2.x. Other packages that provide, to varying degree, some preprocessing functionality or machine learning domain specific language, are: the caret package and the related recipes project the dplyr package An example for a Pipeline that can be constructed using mlr3pipelines is depicted below: "],
["the-building-blocks-pipeops.html", "4.1 The Building Blocks: PipeOps", " 4.1 The Building Blocks: PipeOps The building blocks of mlr3pipelines are PipeOp-objects (PO). They can be constructed directly using PipeOp&lt;NAME&gt;$new(), but the recommended way is to retrieve them from the mlr_pipeops dictionary: library(&quot;mlr3pipelines&quot;) as.data.table(mlr_pipeops) ## # A tibble: 42 x 8 ## key packages input.num output.num input.type.train input.type.pred… ## &lt;chr&gt; &lt;list&gt; &lt;int&gt; &lt;int&gt; &lt;list&gt; &lt;list&gt; ## 1 boxc… &lt;chr [1… 1 1 &lt;chr [1]&gt; &lt;chr [1]&gt; ## 2 bran… &lt;chr [0… 1 NA &lt;chr [1]&gt; &lt;chr [1]&gt; ## 3 chunk &lt;chr [0… 1 NA &lt;chr [1]&gt; &lt;chr [1]&gt; ## 4 clas… &lt;chr [0… 1 1 &lt;chr [1]&gt; &lt;chr [1]&gt; ## 5 clas… &lt;chr [1… NA 1 &lt;chr [1]&gt; &lt;chr [1]&gt; ## 6 clas… &lt;chr [0… 1 1 &lt;chr [1]&gt; &lt;chr [1]&gt; ## 7 cola… &lt;chr [0… 1 1 &lt;chr [1]&gt; &lt;chr [1]&gt; ## 8 coll… &lt;chr [0… 1 1 &lt;chr [1]&gt; &lt;chr [1]&gt; ## 9 copy &lt;chr [0… 1 NA &lt;chr [1]&gt; &lt;chr [1]&gt; ## 10 enco… &lt;chr [1… 1 1 &lt;chr [1]&gt; &lt;chr [1]&gt; ## # … with 32 more rows, and 2 more variables: output.type.train &lt;list&gt;, ## # output.type.predict &lt;list&gt; Single POs can be created using mlr_pipeops$get(&lt;name&gt;): pca = mlr_pipeops$get(&quot;pca&quot;) or using syntactic sugar: pca = po(&quot;pca&quot;) Some POs require additional arguments for construction: learner = mlr_pipeops$get(&quot;learner&quot;) # Error in as_learner(learner) : argument &quot;learner&quot; is missing, with no default argument &quot;learner&quot; is missing, with no default learner = mlr_pipeops$get(&quot;learner&quot;, mlr_learners$get(&quot;classif.rpart&quot;)) or in short po(&quot;learner&quot;, lrn(&quot;classif.rpart&quot;)). Hyperparameters of POs can be set through the param_vals argument. Here we set the fraction of features for a filter: filter = mlr_pipeops$get(&quot;filter&quot;, filter = mlr3filters::FilterVariance$new(), param_vals = list(filter.frac = 0.5)) or in short notation: po(&quot;filter&quot;, mlr3filters::FilterVariance$new(), filter.frac = 0.5) The figure below shows an exemplary PipeOp. It takes an input, transforms it during .$train and .$predict and returns data: knitr::include_graphics(&quot;images/po_viz.png&quot;) "],
["pipe-operator.html", "4.2 The Pipeline Operator: %&gt;&gt;%", " 4.2 The Pipeline Operator: %&gt;&gt;% Although it is possible to create intricate Graphs with edges going all over the place (as long as no loops are introduced), there is usually a clear direction of flow between “layers” in the Graph. It is therefore convenient to build up a Graph from layers. This can be done using the %&gt;&gt;% (“double-arrow”) operator. It takes either a PipeOp or a Graph on each of its sides and connects all of the outputs of its left-hand side to one of the inputs each of its right-hand side. The number of inputs therefore must match the number of outputs. gr = mlr_pipeops$get(&quot;scale&quot;) %&gt;&gt;% mlr_pipeops$get(&quot;pca&quot;) gr$plot(html = TRUE) %&gt;% visInteraction(zoomView = FALSE) # disable zoom "],
["pipe-nodes-edges-graphs.html", "4.3 Nodes, Edges and Graphs", " 4.3 Nodes, Edges and Graphs POs are combined into Graphs. The manual way (= hard way) to construct a Graph is to create an empty graph first. Then one fills the empty graph with POs, and connects edges between the POs. POs are identified by their $id. Note that the operations all modify the object in-place and return the object itself. Therefore, multiple modifications can be chained. For this example we use the pca PO defined above and a new PO named “mutate”. The latter creates a new feature from existing variables. mutate = mlr_pipeops$get(&quot;mutate&quot;) graph = Graph$new()$ add_pipeop(mutate)$ add_pipeop(filter)$ add_edge(&quot;mutate&quot;, &quot;variance&quot;) # add connection mutate -&gt; filter The much quicker way is to use the %&gt;&gt;% operator to chain POs or Graph s. The same result as above can be achieved by doing the following: graph = mutate %&gt;&gt;% filter Now the Graph can be inspected using its $plot() function: graph$plot(html = TRUE) %&gt;% visInteraction(zoomView = FALSE) # disable zoom Chaining multiple POs of the same kind If multiple POs of the same kind should be chained, it is necessary to change the id to avoid name clashes. This can be done by either accessing the $id slot or during construction: graph$add_pipeop(mlr_pipeops$get(&quot;pca&quot;)) graph$add_pipeop(mlr_pipeops$get(&quot;pca&quot;, id = &quot;pca2&quot;)) "],
["pipe-modeling.html", "4.4 Modeling", " 4.4 Modeling The main purpose of a Graph is to build combined preprocessing and model fitting pipelines that can be used as mlr3 Learner. In the following we chain two preprocessing tasks: mutate (creation of a new feature) filter (filtering the dataset) and then chain a PO learner to train and predict on the modified dataset. graph = mutate %&gt;&gt;% filter %&gt;&gt;% mlr_pipeops$get(&quot;learner&quot;, learner = mlr_learners$get(&quot;classif.rpart&quot;)) Until here we defined the main pipeline stored in Graph. Now we can train and predict the pipeline: task = mlr_tasks$get(&quot;iris&quot;) graph$train(task) ## $classif.rpart.output ## NULL graph$predict(task) ## $classif.rpart.output ## &lt;PredictionClassif&gt; for 150 observations: ## row_id truth response ## 1 setosa setosa ## 2 setosa setosa ## 3 setosa setosa ## --- ## 148 virginica virginica ## 149 virginica virginica ## 150 virginica virginica Rather than calling $train() and $predict() manually, we can put the pipeline Graph into a GraphLearner object. A GraphLearner encapsulates the whole pipeline (including the preprocessing steps) and can be put into resample() or benchmark() . If you are familiar with the old mlr package, this is the equivalent of all the make*Wrapper() functions. The pipeline being encapsulated (here Graph ) must always produce a Prediction with its $predict() call, so it will probably contain at least one PipeOpLearner . glrn = GraphLearner$new(graph) This learner can be used for model fitting, resampling, benchmarking, and tuning: cv3 = rsmp(&quot;cv&quot;, folds = 3) resample(task, glrn, cv3) ## &lt;ResampleResult&gt; of 3 iterations ## * Task: iris ## * Learner: mutate.variance.classif.rpart ## * Warnings: 0 in 0 iterations ## * Errors: 0 in 0 iterations 4.4.1 Setting Hyperparameters Individual POs offer hyperparameters because they contain $param_set slots that can be read and written from $param_set$values (via the paradox package). The parameters get passed down to the Graph, and finally to the GraphLearner . This makes it not only possible to easily change the behavior of a Graph / GraphLearner and try different settings manually, but also to perform tuning using the mlr3tuning package. glrn$param_set$values$variance.filter.frac = 0.25 cv3 = rsmp(&quot;cv&quot;, folds = 3) resample(task, glrn, cv3) ## &lt;ResampleResult&gt; of 3 iterations ## * Task: iris ## * Learner: mutate.variance.classif.rpart ## * Warnings: 0 in 0 iterations ## * Errors: 0 in 0 iterations 4.4.2 Tuning If you are unfamiliar with tuning in mlr3, we recommend to take a look at the section about tuning first. Here we define a ParamSet for the “rpart” learner and the “variance” filter which should be optimized during tuning. library(&quot;paradox&quot;) ps = ParamSet$new(list( ParamDbl$new(&quot;classif.rpart.cp&quot;, lower = 0, upper = 0.05), ParamDbl$new(&quot;variance.filter.frac&quot;, lower = 0.25, upper = 1) )) After having defined the PerformanceEvaluator, a random search with 10 iterations is created. For the inner resampling, we are simply doing holdout (single split into train/test) to keep the runtimes reasonable. library(&quot;mlr3tuning&quot;) instance = TuningInstance$new( task = task, learner = glrn, resampling = rsmp(&quot;holdout&quot;), measures = msr(&quot;classif.ce&quot;), param_set = ps, terminator = term(&quot;evals&quot;, n_evals = 20) ) tuner = TunerRandomSearch$new() tuner$tune(instance) The tuning result can be found in the result slot. instance$result "],
["pipe-nonlinear.html", "4.5 Non-Linear Graphs", " 4.5 Non-Linear Graphs The Graphs seen so far all have a linear structure. Some POs may have multiple input or output channels. These make it possible to create non-linear Graphs with alternative paths taken by the data. Possible types are: Branching: Splitting of a node into several paths, e.g. useful when comparing multiple feature-selection methods (pca, filters). Only one path will be executed. Copying: Splitting of a node into several paths, all paths will be executed (sequentially). Parallel execution is not yet supported. Stacking: Single graphs are stacked onto each other, i.e. the output of one Graph is the input for another. In machine learning this means that the prediction of one Graph is used as input for another Graph 4.5.1 Branching &amp; Copying The PipeOpBranch and PipeOpUnbranch POs make it possible to specify multiple alternative paths. Only one is actually executed, the others are ignored. The active path is determined by a hyperparameter. This concept makes it possible to tune alternative preprocessing paths (or learner models). PipeOp(Un)Branch is initialized either with the number of branches, or with a character-vector indicating the names of the branches. If names are given, the “branch-choosing” hyperparameter becomes more readable. In the following, we set three options: Doing nothing (“nop”) Applying a PCA Scaling the data It is important to “unbranch” again after “branching”, so that the outputs are merged into one result objects. In the following we first create the branched graph and then show what happens if the “unbranching” is not applied: branch_graph = mlr_pipeops$get(&quot;branch&quot;, c(&quot;nop&quot;, &quot;pca&quot;, &quot;scale&quot;)) %&gt;&gt;% gunion(list( mlr_pipeops$get(&quot;nop&quot;, id = &quot;null1&quot;), mlr_pipeops$get(&quot;pca&quot;), mlr_pipeops$get(&quot;scale&quot;) )) Without “unbranching” one creates the following graph: branch_graph$plot(html=TRUE) Now when “unbranching”, we obtain the following results: (branch_graph %&gt;&gt;% mlr_pipeops$get(&quot;unbranch&quot;, c(&quot;nop&quot;, &quot;pca&quot;, &quot;scale&quot;)))$plot(html = TRUE) %&gt;% visInteraction(zoomView = FALSE) # disable zoom The same can be achieved using a shorter notation: # List of pipeops opts = list(po(&quot;nop&quot;, &quot;no_op&quot;), po(&quot;pca&quot;), po(&quot;scale&quot;)) # List of po ids opt_ids = mlr3misc::map_chr(opts, `[[`, &quot;id&quot;) po(&quot;branch&quot;, options = opt_ids) %&gt;&gt;% gunion(opts) %&gt;&gt;% po(&quot;unbranch&quot;, options = opt_ids) ## Graph with 5 PipeOps: ## ID State sccssors prdcssors ## branch &lt;&lt;UNTRAINED&gt;&gt; no_op,pca,scale ## no_op &lt;&lt;UNTRAINED&gt;&gt; unbranch branch ## pca &lt;&lt;UNTRAINED&gt;&gt; unbranch branch ## scale &lt;&lt;UNTRAINED&gt;&gt; unbranch branch ## unbranch &lt;&lt;UNTRAINED&gt;&gt; no_op,pca,scale 4.5.2 Model Ensembles We can leverage the different operations presented to connect POs. This allows us to form powerful graphs. Before we go into details, we split the task into train and test indices. task = mlr_tasks$get(&quot;iris&quot;) train.idx = sample(seq_len(task$nrow), 120) test.idx = setdiff(seq_len(task$nrow), train.idx) 4.5.2.1 Bagging We first examine Bagging introduced by (Breiman 1996). The basic idea is to create multiple predictors and then aggregate those to a single, more powerful predictor. “… multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets” (Breiman 1996) Bagging then aggregates a set of predictors by averaging (regression) or majority vote (classification). The idea behind bagging is, that a set of weak, but different predictors can be combined in order to arrive at a single, better predictor. We can achieve this by downsampling our data before training a learner, repeating this e.g. 10 times and then performing a majority vote on the predictions. First, we create a simple pipeline, that uses PipeOpSubsample before a PipeOpLearner is trained: single_pred = PipeOpSubsample$new(param_vals = list(frac = 0.7)) %&gt;&gt;% PipeOpLearner$new(mlr_learners$get(&quot;classif.rpart&quot;)) We can now copy this operation 10 times using greplicate . pred_set = greplicate(single_pred, 10L) Afterwards we need to aggregate the 10 pipelines to form a single model: bagging = pred_set %&gt;&gt;% PipeOpClassifAvg$new(innum = 10L) Now we can plot again to see what happens: bagging$plot(html = TRUE) This pipeline can again be used in conjunction with GraphLearner in order for Bagging to be used like a Learner: baglrn = GraphLearner$new(bagging) baglrn$train(task, train.idx) baglrn$predict(task, test.idx) ## &lt;PredictionClassif&gt; for 30 observations: ## row_id truth response ## 6 setosa setosa ## 21 setosa setosa ## 22 setosa setosa ## --- ## 136 virginica virginica ## 138 virginica virginica ## 147 virginica virginica In conjunction with different Backends, this can be a very powerful tool. In cases when the data does not fully fit in memory, one can obtain a fraction of the data for each learner from a DataBackend and then aggregate predictions over all learners. 4.5.2.2 Stacking Stacking (Wolpert 1992) is another technique that can improve model performance. The basic idea behind stacking is the use of predictions from one model as features for a subsequent model to possibly improve performance. As an example we can train a decision tree and use the predictions from this model in conjunction with the original features in order to train an additional model on top. To limit overfitting, we additionally do not predict on the original predictions of the learner. Instead, we predict on out-of-bag predictions. To do all this, we can use PipeOpLearnerCV . PipeOpLearnerCV performs nested cross-validation on the training data, fitting a model in each fold. Each of the models is then used to predict on the out-of-fold data. As a result, we obtain predictions for every data point in our input data. We first create a “level 0” learner, which is used to extract a lower level prediction. Additionally, we clone() the learner object to obtain a copy of the learner. Subsequently, one sets a custom id for the PipeOp . lrn = mlr_learners$get(&quot;classif.rpart&quot;) lrn_0 = PipeOpLearnerCV$new(lrn$clone()) lrn_0$id = &quot;rpart_cv&quot; We use PipeOpNOP in combination with gunion, in order to send the unchanged Task to the next level. There it is combined with the predictions from our decision tree learner. level_0 = gunion(list(lrn_0, PipeOpNOP$new())) Afterwards, we want to concatenate the predictions from PipeOpLearnerCV and the original Task using PipeOpFeatureUnion : combined = level_0 %&gt;&gt;% PipeOpFeatureUnion$new(2) Now we can train another learner on top of the combined features: stack = combined %&gt;&gt;% PipeOpLearner$new(lrn$clone()) vn = stack$plot(html = TRUE) visNetwork::visInteraction(vn, zoomView = FALSE) # disable zoom stacklrn = GraphLearner$new(stack) stacklrn$train(task, train.idx) stacklrn$predict(task, test.idx) In this vignette, we showed a very simple use-case for stacking. In many real-world applications, stacking is done for multiple levels and on multiple representations of the dataset. On a lower level, different preprocessing methods can be defined in conjunction with several learners. On a higher level, we can then combine those predictions in order to form a very powerful model. 4.5.2.3 Multilevel Stacking In order to showcase the power of mlr3pipelines, we will show a more complicated stacking example. In this case, we train a glmnet and 2 different rpart models (some transform its inputs using PipeOpPCA) on our task in the “level 0” and concatenate them with the original features (via gunion. The result is then passed on to “level 1”, where we copy the concatenated features 3 times and put this task into an rpart and a glmnet model. Additionally, we keep a version of the “level 0” output (via PipeOpNOP) and pass this on to “level 2”. In “level 2” we simply concatenate all “level 1” outputs and train a final decision tree. In the following examples, use &lt;lrn&gt;$param_set$values$&lt;param_name&gt; = &lt;param_value&gt; to set hyperparameters for the different learner. library(&quot;mlr3learners&quot;) # for classif.glmnet rprt = lrn(&quot;classif.rpart&quot;, predict_type = &quot;prob&quot;) glmn = lrn(&quot;classif.glmnet&quot;, predict_type = &quot;prob&quot;) ## Warning: Package &#39;glmnet&#39; required but not installed for Learner ## &#39;classif.glmnet&#39; # Create Learner CV Operators lrn_0 = PipeOpLearnerCV$new(rprt, id = &quot;rpart_cv_1&quot;) lrn_0$param_set$values$maxdepth = 5L lrn_1 = PipeOpPCA$new(id = &quot;pca1&quot;) %&gt;&gt;% PipeOpLearnerCV$new(rprt, id = &quot;rpart_cv_2&quot;) lrn_1$param_set$values$rpart_cv_2.maxdepth = 1L lrn_2 = PipeOpPCA$new(id = &quot;pca2&quot;) %&gt;&gt;% PipeOpLearnerCV$new(glmn) # Union them with a PipeOpNULL to keep original features level_0 = gunion(list(lrn_0, lrn_1,lrn_2, PipeOpNOP$new(id = &quot;NOP1&quot;))) # Cbind the output 3 times, train 2 learners but also keep level # 0 predictions level_1 = level_0 %&gt;&gt;% PipeOpFeatureUnion$new(4) %&gt;&gt;% PipeOpCopy$new(3) %&gt;&gt;% gunion(list( PipeOpLearnerCV$new(rprt, id = &quot;rpart_cv_l1&quot;), PipeOpLearnerCV$new(glmn, id = &quot;glmnt_cv_l1&quot;), PipeOpNOP$new(id = &quot;NOP_l1&quot;) )) # Cbind predictions, train a final learner level_2 = level_1 %&gt;&gt;% PipeOpFeatureUnion$new(3, id = &quot;u2&quot;) %&gt;&gt;% PipeOpLearner$new(rprt, id = &quot;rpart_l2&quot;) # Plot the resulting graph vn = level_2$plot(html = TRUE) visNetwork::visInteraction(vn, zoomView = FALSE) # disable zoom task = tsk(&quot;iris&quot;) lrn = GraphLearner$new(level_2) And we can again call .$train and .$predict lrn$ train(task, train.idx)$ predict(task, test.idx)$ score() References "],
["pipe-special-ops.html", "4.6 Special Operators", " 4.6 Special Operators This section introduces some special operators, that might be useful in many applications. 4.6.1 Imputation: PipeOpImpute An often occurring setting is the imputation of missing data. Imputation methods range from relatively simple imputation using either mean, median or histograms to way more involved methods including using machine learning algorithms in order to predict missing values. The following PipeOp, PipeOpImpute, imputes numeric values from a histogram, adds a new level for factors and additionally adds a column marking whether a value for a given feature was missing or not. pom = PipeOpMissInd$new() pon = PipeOpImputeHist$new(id = &quot;imputer_num&quot;, param_vals = list(affect_columns = is.numeric)) pof = PipeOpImputeNewlvl$new(id = &quot;imputer_fct&quot;, param_vals = list(affect_columns = is.factor)) imputer = pom %&gt;&gt;% pon %&gt;&gt;% pof A learner can thus be equipped with automatic imputation of missing values by adding an imputation Pipeop. polrn = PipeOpLearner$new(mlr_learners$get(&quot;classif.rpart&quot;)) lrn = GraphLearner$new(graph = imputer %&gt;&gt;% polrn) 4.6.2 Feature Engineering: PipeOpMutate New features can be added or computed from a task using PipeOpMutate . The operator evaluates one or multiple expressions provided in an alist. In this example, we compute some new features on top of the iris task. Then we add them to the data as illustrated below: pom = PipeOpMutate$new() # Define a set of mutations mutations = list( Sepal.Sum = ~ Sepal.Length + Sepal.Width, Petal.Sum = ~ Petal.Length + Petal.Width, Sepal.Petal.Ratio = ~ (Sepal.Length / Petal.Length) ) pom$param_set$values$mutation = mutations If outside data is required, we can make use of the env parameter. Moreover, we provide an environment, where expressions are evaluated (env defaults to .GlobalEnv). 4.6.3 Training on data subsets: PipeOpChunk In cases, where data is too big to fit into the machine’s memory, an often-used technique is to split the data into several parts. Subsequently, the parts are trained on each part of the data. After undertaking these steps, we aggregate the models. In this example, we split our data into 4 parts using PipeOpChunk . Additionally, we create 4 PipeOpLearner POS, which are then trained on each split of the data. chks = PipeOpChunk$new(4) lrns = greplicate(PipeOpLearner$new(mlr_learners$get(&quot;classif.rpart&quot;)), 4) Afterwards we can use PipeOpClassifAvg to aggregate the predictions from the 4 different models into a new one. mjv = PipeOpClassifAvg$new(4) We can now connect the different operators and visualize the full graph: pipeline = chks %&gt;&gt;% lrns %&gt;&gt;% mjv pipeline$plot(html = TRUE) %&gt;% visInteraction(zoomView = FALSE) # disable zoom pipelrn = GraphLearner$new(pipeline) pipelrn$train(task, train.idx)$ predict(task, train.idx)$ score() ## classif.ce ## 0.2 4.6.4 Feature Selection: PipeOpFilter and PipeOpSelect The package mlr3filters contains many different mlr3filters::Filters that can be used to select features for subsequent learners. This is often required when the data has a large amount of features. A PipeOp for filters is PipeOpFilter: PipeOpFilter$new(mlr3filters::FilterInformationGain$new()) ## PipeOp: &lt;information_gain&gt; (not trained) ## values: &lt;list()&gt; ## Input channels &lt;name [train type, predict type]&gt;: ## input [Task,Task] ## Output channels &lt;name [train type, predict type]&gt;: ## output [Task,Task] How many features to keep can be set using filter_nfeat, filter_frac and filter_cutoff. Filters can be selected / de-selected by name using PipeOpSelect . "],
["advanced-an-in-depth-look-into-mlr3pipelines.html", "4.7 Advanced: An in-depth look into mlr3pipelines", " 4.7 Advanced: An in-depth look into mlr3pipelines This vignette is an in-depth introduction to mlr3pipelines, the dataflow programming toolkit for machine learning in R using mlr3. It will go through basic concepts and then give a few examples that both show the simplicity as well as the power and versatility of using mlr3pipelines. 4.7.1 What’s the Point Machine learning toolkits often try to abstract away the processes happening inside machine learning algorithms. This makes it easy for the user to switch out one algorithm for another without having to worry about what is happening inside it, what kind of data it is able to operate on etc. The benefit of using mlr3, for example, is that one can create a Learner, a Task, a Resampling etc. and use them for typical machine learning operations. It is trivial to exchange individual components and therefore use, for example, a different Learner in the same experiment for comparison. task = TaskClassif$new(&quot;iris&quot;, as_data_backend(iris), &quot;Species&quot;) lrn = mlr_learners$get(&quot;classif.rpart&quot;) rsmp = mlr_resamplings$get(&quot;holdout&quot;) resample(task, lrn, rsmp) ## &lt;ResampleResult&gt; of 1 iterations ## * Task: iris ## * Learner: classif.rpart ## * Warnings: 0 in 0 iterations ## * Errors: 0 in 0 iterations However, this modularity breaks down as soon as the learning algorithm encompasses more than just model fitting, like data preprocessing, ensembles or other meta models. mlr3pipelines takes modularity one step further than mlr3: it makes it possible to build individual steps within a Learner out of building blocks called PipeOps. 4.7.2 PipeOp: Pipeline Operators The most basic unit of functionality within mlr3pipelines is the PipeOp, short for “pipeline operator”, which represents a trans-formative operation on input (for example a training dataset) leading to output. It can therefore be seen as a generalized notion of a function, with a certain twist: PipeOps behave differently during a “training phase” and a “prediction phase”. The training phase will typically generate a certain model of the data that is saved as internal state. The prediction phase will then operate on the input data depending on the trained model. An example of this behavior is the principal component analysis operation (“PipeOpPCA”): During training, it will transform incoming data by rotating it in a way that leads to uncorrelated features ordered by their contribution to total variance. It will also save the rotation matrix to be used during for new data. This makes it possible to perform “prediction” with single rows of new data, where a row’s scores on each of the principal components (the components of the training data!) is computed. po = mlr_pipeops$get(&quot;pca&quot;) po$train(list(task))[[1]]$data() ## # A tibble: 150 x 5 ## Species PC1 PC2 PC3 PC4 ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa -2.68 0.319 -0.0279 -0.00226 ## 2 setosa -2.71 -0.177 -0.210 -0.0990 ## 3 setosa -2.89 -0.145 0.0179 -0.0200 ## 4 setosa -2.75 -0.318 0.0316 0.0756 ## 5 setosa -2.73 0.327 0.0901 0.0613 ## 6 setosa -2.28 0.741 0.169 0.0242 ## 7 setosa -2.82 -0.0895 0.258 0.0481 ## 8 setosa -2.63 0.163 -0.0219 0.0453 ## 9 setosa -2.89 -0.578 0.0208 0.0267 ## 10 setosa -2.67 -0.114 -0.198 0.0563 ## # … with 140 more rows single_line_task = task$clone()$filter(1) po$predict(list(single_line_task))[[1]]$data() ## # A tibble: 1 x 5 ## Species PC1 PC2 PC3 PC4 ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa -2.68 0.319 -0.0279 -0.00226 po$state ## Standard deviations (1, .., p=4): ## [1] 2.0563 0.4926 0.2797 0.1544 ## ## Rotation (n x k) = (4 x 4): ## PC1 PC2 PC3 PC4 ## Petal.Length 0.85667 -0.17337 0.07624 0.4798 ## Petal.Width 0.35829 -0.07548 0.54583 -0.7537 ## Sepal.Length 0.36139 0.65659 -0.58203 -0.3155 ## Sepal.Width -0.08452 0.73016 0.59791 0.3197 This shows the most important primitives incorporated in a PipeOp: * $train(), taking a list of input arguments, turning them into a list of outputs, meanwhile saving a state in $state * $predict(), taking a list of input arguments, turning them into a list of outputs, making use of the saved $state * $state, the “model” trained with $train() and utilized during $predict(). Schematically we can represent the PipeOp like so: 4.7.2.1 Why the $state It is important to take a moment and notice the importance of a $state variable and the $train() / $predict() dichotomy in a PipeOp. There are many preprocessing methods, for example scaling of parameters or imputation, that could in theory just be applied to training data and prediction / validation data separately, or they could be applied to a task before resampling is performed. This would, however, be fallacious: The preprocessing of each instance of prediction data should not depend on the remaining prediction dataset. A prediction on a single instance of new data should give the same result as prediction performed on a whole dataset. If preprocessing is performed on a task before resampling is done, information about the test set can leak into the training set. Resampling should evaluate the generalization performance of the entire machine learning method, therefore the behavior of this entire method must only depend only on the content of the training split during resampling. 4.7.2.2 Where to Get PipeOps Each PipeOp is an instance of an “R6” class, many of which are provided by the mlr3pipelines package itself. They can be constructed explicitly (“PipeOpPCA$new()”) or retrieved from the mlr_pipelines collection: mlr_pipeops$get(&quot;pca&quot;). The entire list of available PipeOps, and some meta-information, can be retrieved using as.data.table(): as.data.table(mlr_pipeops)[, c(&quot;key&quot;, &quot;input.num&quot;, &quot;output.num&quot;)] ## # A tibble: 42 x 3 ## key input.num output.num ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 boxcox 1 1 ## 2 branch 1 NA ## 3 chunk 1 NA ## 4 classbalancing 1 1 ## 5 classifavg NA 1 ## 6 classweights 1 1 ## 7 colapply 1 1 ## 8 collapsefactors 1 1 ## 9 copy 1 NA ## 10 encode 1 1 ## # … with 32 more rows When retrieving PipeOps from the mlr_pipeops dictionary, it is also possible to give additional constructor arguments, such as an id or parameter values. mlr_pipeops$get(&quot;pca&quot;, param_vals = list(rank. = 3)) ## PipeOp: &lt;pca&gt; (not trained) ## values: &lt;rank.=3&gt; ## Input channels &lt;name [train type, predict type]&gt;: ## input [Task,Task] ## Output channels &lt;name [train type, predict type]&gt;: ## output [Task,Task] 4.7.3 PipeOp Channels 4.7.3.1 Input Channels Just like functions, PipeOps can take multiple inputs. These multiple inputs are always given as elements in the input list. For example, there is a PipeOpFeatureUnion that combines multiple tasks with different features and “cbind()s” them together, creating one combined task. When two halves of the iris task are given, for example, it recreates the original task: iris_first_half = task$clone()$select(c(&quot;Petal.Length&quot;, &quot;Petal.Width&quot;)) iris_second_half = task$clone()$select(c(&quot;Sepal.Length&quot;, &quot;Sepal.Width&quot;)) pofu = mlr_pipeops$get(&quot;featureunion&quot;, innum = 2) pofu$train(list(iris_first_half, iris_second_half))[[1]]$data() ## # A tibble: 150 x 5 ## Species Petal.Length Petal.Width Sepal.Length Sepal.Width ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 1.4 0.2 5.1 3.5 ## 2 setosa 1.4 0.2 4.9 3 ## 3 setosa 1.3 0.2 4.7 3.2 ## 4 setosa 1.5 0.2 4.6 3.1 ## 5 setosa 1.4 0.2 5 3.6 ## 6 setosa 1.7 0.4 5.4 3.9 ## 7 setosa 1.4 0.3 4.6 3.4 ## 8 setosa 1.5 0.2 5 3.4 ## 9 setosa 1.4 0.2 4.4 2.9 ## 10 setosa 1.5 0.1 4.9 3.1 ## # … with 140 more rows Because PipeOpFeatureUnion effectively takes two input arguments here, we can say it has two input channels. An input channel also carries information about the type of input that is acceptable. The input channels of the pofu object constructed above, for example, each accept a Task during training and prediction. This information can be queried from the $input slot: pofu$input ## # A tibble: 2 x 3 ## name train predict ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 input1 Task Task ## 2 input2 Task Task Other PipeOps may have channels that take different types during different phases. The backuplearner PipeOp, for example, takes a NULL and a Task during training, and a Prediction and a Task during prediction: ## TODO this is an important case to handle here, do not delete unless there is a better example. ## mlr_pipeops$get(&quot;backuplearner&quot;)$input 4.7.3.2 Output Channels Unlike the typical notion of a function, PipeOps can also have multiple output channels. $train() and $predict() always return a list, so certain PipeOps may return lists with more than one element. Similar to input channels, the information about the number and type of outputs given by a PipeOp is available in the $output slot. The chunk PipeOp, for example, chunks a given Task into subsets and consequently returns multiple Task objects, both during training and prediction. The number of output channels must be given during construction through the outnum argument. mlr_pipeops$get(&quot;chunk&quot;, outnum = 3)$output ## # A tibble: 3 x 3 ## name train predict ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 output1 Task Task ## 2 output2 Task Task ## 3 output3 Task Task Note that the number of output channels during training and prediction is the same. A schema of a PipeOp with two output channels: 4.7.3.3 Channel Configuration Most PipeOps have only one input channel (so they take a list with a single element), but there are a few with more than one; In many cases, the number of input or output channels is determined during construction, e.g. through the innum / outnum arguments. The input.num and output.num columns of the mlr_pipeops-table above show the default number of channels, and NA if the number depends on a construction argument. The default printer of a PipeOp gives information about channel names and types: ## mlr_pipeops$get(&quot;backuplearner&quot;) 4.7.4 Graph: Networks of PipeOps 4.7.4.1 Basics What is the advantage of this tedious way of declaring input and output channels and handling in/output through lists? Because each PipeOp has a known number of input and output channels that always produce or accept data of a known type, it is possible to network them together in Graphs. A Graph is a collection of PipeOps with “edges” that mandate that data should be flowing along them. Edges always pass between PipeOp channels, so it is not only possible to explicitly prescribe which position of an input or output list an edge refers to, it makes it possible to make different components of a PipeOp’s output flow to multiple different other PipeOps, as well as to have a PipeOp gather its input from multiple other PipeOps. A schema of a simple graph of PipeOps: A Graph is empty when first created, and PipeOps can be added using the $add_pipeop() method. The $add_edge() method is used to create connections between them. While the printer of a Graph gives some information about its layout, the most intuitive way of visualizing it is using the $plot() function. gr = Graph$new() gr$add_pipeop(mlr_pipeops$get(&quot;scale&quot;)) gr$add_pipeop(mlr_pipeops$get(&quot;subsample&quot;, param_vals = list(frac = 0.1))) gr$add_edge(&quot;scale&quot;, &quot;subsample&quot;) print(gr) ## Graph with 2 PipeOps: ## ID State sccssors prdcssors ## scale &lt;&lt;UNTRAINED&gt;&gt; subsample ## subsample &lt;&lt;UNTRAINED&gt;&gt; scale gr$plot(html = TRUE) A Graph itself has a $train() and a $predict() method that accept some data and propagate this data through the network of PipeOps. The return value corresponds to the output of the PipeOp output channels that are not connected to other PipeOps. gr$train(task)[[1]]$data() ## # A tibble: 15 x 5 ## Species Petal.Length Petal.Width Sepal.Length Sepal.Width ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa -1.17 -1.31 -0.535 0.786 ## 2 setosa -1.45 -1.31 -1.02 0.327 ## 3 setosa -1.28 -1.31 -0.656 1.47 ## 4 versicolor 0.137 0.132 0.310 -0.590 ## 5 versicolor 0.307 0.132 0.672 -0.361 ## 6 versicolor -0.146 -0.262 -0.173 -1.05 ## 7 versicolor 0.137 0.000875 -0.0523 -1.05 ## 8 virginica 1.44 0.788 1.76 -0.361 ## 9 virginica 0.987 1.18 1.16 -0.132 ## 10 virginica 0.647 0.788 0.551 -0.820 ## 11 virginica 1.50 1.05 2.48 1.70 ## 12 virginica 0.760 0.394 0.551 -0.590 ## 13 virginica 1.33 1.44 2.24 -0.132 ## 14 virginica 0.817 1.05 0.793 -0.132 ## 15 virginica 0.760 0.788 0.0684 -0.132 gr$predict(single_line_task)[[1]]$data() ## # A tibble: 1 x 5 ## Species Petal.Length Petal.Width Sepal.Length Sepal.Width ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa -1.34 -1.31 -0.898 1.02 The collection of PipeOps inside a Graph can be accessed through the $pipeops slot. The set of edges in the Graph can be inspected through the $edges slot. It is possible to modify individual PipeOps and edges in a Graph through these slots, but this is not recommended because no error checking is performed and it may put the Graph in an unsupported state. 4.7.4.2 Networks The example above showed a linear preprocessing pipeline, but it is in fact possible to build true “graphs” of operations, as long as no loops are introduced1. PipeOps with multiple output channels can feed their data to multiple different subsequent PipeOps, and PipeOps with multiple input channels can take results from different PipeOps. When a PipeOp has more than one input / output channel, then the Graph’s $add_edge() method needs an additional argument that indicates which channel to connect to. This argument can be given in the form of an integer, or as the name of the channel. The following constructs a Graph that copies the input and gives one copy each to a “scale” and a “pca” PipeOp. The resulting columns of each operation are put next to each other by “featureunion”. gr = Graph$new()$ add_pipeop(mlr_pipeops$get(&quot;copy&quot;, outnum = 2))$ add_pipeop(mlr_pipeops$get(&quot;scale&quot;))$ add_pipeop(mlr_pipeops$get(&quot;pca&quot;))$ add_pipeop(mlr_pipeops$get(&quot;featureunion&quot;, innum = 2)) gr$ add_edge(&quot;copy&quot;, &quot;scale&quot;, src_channel = 1)$ ## designating channel by index add_edge(&quot;copy&quot;, &quot;pca&quot;, src_channel = &quot;output2&quot;)$ ## designating channel by name add_edge(&quot;scale&quot;, &quot;featureunion&quot;, dst_channel = 1)$ add_edge(&quot;pca&quot;, &quot;featureunion&quot;, dst_channel = 2) gr$plot(html = TRUE) gr$train(iris_first_half)[[1]]$data() ## # A tibble: 150 x 5 ## Species Petal.Length Petal.Width PC1 PC2 ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa -1.34 -1.31 -2.56 -0.00692 ## 2 setosa -1.34 -1.31 -2.56 -0.00692 ## 3 setosa -1.39 -1.31 -2.65 0.0318 ## 4 setosa -1.28 -1.31 -2.47 -0.0457 ## 5 setosa -1.34 -1.31 -2.56 -0.00692 ## 6 setosa -1.17 -1.05 -2.21 0.0611 ## 7 setosa -1.34 -1.18 -2.52 0.0853 ## 8 setosa -1.28 -1.31 -2.47 -0.0457 ## 9 setosa -1.34 -1.31 -2.56 -0.00692 ## 10 setosa -1.28 -1.44 -2.51 -0.138 ## # … with 140 more rows 4.7.4.3 Syntactic Sugar Although it is possible to create intricate Graphs with edges going all over the place (as long as no loops are introduced), there is usually a clear direction of flow between “layers” in the Graph. It is therefore convenient to build up a Graph from layers, which can be done using the %&gt;&gt;% (“double-arrow”) operator. It takes either a PipeOp or a Graph on each of its sides and connects all of the outputs of its left-hand side to one of the inputs each of its right-hand side–the number of inputs therefore must match the number of outputs. Together with the gunion() operation, which takes PipeOps or Graphs and arranges them next to each other akin to a (disjoint) graph union, the above network can more easily be constructed as follows: gr = mlr_pipeops$get(&quot;copy&quot;, outnum = 2) %&gt;&gt;% gunion(list(mlr_pipeops$get(&quot;scale&quot;), mlr_pipeops$get(&quot;pca&quot;))) %&gt;&gt;% mlr_pipeops$get(&quot;featureunion&quot;, innum = 2) gr$plot(html = TRUE) 4.7.4.4 PipeOp IDs and ID Name Clashes PipeOps within a graph are addressed by their $id-slot. It is therefore necessary for all PipeOps within a Graph to have a unique $id. The $id can be set during or after construction, but it should not directly be changed after a PipeOp was inserted in a Graph. At that point, the $set_names()-method can be used to change PipeOp ids. po1 = mlr_pipeops$get(&quot;scale&quot;) po2 = mlr_pipeops$get(&quot;scale&quot;) po1 %&gt;&gt;% po2 ## name clash ## Error in gunion(list(g1, g2)): Assertion on &#39;ids of pipe operators&#39; failed: Must have unique names, but element 2 is duplicated. po2$id = &quot;scale2&quot; gr = po1 %&gt;&gt;% po2 gr ## Graph with 2 PipeOps: ## ID State sccssors prdcssors ## scale &lt;&lt;UNTRAINED&gt;&gt; scale2 ## scale2 &lt;&lt;UNTRAINED&gt;&gt; scale ## Alternative ways of getting new ids: mlr_pipeops$get(&quot;scale&quot;, id = &quot;scale2&quot;) ## PipeOp: &lt;scale2&gt; (not trained) ## values: &lt;list()&gt; ## Input channels &lt;name [train type, predict type]&gt;: ## input [Task,Task] ## Output channels &lt;name [train type, predict type]&gt;: ## output [Task,Task] PipeOpScale$new(id = &quot;scale2&quot;) ## PipeOp: &lt;scale2&gt; (not trained) ## values: &lt;list()&gt; ## Input channels &lt;name [train type, predict type]&gt;: ## input [Task,Task] ## Output channels &lt;name [train type, predict type]&gt;: ## output [Task,Task] ## sometimes names of PipeOps within a Graph need to be changed gr2 = mlr_pipeops$get(&quot;scale&quot;) %&gt;&gt;% mlr_pipeops$get(&quot;pca&quot;) gr %&gt;&gt;% gr2 ## Error in gunion(list(g1, g2)): Assertion on &#39;ids of pipe operators&#39; failed: Must have unique names, but element 3 is duplicated. gr2$set_names(&quot;scale&quot;, &quot;scale3&quot;) gr %&gt;&gt;% gr2 ## Graph with 4 PipeOps: ## ID State sccssors prdcssors ## scale &lt;&lt;UNTRAINED&gt;&gt; scale2 ## scale2 &lt;&lt;UNTRAINED&gt;&gt; scale3 scale ## scale3 &lt;&lt;UNTRAINED&gt;&gt; pca scale2 ## pca &lt;&lt;UNTRAINED&gt;&gt; scale3 4.7.5 Learners in Graphs, Graphs in Learners The true power of mlr3pipelines derives from the fact that it can be integrated seamlessly with mlr3. Two components are mainly responsible for this: PipeOpLearner, a PipeOp that encapsulates a mlr3 Learner and creates a PredictionData object in its $predict() phase GraphLearner, a mlr3 Learner that can be used in place of any other mlr3 Learner, but which does prediction using a Graph given to it Note that these are dual to each other: One takes a Learner and produces a PipeOp (and by extension a Graph); the other takes a Graph and produces a Learner. 4.7.5.1 PipeOpLearner The PipeOpLearner is constructed using a mlr3 Learner and will use it to create PredictionData in the $predict() phase. The output during $train() is NULL. It can be used after a preprocessing pipeline, and it is even possible to perform operations on the PredictionData, for example by averaging multiple predictions or by using the “PipeOpBackupLearner” operator to impute predictions that a given model failed to create. The following is a very simple Graph that performs training and prediction on data after performing principal component analysis. gr = mlr_pipeops$get(&quot;pca&quot;) %&gt;&gt;% mlr_pipeops$get(&quot;learner&quot;, mlr_learners$get(&quot;classif.rpart&quot;)) gr$train(task) ## $classif.rpart.output ## NULL gr$predict(task) ## $classif.rpart.output ## &lt;PredictionClassif&gt; for 150 observations: ## row_id truth response ## 1 setosa setosa ## 2 setosa setosa ## 3 setosa setosa ## --- ## 148 virginica virginica ## 149 virginica virginica ## 150 virginica virginica 4.7.5.2 GraphLearner Although a Graph has $train() and $predict() functions, it can not be used directly in places where mlr3 Learners can be used like resampling or benchmarks. For this, it needs to be wrapped in a GraphLearner object, which is a thin wrapper that enables this functionality. The resulting Learner is extremely versatile, because every part of it can be modified, replaced, parameterized and optimized over. Resampling the graph above can be done the same way that resampling of the Learner was performed in the introductory example. lrngrph = GraphLearner$new(gr) resample(task, lrngrph, rsmp) ## &lt;ResampleResult&gt; of 1 iterations ## * Task: iris ## * Learner: pca.classif.rpart ## * Warnings: 0 in 0 iterations ## * Errors: 0 in 0 iterations 4.7.6 Hyperparameters mlr3pipelines relies on the paradox package to provide parameters that can modify each PipeOp’s behavior. paradox parameters provide information about the parameters that can be changed, as well as their types and ranges. They provide a unified interface for benchmarks and parameter optimization (“tuning”). For a deep dive into paradox, see the mlr3book. The ParamSet, representing the space of possible parameter configurations of a PipeOp, can be inspected by accessing the $param_set slot of a PipeOp or a Graph. op_pca = mlr_pipeops$get(&quot;pca&quot;) op_pca$param_set ## ParamSet: pca ## id class lower upper levels default value ## 1: center ParamLgl NA NA TRUE,FALSE TRUE ## 2: scale. ParamLgl NA NA TRUE,FALSE FALSE ## 3: rank. ParamInt 1 Inf ## 4: affect_columns ParamUty NA NA &lt;Selector&gt; To set or retrieve a parameter, the $param_set$values slot can be accessed. Alternatively, the param_vals value can be given during construction. op_pca$param_set$values$center = FALSE op_pca$param_set$values ## $center ## [1] FALSE op_pca = mlr_pipeops$get(&quot;pca&quot;, param_vals = list(center = TRUE)) op_pca$param_set$values ## $center ## [1] TRUE Each PipeOp can bring its own individual parameters which are collected together in the Graph’s $param_set. A PipeOp’s parameter names are prefixed with its $id to prevent parameter name clashes. gr = op_pca %&gt;&gt;% mlr_pipeops$get(&quot;scale&quot;) gr$param_set ## ParamSet: ## id class lower upper levels default value ## 1: pca.center ParamLgl NA NA TRUE,FALSE TRUE TRUE ## 2: pca.scale. ParamLgl NA NA TRUE,FALSE FALSE ## 3: pca.rank. ParamInt 1 Inf ## 4: pca.affect_columns ParamUty NA NA &lt;Selector&gt; ## 5: scale.center ParamLgl NA NA TRUE,FALSE TRUE ## 6: scale.scale ParamLgl NA NA TRUE,FALSE TRUE ## 7: scale.affect_columns ParamUty NA NA &lt;Selector&gt; gr$param_set$values ## $pca.center ## [1] TRUE Both PipeOpLearner and GraphLearner preserve parameters of the objects they encapsulate. op_rpart = mlr_pipeops$get(&quot;learner&quot;, mlr_learners$get(&quot;classif.rpart&quot;)) op_rpart$param_set ## ParamSet: classif.rpart ## id class lower upper levels default value ## 1: minsplit ParamInt 1 Inf 20 ## 2: minbucket ParamInt 1 Inf &lt;NoDefault&gt; ## 3: cp ParamDbl 0 1 0.01 ## 4: maxcompete ParamInt 0 Inf 4 ## 5: maxsurrogate ParamInt 0 Inf 5 ## 6: maxdepth ParamInt 1 30 30 ## 7: usesurrogate ParamInt 0 2 2 ## 8: surrogatestyle ParamInt 0 1 0 ## 9: xval ParamInt 0 Inf 10 0 glrn = GraphLearner$new(gr %&gt;&gt;% op_rpart) glrn$param_set ## ParamSet: ## id class lower upper levels default ## 1: pca.center ParamLgl NA NA TRUE,FALSE TRUE ## 2: pca.scale. ParamLgl NA NA TRUE,FALSE FALSE ## 3: pca.rank. ParamInt 1 Inf ## 4: pca.affect_columns ParamUty NA NA &lt;Selector&gt; ## 5: scale.center ParamLgl NA NA TRUE,FALSE TRUE ## 6: scale.scale ParamLgl NA NA TRUE,FALSE TRUE ## 7: scale.affect_columns ParamUty NA NA &lt;Selector&gt; ## 8: classif.rpart.minsplit ParamInt 1 Inf 20 ## 9: classif.rpart.minbucket ParamInt 1 Inf &lt;NoDefault&gt; ## 10: classif.rpart.cp ParamDbl 0 1 0.01 ## 11: classif.rpart.maxcompete ParamInt 0 Inf 4 ## 12: classif.rpart.maxsurrogate ParamInt 0 Inf 5 ## 13: classif.rpart.maxdepth ParamInt 1 30 30 ## 14: classif.rpart.usesurrogate ParamInt 0 2 2 ## 15: classif.rpart.surrogatestyle ParamInt 0 1 0 ## 16: classif.rpart.xval ParamInt 0 Inf 10 ## value ## 1: TRUE ## 2: ## 3: ## 4: ## 5: ## 6: ## 7: ## 8: ## 9: ## 10: ## 11: ## 12: ## 13: ## 14: ## 15: ## 16: 0 It is tempting to denote this as a “directed acyclic graph”, but this would not be entirely correct because edges run between channels of PipeOps, not PipeOps themselves.↩ "],
["technical.html", "5 Technical", " 5 Technical This chapter provides an overview of technical details of the mlr3 framework. Parallelization At first, some details about Parallelization and the usage of the future are given. Parallelization refers to the process of running multiple jobs simultaneously. This process is employed to minimize the necessary computing power. Algorithms consist of both sequential (non-parallelizable) and parallelizable parts. Therefore, parallelization does not always alter performance in a positive substantial manner. Summed up, this sub-chapter illustrates how and when to use parallelization in mlr3. Database Backends The section Database Backends describes how to work with database backends that mlr3 supports. Database backends can be helpful for large data processing which does not fit in memory or is stored natively in a database (e.g. SQLite). Specifically when working with large data sets, or when undertaking numerous tasks simultaneously, it can be advantageous to interface out-of-memory data. The section provides an illustration of how to implement Database Backends using of NYC flight data. Parameters In the section Parameters instructions are given on how to: define parameter sets for learners undertake parameter sampling apply parameter transformations For illustrative purposes, this sub-chapter uses the paradox package, the successor of ParamHelpers. Logging and Verbosity The sub-chapter on Logging and Verbosity shows how to change the most important settings related to logging. In mlr3 we use the lgr package. Transition Guide Lastly, we provide a Transition Guide for users of the old mlr who want to switch to mlr3. "],
["parallelization.html", "5.1 Parallelization", " 5.1 Parallelization Parallelization refers to the process of running multiple jobs in parallel, simultaneously. This process allows for significant savings in computing power. mlr3 uses the future backends for parallelization. Make sure you have installed the required packages future and future.apply: mlr3 is capable of parallelizing a variety of different scenarios. One of the most used cases is to parallelize the Resampling iterations. See Section Resampling for a detailed introduction to resampling. In the following section, we will use the spam task and a simple classification tree (&quot;classif.rpart&quot;) to showcase parallelization. We use the future package to parallelize the resampling by selecting a backend via the function future::plan(). We use the future::multiprocess backend here which uses forks (c.f. parallel::mcparallel()) on UNIX based systems and a socket cluster on Windows or if running in RStudio: future::plan(&quot;multiprocess&quot;) task = tsk(&quot;spam&quot;) learner = lrn(&quot;classif.rpart&quot;) resampling = rsmp(&quot;subsampling&quot;) time = Sys.time() resample(task, learner, resampling) Sys.time() - time By default all CPUs of your machine are used unless you specify argument workers in future::plan(). On most systems you should see a decrease in the reported elapsed time. On some systems (e.g. Windows), the overhead for parallelization is quite large though. Therefore, it is advised to only enable parallelization for resamplings where each iteration runs at least 10 seconds. Choosing the parallelization level If you are transitioning from mlr, you might be used to selecting different parallelization levels, e.g. for resampling, benchmarking or tuning. In mlr3 this is no longer required. All kind of events are rolled out on the same level. Therefore, there is no need to decide whether you want to parallelize the tuning OR the resampling. In mlr3 this is no longer required. All kind of events are rolled out on the same level - there is no need to decide whether you want to parallelize the tuning OR the resampling. Just lean back and let the machine do the work :-) "],
["error-handling.html", "5.2 Error Handling", " 5.2 Error Handling To demonstrate how to properly deal with misbehaving learners, mlr3 ships with the learner classif.debug: task = tsk(&quot;iris&quot;) learner = lrn(&quot;classif.debug&quot;) print(learner) ## &lt;LearnerClassifDebug:classif.debug&gt; ## * Model: - ## * Parameters: list() ## * Packages: - ## * Predict Type: response ## * Feature types: logical, integer, numeric, character, factor, ordered ## * Properties: missings, multiclass, twoclass This learner comes with special hyperparameters that let us control what conditions should be signaled (message, warning, error, segfault) with what probability, during which stage the conditions should be signaled (train or predict), and the ratio of predictions being NA (predict_missing). learner$param_set ## ParamSet: ## id class lower upper levels default value ## 1: message_train ParamDbl 0 1 0 ## 2: message_predict ParamDbl 0 1 0 ## 3: warning_train ParamDbl 0 1 0 ## 4: warning_predict ParamDbl 0 1 0 ## 5: error_train ParamDbl 0 1 0 ## 6: error_predict ParamDbl 0 1 0 ## 7: segfault_train ParamDbl 0 1 0 ## 8: segfault_predict ParamDbl 0 1 0 ## 9: predict_missing ParamDbl 0 1 0 ## 10: save_tasks ParamLgl NA NA TRUE,FALSE FALSE ## 11: x ParamDbl 0 1 &lt;NoDefault&gt; With the learner’s default settings, the learner will do nothing special: The learner learns a random label and creates constant predictions. task = tsk(&quot;iris&quot;) learner$train(task)$predict(task)$confusion ## truth ## response setosa versicolor virginica ## setosa 0 0 0 ## versicolor 50 50 50 ## virginica 0 0 0 We now set a hyperparameter to let the debug learner signal an error during the train step. By default,mlr3 does not catch conditions such as warnings or errors raised by third-party code like learners: learner$param_set$values = list(error_train = 1) learner$train(tsk(&quot;iris&quot;)) ## Error in learner$.__enclos_env__$private$.train(task): Error from classif.debug-&gt;train() If this would be a regular learner, we could now start debugging with traceback() (or create a MRE to file a bug report). However, machine learning algorithms raising errors is not uncommon as algorithms typically cannot process all possible data. Thus, we need a mechanism to capture all signaled conditions such as messages, warnings and errors so that we can analyze them post-hoc, and a statistically sound way to proceed the calculation and be able to aggregate over partial results. These two mechanisms are explained in the following subsections. 5.2.1 Encapsulation With encapsulation, exceptions do not stop the program flow and all output is logged to the learner (instead of printed to the console). Each Learner has a field encapsulate to control how the train or predict steps are executed. One way to encapsulate the execution is provided by the package evaluate (see encapsulate() for more details): task = tsk(&quot;iris&quot;) learner = lrn(&quot;classif.debug&quot;) learner$param_set$values = list(warning_train = 1, error_train = 1) learner$encapsulate = c(train = &quot;evaluate&quot;, predict = &quot;evaluate&quot;) learner$train(task) After training the learner, one can access the recorded log via the fields log, warnings and errors: learner$log ## # A tibble: 2 x 3 ## stage class msg ## &lt;fct&gt; &lt;ord&gt; &lt;chr&gt; ## 1 train warning Warning from classif.debug-&gt;train() ## 2 train error Error from classif.debug-&gt;train() learner$warnings ## [1] &quot;Warning from classif.debug-&gt;train()&quot; learner$errors ## [1] &quot;Error from classif.debug-&gt;train()&quot; Another method for encapsulation is implemented in the callr package. callr spawns a new R process to execute the respective step, and thus even guards the current session from segfaults. On the downside, starting new processes comes with a computational overhead. learner$encapsulate = c(train = &quot;callr&quot;, predict = &quot;callr&quot;) learner$param_set$values = list(segfault_train = 1) learner$train(task = task) learner$errors ## [1] &quot;callr process exited with status -11&quot; Without a model, it is not possible to get predictions though: learner$predict(task) ## Error: Cannot predict, Learner &#39;classif.debug&#39; has not been trained yet To handle the missing predictions in a graceful way during resample() or benchmark(), fallback learners are introduced next. 5.2.2 Fallback learners Fallback learners have the purpose to allow scoring results in cases where a Learner is misbehaving in some sense. Some typical examples include: The learner fails to fit a model during training, e.g., if some convergence criterion is not met or the learner ran out of memory. The learner fails to predict for some or all observations. A typical case is e.g. new factor levels in the test data. We first handle the most common case that a learner completely breaks while fitting a model or while predicting on new data. If the learner fails in either of these two steps, we rely on a second learner to generate predictions: the fallback learner. In the next example, in addition to the debug learner, we attach a simple featureless learner to the debug learner. So whenever the debug learner fails (which is every time with the given parametrization) and encapsulation in enabled, mlr3 falls back to the predictions of the featureless learner internally: task = tsk(&quot;iris&quot;) learner = lrn(&quot;classif.debug&quot;) learner$param_set$values = list(error_train = 1) learner$encapsulate = c(train = &quot;evaluate&quot;) learner$fallback = lrn(&quot;classif.featureless&quot;) learner$train(task) learner ## &lt;LearnerClassifDebug:classif.debug&gt; ## * Model: - ## * Parameters: error_train=1 ## * Packages: - ## * Predict Type: response ## * Feature types: logical, integer, numeric, character, factor, ordered ## * Properties: missings, multiclass, twoclass ## * Errors: Error from classif.debug-&gt;train() Note that the log contains the captured error (which is also included in the print output), and although we don’t have a model, we can still get predictions: learner$model ## NULL prediction = learner$predict(task) prediction$score() ## classif.ce ## 0.6667 While the fallback learner is of limited use for this stepwise train-predict procedure, it is invaluable for larger benchmark studies where only few resampling iterations are failing. Here, we need to replace the missing scores with a number in order to aggregate over all resampling iterations. And imputing a number which is equivalent to guessing labels often seems to be the right amount of penalization. In the following snippet we compare the previously created debug learner with a simple classification tree. We re-parametrize the debug learner to fail in roughly 30% of the resampling iterations during the training step: learner$param_set$values = list(error_train = 0.3) bmr = benchmark(benchmark_grid(tsk(&quot;iris&quot;), list(learner, lrn(&quot;classif.rpart&quot;)), rsmp(&quot;cv&quot;))) aggr = bmr$aggregate(conditions = TRUE) aggr ## # A tibble: 2 x 9 ## nr resample_result task_id learner_id resampling_id iters warnings errors ## &lt;int&gt; &lt;list&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 &lt;RsmplRsl&gt; iris classif.d… cv 10 0 6 ## 2 2 &lt;RsmplRsl&gt; iris classif.r… cv 10 0 0 ## # … with 1 more variable: classif.ce &lt;dbl&gt; To further investigate the errors, we can extract the ResampleResult: rr = aggr[learner_id == &quot;classif.debug&quot;]$resample_result[[1L]] rr$errors ## # A tibble: 6 x 2 ## iteration msg ## &lt;int&gt; &lt;chr&gt; ## 1 1 Error from classif.debug-&gt;train() ## 2 2 Error from classif.debug-&gt;train() ## 3 5 Error from classif.debug-&gt;train() ## 4 7 Error from classif.debug-&gt;train() ## 5 8 Error from classif.debug-&gt;train() ## 6 10 Error from classif.debug-&gt;train() A similar yet different problem emerges when a learner predicts only a subset of the observations in the test set (and predicts NA for others). Handling such predictions in a statistically sound way is not straight-forward and a common source for over-optimism when reporting results. Imagine that our goal is to benchmark two algorithms using a 10-fold cross validation on some binary classification task: Algorithm A is a ordinary logistic regression. Algorithm B is also a ordinary logistic regression, but with a twist: If the logistic regression is rather certain about the predicted label (&gt; 90% probability), it returns the label and a missing value otherwise. When comparing the performance of these two algorithms, it is obviously not fair to average over all predictions of algorithm A while only average over the “easy-to-predict” observations for algorithm B. By doing so, algorithm B would easily outperform algorithm A, but you have not factored in that you can not generate predictions for many observations. On the other hand, it is also not feasible to exclude all observations from the test set of a benchmark study where at least one algorithm failed to predict a label. Instead, we proceed by imputing all missing predictions with something naive, e.g., by predicting the majority class with a featureless learner. And as the majority class may depend on the resampling split (or we opt for some other arbitrary baseline learner), it is best to just train a second learner on the same resampling split. Long story short, if a fallback learner is involved, missing predictions of the base learner will be automatically replaced with predictions from the fallback learner. This is illustrated in the following example: task = tsk(&quot;iris&quot;) learner = lrn(&quot;classif.debug&quot;) # this hyperparameter sets the ratio of missing predictions learner$param_set$values = list(predict_missing = 0.5) # without fallback p = learner$train(task)$predict(task) table(p$response, useNA = &quot;always&quot;) ## ## setosa versicolor virginica &lt;NA&gt; ## 0 75 0 75 # with fallback learner$fallback = lrn(&quot;classif.featureless&quot;) p = learner$train(task)$predict(task) table(p$response, useNA = &quot;always&quot;) ## ## setosa versicolor virginica &lt;NA&gt; ## 75 75 0 0 Summed up, by combining encapsulation and fallback learners, it is possible to benchmark even quite unreliable or instable learning algorithms in a convenient way. "],
["backends.html", "5.3 Database Backends", " 5.3 Database Backends In mlr3, Tasks store their data in an abstract data format, the DataBackend. The default backend uses data.table via the DataBackendDataTable as an in-memory data base. For larger data, or when working with many tasks in parallel, it can be advantageous to interface an out-of-memory data. We use the excellent R package dbplyr which extends dplyr to work on many popular data bases like MariaDB, PostgreSQL or SQLite. 5.3.1 Use Case: NYC Flights To generate a halfway realistic scenario, we use the NYC flights data set from package nycflights13: # load data requireNamespace(&quot;DBI&quot;) ## Loading required namespace: DBI requireNamespace(&quot;RSQLite&quot;) ## Loading required namespace: RSQLite requireNamespace(&quot;nycflights13&quot;) ## Loading required namespace: nycflights13 data(&quot;flights&quot;, package = &quot;nycflights13&quot;) str(flights) ## tibble [336,776 × 19] (S3: tbl_df/tbl/data.frame) ## $ year : int [1:336776] 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ... ## $ month : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ... ## $ day : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ... ## $ dep_time : int [1:336776] 517 533 542 544 554 554 555 557 557 558 ... ## $ sched_dep_time: int [1:336776] 515 529 540 545 600 558 600 600 600 600 ... ## $ dep_delay : num [1:336776] 2 4 2 -1 -6 -4 -5 -3 -3 -2 ... ## $ arr_time : int [1:336776] 830 850 923 1004 812 740 913 709 838 753 ... ## $ sched_arr_time: int [1:336776] 819 830 850 1022 837 728 854 723 846 745 ... ## $ arr_delay : num [1:336776] 11 20 33 -18 -25 12 19 -14 -8 8 ... ## $ carrier : chr [1:336776] &quot;UA&quot; &quot;UA&quot; &quot;AA&quot; &quot;B6&quot; ... ## $ flight : int [1:336776] 1545 1714 1141 725 461 1696 507 5708 79 301 ... ## $ tailnum : chr [1:336776] &quot;N14228&quot; &quot;N24211&quot; &quot;N619AA&quot; &quot;N804JB&quot; ... ## $ origin : chr [1:336776] &quot;EWR&quot; &quot;LGA&quot; &quot;JFK&quot; &quot;JFK&quot; ... ## $ dest : chr [1:336776] &quot;IAH&quot; &quot;IAH&quot; &quot;MIA&quot; &quot;BQN&quot; ... ## $ air_time : num [1:336776] 227 227 160 183 116 150 158 53 140 138 ... ## $ distance : num [1:336776] 1400 1416 1089 1576 762 ... ## $ hour : num [1:336776] 5 5 5 5 6 5 6 6 6 6 ... ## $ minute : num [1:336776] 15 29 40 45 0 58 0 0 0 0 ... ## $ time_hour : POSIXct[1:336776], format: &quot;2013-01-01 05:00:00&quot; &quot;2013-01-01 05:00:00&quot; ... # add column of unique row ids flights$row_id = 1:nrow(flights) # create sqlite database in temporary file path = tempfile(&quot;flights&quot;, fileext = &quot;.sqlite&quot;) con = DBI::dbConnect(RSQLite::SQLite(), path) tbl = DBI::dbWriteTable(con, &quot;flights&quot;, as.data.frame(flights)) DBI::dbDisconnect(con) # remove in-memory data rm(flights) 5.3.2 Preprocessing with dplyr With the SQLite database in path, we now re-establish a connection and switch to dplyr/dbplyr for some essential preprocessing. # establish connection con = DBI::dbConnect(RSQLite::SQLite(), path) # select the &quot;flights&quot; table, enter dplyr library(&quot;dplyr&quot;) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:data.table&#39;: ## ## between, first, last ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union library(&quot;dbplyr&quot;) ## ## Attaching package: &#39;dbplyr&#39; ## The following objects are masked from &#39;package:dplyr&#39;: ## ## ident, sql tbl = tbl(con, &quot;flights&quot;) First, we select a subset of columns to work on: keep = c(&quot;row_id&quot;, &quot;year&quot;, &quot;month&quot;, &quot;day&quot;, &quot;hour&quot;, &quot;minute&quot;, &quot;dep_time&quot;, &quot;arr_time&quot;, &quot;carrier&quot;, &quot;flight&quot;, &quot;air_time&quot;, &quot;distance&quot;, &quot;arr_delay&quot;) tbl = select(tbl, keep) ## Note: Using an external vector in selections is ambiguous. ## ℹ Use `all_of(keep)` instead of `keep` to silence this message. ## ℹ See &lt;https://tidyselect.r-lib.org/reference/faq-external-vector.html&gt;. ## This message is displayed once per session. Additionally, we remove those observations where the arrival delay (arr_delay) has a missing value: tbl = filter(tbl, !is.na(arr_delay)) To keep runtime reasonable for this toy example, we filter the data to only use every second row: tbl = filter(tbl, row_id %% 2 == 0) The factor levels of the feature carrier are merged so that infrequent carriers are replaced by level “other”: tbl = mutate(tbl, carrier = case_when( carrier %in% c(&quot;OO&quot;, &quot;HA&quot;, &quot;YV&quot;, &quot;F9&quot;, &quot;AS&quot;, &quot;FL&quot;, &quot;VX&quot;, &quot;WN&quot;) ~ &quot;other&quot;, TRUE ~ carrier) ) 5.3.3 DataBackendDplyr The processed table is now used to create a mlr3db::DataBackendDplyr from mlr3db: library(&quot;mlr3db&quot;) b = as_data_backend(tbl, primary_key = &quot;row_id&quot;) We can now use the interface of DataBackend to query some basic information of the data: b$nrow ## [1] 163707 b$ncol ## [1] 13 b$head() ## # A tibble: 6 x 13 ## row_id year month day hour minute dep_time arr_time carrier flight ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; ## 1 2 2013 1 1 5 29 533 850 UA 1714 ## 2 4 2013 1 1 5 45 544 1004 B6 725 ## 3 6 2013 1 1 5 58 554 740 UA 1696 ## 4 8 2013 1 1 6 0 557 709 EV 5708 ## 5 10 2013 1 1 6 0 558 753 AA 301 ## 6 12 2013 1 1 6 0 558 853 B6 71 ## # … with 3 more variables: air_time &lt;dbl&gt;, distance &lt;dbl&gt;, arr_delay &lt;dbl&gt; Note that the DataBackendDplyr does not know about any rows or columns we have filtered out with dplyr before, it just operates on the view we provided. 5.3.4 Model fitting We create the following mlr3 objects: A regression task, based on the previously created mlr3db::DataBackendDplyr. A regression learner (regr.rpart). A resampling strategy: 3 times repeated subsampling using 2% of the observations for training (“subsampling”) Measures “mse”, “time_predict” and “time_predict” task = TaskRegr$new(&quot;flights_sqlite&quot;, b, target = &quot;arr_delay&quot;) learner = lrn(&quot;regr.rpart&quot;) measures = mlr_measures$mget(c(&quot;regr.mse&quot;, &quot;time_train&quot;, &quot;time_predict&quot;)) resampling = rsmp(&quot;subsampling&quot;) resampling$param_set$values = list(repeats = 3, ratio = 0.02) We pass all these objects to resample() to perform a simple resampling with three iterations. In each iteration, only the required subset of the data is queried from the SQLite data base and passed to rpart::rpart(): rr = resample(task, learner, resampling) print(rr) ## &lt;ResampleResult&gt; of 3 iterations ## * Task: flights_sqlite ## * Learner: regr.rpart ## * Warnings: 0 in 0 iterations ## * Errors: 0 in 0 iterations rr$aggregate(measures) ## regr.mse time_train time_predict ## 1209.7151 0.1483 1.6900 5.3.5 Cleanup Finally, we remove the tbl object and close the connection. rm(tbl) DBI::dbDisconnect(con) "],
["paradox.html", "5.4 Parameters (using paradox)", " 5.4 Parameters (using paradox) The paradox package offers a language for the description of parameter spaces, as well as tools for useful operations on these parameter spaces. A parameter space is often useful when describing: A set of sensible input values for an R function The set of possible values that slots of a configuration object can take The search space of an optimization process The tools provided by paradox therefore relate to: Parameter checking: Verifying that a set of parameters satisfies the conditions of a parameter space Parameter sampling: Generating parameter values that lie in the parameter space for systematic exploration of program behavior depending on these parameters paradox is, by nature, an auxiliary package that derives its usefulness from other packages that make use of it. It is heavily utilized in other mlr-org packages such as mlr3, mlr3pipelines, and mlr3tuning. 5.4.1 Reference Based Objects paradox is the spiritual successor to the ParamHelpers package and was written from scratch using the R6 class system. The most important consequence of this is that all objects created in paradox are “reference-based”, unlike most other objects in R. When a change is made to a ParamSet object, for example by adding a parameter using the $add() function, all variables that point to this ParamSet will contain the changed object. To create an independent copy of a ParamSet, the $clone() method needs to be used: library(&quot;paradox&quot;) ps = ParamSet$new() ps2 = ps ps3 = ps$clone(deep = TRUE) print(ps) # the same for ps2 and ps3 ## ParamSet: ## Empty. ps$add(ParamLgl$new(&quot;a&quot;)) print(ps) # ps was changed ## ParamSet: ## id class lower upper levels default value ## 1: a ParamLgl NA NA TRUE,FALSE &lt;NoDefault&gt; print(ps2) # contains the same reference as ps ## ParamSet: ## id class lower upper levels default value ## 1: a ParamLgl NA NA TRUE,FALSE &lt;NoDefault&gt; print(ps3) # is a &quot;clone&quot; of the old (empty) ps ## ParamSet: ## Empty. 5.4.2 Defining a Parameter Space 5.4.2.1 Single Parameters The basic building block for describing parameter spaces is the Param class. It represents a single parameter, which usually can take a single atomic value. Consider, for example, trying to configure the rpart package’s rpart.control object. It has various components (minsplit, cp, …) that all take a single value, and that would all be represented by a different instance of a Param object. The Param class has various sub-classes that represent different value types: ParamInt: Integer numbers ParamDbl: Real numbers ParamFct: String values from a set of possible values, similar to R factors ParamLgl: Truth values (TRUE / FALSE), as logicals in R ParamUty: Parameter that can take any value A particular instance of a parameter is created by calling the attached $new() function: library(&quot;paradox&quot;) parA = ParamLgl$new(id = &quot;A&quot;) parB = ParamInt$new(id = &quot;B&quot;, lower = 0, upper = 10, tags = c(&quot;tag1&quot;, &quot;tag2&quot;)) parC = ParamDbl$new(id = &quot;C&quot;, lower = 0, upper = 4, special_vals = list(NULL)) parD = ParamFct$new(id = &quot;D&quot;, levels = c(&quot;x&quot;, &quot;y&quot;, &quot;z&quot;), default = &quot;y&quot;) parE = ParamUty$new(id = &quot;E&quot;, custom_check = function(x) checkmate::checkFunction(x)) Every parameter must have: id - A name for the parameter within the parameter set default - A default value special_vals - A list of values that are accepted even if they do not conform to the type tags - Tags that can be used to organize parameters The numeric (Int and Dbl) parameters furthermore allow for specification of a lower and upper bound. Meanwhile, the Fct parameter must be given a vector of levels that define the possible states its parameter can take. The Uty parameter can also have a custom_check function that must return TRUE when a value is acceptable and may return a character(1) error description otherwise. The example above defines parE as a parameter that only accepts functions. All values which are given to the constructor are then accessible from the object for inspection using $. Although all these values can be changed for a parameter after construction, this can be a bad idea and should be avoided when possible. Instead, a new parameter should be constructed. Besides the possible values that can be given to a constructor, there are also the $class, $nlevels, $is_bounded, $has_default, $storage_type, $is_number and $is_categ slots that give information about a parameter. A list of all slots can be found in ?Param. parB$lower ## [1] 0 parA$levels ## [1] TRUE FALSE parE$class ## [1] &quot;ParamUty&quot; It is also possible to get all information of a Param as data.table by calling as.data.table. as.data.table(parA) ## # A tibble: 1 x 11 ## id class lower upper levels nlevels is_bounded special_vals default storage_type tags ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt; &lt;int&gt; &lt;lgl&gt; &lt;list&gt; &lt;list&gt; &lt;chr&gt; &lt;list&gt; ## 1 A ParamLgl NA NA &lt;lgl [2]&gt; 2 TRUE &lt;list [0]&gt; &lt;NoDefalt&gt; logical &lt;chr [0]&gt; 5.4.2.1.1 Type / Range Checking A Param object offers the possibility to check whether a value satisfies its condition, i.e. is of the right type, and also falls within the range of allowed values, using the $test(), $check(), and $assert() functions. test() should be used within conditional checks and returns TRUE or FALSE, while check() returns an error description when a value does not conform to the parameter (and thus plays well with the checkmate::assert() function). assert() will throw an error whenever a value does not fit. parA$test(FALSE) ## [1] TRUE parA$test(&quot;FALSE&quot;) ## [1] FALSE parA$check(&quot;FALSE&quot;) ## [1] &quot;Must be of type &#39;logical flag&#39;, not &#39;character&#39;&quot; Instead of testing single parameters, it is often more convenient to check a whole set of parameters using a ParamSet. 5.4.2.2 Parameter Sets The ordered collection of parameters is handled in a ParamSet2. It is initialized using the $new() function and optionally takes a list of Params as argument. Parameters can also be added to the constructed ParamSet using the $add() function. It is even possible to add whole ParamSets to other ParamSets. ps = ParamSet$new(list(parA, parB)) ps$add(parC) ps$add(ParamSet$new(list(parD, parE))) print(ps) ## ParamSet: ## id class lower upper levels default value ## 1: A ParamLgl NA NA TRUE,FALSE &lt;NoDefault&gt; ## 2: B ParamInt 0 10 &lt;NoDefault&gt; ## 3: C ParamDbl 0 4 &lt;NoDefault&gt; ## 4: D ParamFct NA NA x,y,z y ## 5: E ParamUty NA NA &lt;NoDefault&gt; The individual parameters can be accessed through the $params slot. It is also possible to get information about all parameters in a vectorized fashion using mostly the same slots as for individual Params (i.e. $class, $levels etc.), see ?ParamSet for details. It is possible to reduce ParamSets using the $subset method. Be aware that it modifies a ParamSet in-place, so a “clone” must be created first if the original ParamSet should not be modified. psSmall = ps$clone() psSmall$subset(c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;)) print(psSmall) ## ParamSet: ## id class lower upper levels default value ## 1: A ParamLgl NA NA TRUE,FALSE &lt;NoDefault&gt; ## 2: B ParamInt 0 10 &lt;NoDefault&gt; ## 3: C ParamDbl 0 4 &lt;NoDefault&gt; Just as for Params, and much more useful, it is possible to get the ParamSet as a data.table using as.data.table(). This makes it easy to subset parameters on certain conditions and aggregate information about them, using the variety of methods provided by data.table. as.data.table(ps) ## # A tibble: 5 x 11 ## id class lower upper levels nlevels is_bounded special_vals default storage_type tags ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;list&gt; &lt;list&gt; &lt;chr&gt; &lt;list&gt; ## 1 A ParamLgl NA NA &lt;lgl [2]&gt; 2 TRUE &lt;list [0]&gt; &lt;NoDefalt&gt; logical &lt;chr [0]&gt; ## 2 B ParamInt 0 10 &lt;NULL&gt; 11 TRUE &lt;list [0]&gt; &lt;NoDefalt&gt; integer &lt;chr [2]&gt; ## 3 C ParamDbl 0 4 &lt;NULL&gt; Inf TRUE &lt;list [1]&gt; &lt;NoDefalt&gt; numeric &lt;chr [0]&gt; ## 4 D ParamFct NA NA &lt;chr [3]&gt; 3 TRUE &lt;list [0]&gt; &lt;chr [1]&gt; character &lt;chr [0]&gt; ## 5 E ParamUty NA NA &lt;NULL&gt; Inf FALSE &lt;list [0]&gt; &lt;NoDefalt&gt; list &lt;chr [0]&gt; 5.4.2.2.1 Type / Range Checking Similar to individual Params, the ParamSet provides $test(), $check() and $assert() functions that allow for type and range checking of parameters. Their argument must be a named list with values that are checked against the respective parameters. It is possible to check only a subset of parameters. ps$check(list(A = TRUE, B = 0, E = identity)) ## [1] TRUE ps$check(list(A = 1)) ## [1] &quot;A: Must be of type &#39;logical flag&#39;, not &#39;double&#39;&quot; ps$check(list(Z = 1)) ## [1] &quot;Parameter &#39;Z&#39; not available. Did you mean &#39;A&#39; / &#39;B&#39; / &#39;C&#39;?&quot; 5.4.2.2.2 Values in a ParamSet Although a ParamSet fundamentally represents a value space, it also has a slot $values that can contain a point within that space. This is useful because many things that define a parameter space need similar operations (like parameter checking) that can be simplified. The $values slot contains a named list that is always checked against parameter constraints. When trying to set parameter values, e.g. for mlr3 Learners, it is the $values slot of its $param_set that needs to be used. ps$values = list(A = TRUE, B = 0) ps$values$B = 1 print(ps$values) ## $A ## [1] TRUE ## ## $B ## [1] 1 The parameter constraints are automatically checked: ps$values$B = 100 ## Error in (function (xs) : Assertion on &#39;xs&#39; failed: B: Element 1 is not &lt;= 10. 5.4.2.2.3 Dependencies It is often the case that certain parameters are irrelevant or should not be given depending on values of other parameters. An example would be a parameter that switches a certain algorithm feature (for example regularization) on or off, combined with another parameter that controls the behavior of that feature (e.g. a regularization parameter). The second parameter would be said to depend on the first parameter having the value TRUE. A dependency can be added using the $add_dep method, which takes both the ids of the “depender” and “dependee” parameters as well as a Condition object. The Condition object represents the check to be performed on the “dependee”. Currently it can be created using CondEqual$new() and CondAnyOf$new(). Multiple dependencies can be added, and parameters that depend on others can again be depended on, as long as no cyclic dependencies are introduced. The consequence of dependencies are twofold: For one, the $check(), $test() and $assert() tests will not accept the presence of a parameter if its dependency is not met. Furthermore, when sampling or creating grid designs from a ParamSet, the dependencies will be respected (see Parameter Sampling, in particular Hierarchical Sampler). The following example makes parameter D depend on parameter A being FALSE, and parameter B depend on parameter D being one of &quot;x&quot; or &quot;y&quot;. This introduces an implicit dependency of B on A being FALSE as well, because D does not take any value if A is TRUE. ps$add_dep(&quot;D&quot;, &quot;A&quot;, CondEqual$new(FALSE)) ps$add_dep(&quot;B&quot;, &quot;D&quot;, CondAnyOf$new(c(&quot;x&quot;, &quot;y&quot;))) ps$check(list(A = FALSE, D = &quot;x&quot;, B = 1)) # OK: all dependencies met ## [1] TRUE ps$check(list(A = FALSE, D = &quot;z&quot;, B = 1)) # B&#39;s dependency is not met ## [1] &quot;The parameter &#39;B&#39; can only be set if the following condition is met &#39;D ∈ {x, y}&#39;. Instead the current parameter value is: D=z&quot; ps$check(list(A = FALSE, B = 1)) # B&#39;s dependency is not met ## [1] &quot;The parameter &#39;B&#39; can only be set if the following condition is met &#39;D ∈ {x, y}&#39;. Instead the parameter value for &#39;D&#39; is not set at all. Try setting &#39;D&#39; to a value that satisfies the condition&quot; ps$check(list(A = FALSE, D = &quot;z&quot;)) # OK: B is absent ## [1] TRUE ps$check(list(A = TRUE)) # OK: neither B nor D present ## [1] TRUE ps$check(list(A = TRUE, D = &quot;x&quot;, B = 1)) # D&#39;s dependency is not met ## [1] &quot;The parameter &#39;D&#39; can only be set if the following condition is met &#39;A = FALSE&#39;. Instead the current parameter value is: A=TRUE&quot; ps$check(list(A = TRUE, B = 1)) # B&#39;s dependency is not met ## [1] &quot;The parameter &#39;B&#39; can only be set if the following condition is met &#39;D ∈ {x, y}&#39;. Instead the parameter value for &#39;D&#39; is not set at all. Try setting &#39;D&#39; to a value that satisfies the condition&quot; Internally, the dependencies are represented as a data.table, which can be accessed listed in the $deps slot. This data.table can even be mutated, to e.g. remove dependencies. There are no sanity checks done when the $deps slot is changed this way. Therefore it is advised to be cautious. ps$deps ## # A tibble: 2 x 3 ## id on cond ## &lt;chr&gt; &lt;chr&gt; &lt;list&gt; ## 1 D A &lt;CondEqul&gt; ## 2 B D &lt;CndAnyOf&gt; 5.4.2.3 Vector Parameters Unlike in the old ParamHelpers package, there are no more vectorial parameters in paradox. Instead, it is now possible to create multiple copies of a single parameter using the $rep function. This creates a ParamSet consisting of multiple copies of the parameter, which can then (optionally) be added to another ParamSet. ps2d = ParamDbl$new(&quot;x&quot;, lower = 0, upper = 1)$rep(2) print(ps2d) ## ParamSet: ## id class lower upper levels default value ## 1: x_rep_1 ParamDbl 0 1 &lt;NoDefault&gt; ## 2: x_rep_2 ParamDbl 0 1 &lt;NoDefault&gt; ps$add(ps2d) print(ps) ## ParamSet: ## id class lower upper levels default parents value ## 1: A ParamLgl NA NA TRUE,FALSE &lt;NoDefault&gt; TRUE ## 2: B ParamInt 0 10 &lt;NoDefault&gt; D 1 ## 3: C ParamDbl 0 4 &lt;NoDefault&gt; ## 4: D ParamFct NA NA x,y,z y A ## 5: E ParamUty NA NA &lt;NoDefault&gt; ## 6: x_rep_1 ParamDbl 0 1 &lt;NoDefault&gt; ## 7: x_rep_2 ParamDbl 0 1 &lt;NoDefault&gt; It is also possible to use a ParamUty to accept vectorial parameters, which also works for parameters of variable length. A ParamSet containing a ParamUty can be used for parameter checking, but not for sampling. To sample values for a method that needs a vectorial parameter, it is advised to use a parameter transformation function that creates a vector from atomic values. Assembling a vector from repeated parameters is aided by the parameter’s $tags: Parameters that were generated by the $rep() command automatically get tagged as belonging to a group of repeated parameters. ps$tags ## $A ## character(0) ## ## $B ## [1] &quot;tag1&quot; &quot;tag2&quot; ## ## $C ## character(0) ## ## $D ## character(0) ## ## $E ## character(0) ## ## $x_rep_1 ## [1] &quot;x_rep&quot; ## ## $x_rep_2 ## [1] &quot;x_rep&quot; 5.4.3 Parameter Sampling It is often useful to have a list of possible parameter values that can be systematically iterated through, for example to find parameter values for which an algorithm performs particularly well (tuning). paradox offers a variety of functions that allow creating evenly-spaced parameter values in a “grid” design as well as random sampling. In the latter case, it is possible to influence the sampling distribution in more or less fine detail. A point to always keep in mind while sampling is that only numerical and factorial parameters that are bounded can be sampled from, i.e. not ParamUty. Furthermore, for most samplers ParamInt and ParamDbl must have finite lower and upper bounds. 5.4.3.1 Parameter Designs Functions that sample the parameter space fundamentally return an object of the Design class. These objects contain the sampled data as a data.table under the $data slot, and also offer conversion to a list of parameter-values using the $transpose() function. 5.4.3.2 Grid Design The generate_design_grid() function is used to create grid designs that contain all combinations of parameter values: All possible values for ParamLgl and ParamFct, and values with a given resolution for ParamInt and ParamDbl. The resolution can be given for all numeric parameters, or for specific named parameters through the param_resolutions parameter. design = generate_design_grid(psSmall, 2) print(design) ## &lt;Design&gt; with 8 rows: ## A B C ## 1: TRUE 0 0 ## 2: TRUE 0 4 ## 3: TRUE 10 0 ## 4: TRUE 10 4 ## 5: FALSE 0 0 ## 6: FALSE 0 4 ## 7: FALSE 10 0 ## 8: FALSE 10 4 generate_design_grid(psSmall, param_resolutions = c(B = 1, C = 2)) ## &lt;Design&gt; with 4 rows: ## B C A ## 1: 0 0 TRUE ## 2: 0 0 FALSE ## 3: 0 4 TRUE ## 4: 0 4 FALSE 5.4.3.3 Random Sampling paradox offers different methods for random sampling, which vary in the degree to which they can be configured. The easiest way to get a uniformly random sample of parameters is generate_design_random. It is also possible to create “latin hypercube” sampled parameter values using generate_design_lhs, which utilizes the lhs package. LHS-sampling creates low-discrepancy sampled values that cover the parameter space more evenly than purely random values. pvrand = generate_design_random(ps2d, 500) pvlhs = generate_design_lhs(ps2d, 500) 5.4.3.4 Generalized Sampling: The Sampler Class It may sometimes be desirable to configure parameter sampling in more detail. paradox uses the Sampler abstract base class for sampling, which has many different sub-classes that can be parameterized and combined to control the sampling process. It is even possible to create further sub-classes of the Sampler class (or of any of its subclasses) for even more possibilities. Every Sampler object has a sample() function, which takes one argument, the number of instances to sample, and returns a Design object. 5.4.3.4.1 1D-Samplers There is a variety of samplers that sample values for a single parameter. These are Sampler1DUnif (uniform sampling), Sampler1DCateg (sampling for categorical parameters), Sampler1DNormal (normally distributed sampling, truncated at parameter bounds), and Sampler1DRfun (arbitrary 1D sampling, given a random-function). These are initialized with a single Param, and can then be used to sample values. sampA = Sampler1DCateg$new(parA) sampA$sample(5) ## &lt;Design&gt; with 5 rows: ## A ## 1: FALSE ## 2: FALSE ## 3: TRUE ## 4: FALSE ## 5: TRUE 5.4.3.4.2 Hierarchical Sampler The SamplerHierarchical sampler is an auxiliary sampler that combines many 1D-Samplers to get a combined distribution. Its name “hierarchical” implies that it is able to respect parameter dependencies. This suggests that parameters only get sampled when their dependencies are met. The following example shows how this works: The Int parameter B depends on the Lgl parameter A being TRUE. A is sampled to be TRUE in about half the cases, in which case B takes a value between 0 and 10. In the cases where A is FALSE, B is set to NA. psSmall$add_dep(&quot;B&quot;, &quot;A&quot;, CondEqual$new(TRUE)) sampH = SamplerHierarchical$new(psSmall, list(Sampler1DCateg$new(parA), Sampler1DUnif$new(parB), Sampler1DUnif$new(parC)) ) sampled = sampH$sample(1000) table(sampled$data[, c(&quot;A&quot;, &quot;B&quot;)], useNA = &quot;ifany&quot;) ## B ## A 0 1 2 3 4 5 6 7 8 9 10 &lt;NA&gt; ## FALSE 0 0 0 0 0 0 0 0 0 0 0 510 ## TRUE 49 43 54 44 34 40 55 45 41 42 43 0 5.4.3.4.3 Joint Sampler Another way of combining samplers is the SamplerJointIndep. SamplerJointIndep also makes it possible to combine Samplers that are not 1D. However, SamplerJointIndep currently can not handle ParamSets with dependencies. sampJ = SamplerJointIndep$new( list(Sampler1DUnif$new(ParamDbl$new(&quot;x&quot;, 0, 1)), Sampler1DUnif$new(ParamDbl$new(&quot;y&quot;, 0, 1))) ) sampJ$sample(5) ## &lt;Design&gt; with 5 rows: ## x y ## 1: 0.5907 0.65968 ## 2: 0.5573 0.40329 ## 3: 0.5139 0.76457 ## 4: 0.3071 0.18659 ## 5: 0.7489 0.08103 5.4.3.4.4 SamplerUnif The Sampler used in generate_design_random is the SamplerUnif sampler, which corresponds to a HierarchicalSampler of Sampler1DUnif for all parameters. 5.4.4 Parameter Transformation While the different Samplers allow for a wide specification of parameter distributions, there are cases where the simplest way of getting a desired distribution is to sample parameters from a simple distribution (such as the uniform distribution) and then transform them. This can be done by assigning a function to the $trafo slot of a ParamSet. The $trafo function is called with two parameters: The list of parameter values to be transformed as x The ParamSet itself as param_set The $trafo function must return a list of transformed parameter values. The transformation is performed when calling the $transpose function of the Design object returned by a Sampler with the trafo ParamSet to TRUE (the default). The following, for example, creates a parameter that is exponentially distributed: psexp = ParamSet$new(list(ParamDbl$new(&quot;par&quot;, 0, 1))) psexp$trafo = function(x, param_set) { x$par = -log(x$par) x } design = generate_design_random(psexp, 2) print(design) ## &lt;Design&gt; with 2 rows: ## par ## 1: 0.62544 ## 2: 0.03342 design$transpose() # trafo is TRUE ## [[1]] ## [[1]]$par ## [1] 0.4693 ## ## ## [[2]] ## [[2]]$par ## [1] 3.399 Compare this to $transpose() without transformation: design$transpose(trafo = FALSE) ## [[1]] ## [[1]]$par ## [1] 0.6254 ## ## ## [[2]] ## [[2]]$par ## [1] 0.03342 5.4.4.1 Transformation between Types Usually the design created with one ParamSet is then used to configure other objects that themselves have a ParamSet which defines the values they take. The ParamSets which can be used for random sampling, however, are restricted in some ways: They must have finite bounds, and they may not contain “untyped” (ParamUty) parameters. $trafo provides the glue for these situations. There is relatively little constraint on the trafo function’s return value, so it is possible to return values that have different bounds or even types than the original ParamSet. It is even possible to remove some parameters and add new ones. Suppose, for example, that a certain method requires a function as a parameter. Let’s say a function that summarizes its data in a certain way. The user can pass functions like median() or mean(), but could also pass quantiles or something completely different. This method would probably use the following ParamSet: methodPS = ParamSet$new( list( ParamUty$new(&quot;fun&quot;, custom_check = function(x) checkmate::checkFunction(x, nargs = 1)) ) ) print(methodPS) ## ParamSet: ## id class lower upper levels default value ## 1: fun ParamUty NA NA &lt;NoDefault&gt; If one wanted to sample this method, using one of four functions, a way to do this would be: samplingPS = ParamSet$new( list( ParamFct$new(&quot;fun&quot;, c(&quot;mean&quot;, &quot;median&quot;, &quot;min&quot;, &quot;max&quot;)) ) ) samplingPS$trafo = function(x, param_set) { # x$fun is a `character(1)`, # in particular one of &#39;mean&#39;, &#39;median&#39;, &#39;min&#39;, &#39;max&#39;. # We want to turn it into a function! x$fun = get(x$fun, mode = &quot;function&quot;) x } design = generate_design_random(samplingPS, 2) print(design) ## &lt;Design&gt; with 2 rows: ## fun ## 1: median ## 2: mean Note that the Design only contains the column “fun” as a character column. To get a single value as a function, the $transpose function is used. xvals = design$transpose() print(xvals[[1]]) ## $fun ## function (x, na.rm = FALSE, ...) ## UseMethod(&quot;median&quot;) ## &lt;bytecode: 0x1584d60&gt; ## &lt;environment: namespace:stats&gt; We can now check that it fits the requirements set by methodPS, and that fun it is in fact a function: methodPS$check(xvals[[1]]) ## [1] &quot;fun: Must have exactly 1 formal arguments, but has 2&quot; xvals[[1]]$fun(1:10) ## [1] 5.5 Imagine now that a different kind of parametrization of the function is desired: The user wants to give a function that selects a certain quantile, where the quantile is set by a parameter. In that case the $transpose function could generate a function in a different way. For interpretability, the parameter is called “quantile” before transformation, and the “fun” parameter is generated on the fly. samplingPS2 = ParamSet$new( list( ParamDbl$new(&quot;quantile&quot;, 0, 1) ) ) samplingPS2$trafo = function(x, param_set) { # x$quantile is a `numeric(1)` between 0 and 1. # We want to turn it into a function! list(fun = function(input) quantile(input, x$quantile)) } design = generate_design_random(samplingPS2, 2) print(design) ## &lt;Design&gt; with 2 rows: ## quantile ## 1: 0.01826 ## 2: 0.32492 The Design now contains the column “quantile” that will be used by the $transpose function to create the fun parameter. We also check that it fits the requirement set by methodPS, and that it is a function. xvals = design$transpose() print(xvals[[1]]) ## $fun ## function(input) quantile(input, x$quantile) ## &lt;environment: 0x11a31f40&gt; methodPS$check(xvals[[1]]) ## [1] TRUE xvals[[1]]$fun(1:10) ## 1.826% ## 1.164 Although the name is suggestive of a “Set”-valued Param, this is unrelated to the other objects that follow the ParamXxx naming scheme.↩ "],
["logging.html", "5.5 Logging and Verbosity", " 5.5 Logging and Verbosity We use the lgr package for logging and progress output. Because lgr comes with its own exhaustive vignette, we will just briefly give examples how you can change the most important settings related to logging in mlr3. 5.5.1 Available logging levels lgr comes with certain numeric thresholds which correspond to verbosity levels of the logging. For mlr3 the default is set to 400 which corresponds to level “info”. The following ones are available: library(&quot;lgr&quot;) ## ## Attaching package: &#39;lgr&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## Layout getOption(&quot;lgr.log_levels&quot;) ## fatal error warn info debug trace ## 100 200 300 400 500 600 5.5.2 Global Setting lgr comes with a global option called &quot;lgr.default_threshold&quot; which can be set via options(). You can set a specific level in your .Rprofile which is then used for all packages that use the lgr package. This approach may not be desirable if you want to only change the logging level for mlr3. 5.5.3 Changing mlr3 logging levels To change the setting for mlr3 only, you need to change the threshold of the mlr3 logger like this: lgr::get_logger(&quot;mlr3&quot;)$set_threshold(&quot;&lt;level&gt;&quot;) Remember that this change only applies to the current R session. "],
["transition.html", "5.6 mlr -&gt; mlr3 Transition Guide", " 5.6 mlr -&gt; mlr3 Transition Guide In case you have already worked with mlr, you may want to quickstart with mlr3 by looking up the specific equivalent of an element of mlr in the new version mlr3. For this, you can use the following table. This table is not complete but should give you an overview about how mlr3 is organized. Category mlr mlr3 Note General / Helper getCacheDir() / deleteCacheDir() Not yet implemented — configureMlr() — — getMlrOptions() — — createDummyFeatures() Not yet implemented mlr3pipelines crossover() — — downsample() Not yet implemented — generateCalibrationData() Not yet implemented — generateCritDifferencesData() Not yet implemented — generateLearningCurveData() Not yet implemented mlr3viz generatePartialDependenceData() Not yet implemented mlr3viz generateThreshVsPerfData() Not yet implemented mlr3viz getCaretParamSet() Not used anymore — reimpute() / impute() Not yet implemented mlr3pipelines fn() / fnr() / fp() / fpr() ??? tn() / tnr() / tp() / tpr() ??? summarizeColumns() ??? summarizeLevels() ??? Task Task mlr_tasks / Task — SurvTask TaskSurv mlr3proba ClusterTask mlr_tasks — MultilabelTask mlr_tasks — SpatialTask Not yet implemented mlr3spatiotemporal Example tasks (iris.task,mtcars.task) mlr_tasks$get(‘iris’) / tsk(‘iris’) — convertMLBenchObjToTask() Not yet implemented mlr3 dropFeatures() Task$select() — getTaskCosts() Not yet implemented — getTaskData() Task$data() — getTaskDesc() / getTaskDescription() Task$print() — getTaskFeatureNames() Task$feature_names — getTaskFormula() Task$formula — getTaskId() Task$id — getTaskNFeats() length(Task$feature_names) — getTaskSize() Task$nrow() — getTaskTargetNames() Task$target_names — getTaskTargets() as.data.table(Task)[,Task$feature_names,with = FALSE] — getTaskType() Task$task_type — oversample() / undersample() — Learner helpLearner() Not yet implemented — helpLearnerParam() Not yet implemented — getLearnerId() Learner$id — setLearnerId() Learner$id — getLearnerModel() Learner$model — getLearnerNote() Not used anymore — getLearnerPackages() Learner$packages — getLearnerParVals() / getLearnerParamSet() Learner$param_set — getLearnerPredictType() Learner$predict_type — getLearnerShortName() Learner$predict_type — getLearnerType() Learner$Type — setPredictType() Learner$Type — getLearnerProperties ??? — getParamSet() Learner$param_set — trainLearner() Learner$train() — predictLearner() Learner$predict() — makeRLearner*() Learner — generateLearningCurveData() Not yet implemented mlr3viz FailureModel — — getFailureModelDump() — — getFailureModelMsg() — — isFailureModel() — — makeLearner() / makeLearners() ??? — Train/Predict/Resample train() Experiment$train() — predict() Experiment$predict() — performance() Experiment$score() — makeResampleDesc() Resampling mlr_resamplings resample() resample() — ResamplePrediction ResampleResult — Aggregation / makeAggregation Not yet implemented — asROCRPrediction() Not yet implemented — ConfusionMatrix / getConfMatrix() / calculateConfusionMatrix() Not yet implemented — calculateROCMeasures() Not yet implemented — estimateRelativeOverfitting() Not yet implemented — estimateResidualVariance() Not yet implemented — getDefaultMeasure() — getMeasureProperties() ??? — getPredictionResponse() / getPredictionSE() / getPredictionTruth() ??? — getPredictionDump() ??? — getPredictionTaskDesc() ??? — getRRDump() ??? — getRRPredictionList() ??? — getRRPredictions() ResampleResult$prediction — getRRTaskDesc() / getRRTaskDescription() ResampleResult$task$print() — Benchmark benchmark() benchmark() — batchmark() / reduceBatchmarkResults() not used anymore — BenchmarkResult BenchmarkResult — convertBMRToRankMatrix() Not yet implemented — convertMLBenchObjToTask() Not yet implemented — getBMRAggrPerformances() BenchmarkResult$aggregated() — getBMRFeatSelResults() Not yet implemented mlr3filters getBMRFilteredFeatures() Not yet implemented mlr3filters getBMRLearners() / getBMRLearnerIds() / getBMRLearnerShortNames() BenchmarkResult$learners — getBMRMeasures() / getBMRMeasureIds() BenchmarkResult$measures — getBMRModels() BenchmarkResult$data$learner[[1]]$model — getBMRPerformances() BenchmarkResult$data$performance — getBMRTaskDescriptions() / getBMRTaskDescs() / getBMRTaskIds() BenchmarkResult$tasks — getBMRTuneResults() Not yet implemented — getBMRPredictions() Not yet implemented — friedmanTestBMR() Not yet implemented — mergeBenchmarkResults() BenchmarkResult$combine() — plotBMRBoxplots() Not yet implemented mlr3viz plotBMRRanksAsBarChart() Not yet implemented mlr3viz plotBMRSummary() Not yet implemented mlr3viz plotResiduals() Not yet implemented mlr3viz Parameter Specification ParamHelpers::makeNumericParam() ParamDbl$new() paradox ParamHelpers::makeNumericVectorParam() ParamDbl$new() paradox ParamHelpers::makeIntegerParam() paradox::ParamInt$new() paradox ParamHelpers::makeIntegerVectorParam() paradox::ParamInt$new() paradox ParamHelpers::makeDiscreteParam() paradox::ParamFct$new() paradox ParamHelpers::makeDiscreteVectorParam() paradox::ParamFct$new() paradox ParamHelpers::makeLogicalParam() paradox::ParamLgl$new() paradox ParamHelpers::makeLogicalVectorParam() paradox::ParamLgl$new() paradox Preprocessing — — — — — — Feature Selection makeFeatSelControlExhaustive() Not yet implemented mlr3filters makeFeatSelControlRandom() Not yet implemented mlr3filters makeFeatSelControlSequential() Not yet implemented mlr3filters makeFeatSelControlGA() Not yet implemented mlr3filters makeFilter() Filter$new() mlr3filters FeatSelResult Not yet implemented mlr3filters listFilterMethods() mlr_filters mlr3filters analyzeFeatSelResult() Not yet implemented mlr3filters getBMRFeatSelResults() Not yet implemented mlr3filters getBMRFilteredFeatures() Not yet implemented mlr3filters getFeatSelResult() Not yet implemented mlr3filters getFeatureImportance() Not yet implemented mlr3filters getFilteredFeatures() Not yet implemented mlr3filters makeFeatSelWrapper() Not used anymore mlr3filters makeFilterWrapper() Not used anymore mlr3filters getResamplingIndices() Not yet implemented selectFeatures() Not yet implemented mlr3filters filterFeatures() Filter$filter_*() mlr3filters generateFilterValuesData() Filter$calculate() mlr3filters Tuning getTuneResult() Not yet implemented mlr3tuning getTuneResultOptPath() Not yet implemented mlr3tuning makeTuneControl*() Tuner mlr3tuning makeTuneMultiCritControl*() Tuner mlr3tuning Parallelization ParallelMap::parallelStart*(), parallelMap::parallelStop() future::plan() / future Plotting plotBMRBoxplots() Not yet implemented mlr3viz plotBMRRanksAsBarChart() Not yet implemented mlr3viz plotBMRSummary() Not yet implemented mlr3viz plotCalibration() Not yet implemented mlr3viz plotCritDifferences() Not yet implemented mlr3viz plotFilterValues() Not yet implemented mlr3viz plotHyperParsEffect() Not yet implemented mlr3viz plotLearnerPrediction() Not yet implemented mlr3viz plotLearningCurve() Not yet implemented mlr3viz plotPartialDependence() Not yet implemented mlr3viz plotResiduals() Not yet implemented mlr3viz plotROCCurves() Not yet implemented mlr3viz plotThreshVsPerf() Not yet implemented mlr3viz plotTuneMultiCritResult() Not yet implemented mlr3viz FDA extractFDAFPCA() Not yet implemented mlr3fda extractFDAFourier() Not yet implemented mlr3fda extractFDAMultiResFeatures() Not yet implemented mlr3fda extractFDAWavelets() Not yet implemented mlr3fda "],
["extending.html", "6 Extending", " 6 Extending This chapter gives instructions on how to extend mlr3 and its extension packages with custom objects. The approach is always the same: determine the base class you want to inherit from, extend the class with your custom functionality, test your implementation (optionally) add new object to the respective Dictionary. The chapter Create a new learner illustrates the steps needed to create a custom learner in mlr3. "],
["extending-learners.html", "6.1 Adding new Learners", " 6.1 Adding new Learners Here, we show how to create a custom LearnerClassif step-by-step. Preferably, you checkout our mlr3learnertemplate for new learners. Alternatively, here is a template snippet for a new classification learner: LearnerClassifYourLearner = R6::R6Class(&quot;LearnerClassifYourLearner&quot;, inherit = LearnerClassif, public = list( initialize = function(id = &quot;classif.yourlearner&quot;) { super$initialize( id = id, param_set = ParamSet$new(), predict_types = , feature_types = , properties = , packages = , ) }, train = function(task) { }, predict = function(task) { } ) ) In the first line of the template, we create a new R6 class with class &quot;LearnerClassifYourLearner&quot;. The next line determines the parent class: As we want to create a classification learner, we obviously want to inherit from LearnerClassif. A learner consists of three parts: Meta information about the learners A train_internal() function which takes a (filtered) TaskClassif and returns a model A predict_internal() function which operates on the model in self$model (stored during $train()) and a (differently subsetted) TaskClassif to return a named list of predictions. 6.1.1 Meta-information In the constructor function initialize() the constructor of the super class LearnerClassif is called with meta information about the learner we want to construct. This includes: id: The id of the new learner. packages: Set of required packages to run the learner. param_set: A set of hyperparameters and their description, provided as paradox::ParamSet. It is perfectly fine to add no parameters here for a first draft. For each hyperparameter you want to add, you have to select the appropriate class: paradox::ParamLgl for scalar logical hyperparameters. paradox::ParamInt for scalar integer hyperparameters. paradox::ParamDbl for scalar numeric hyperparameters. paradox::ParamFct for scalar factor hyperparameters (this includes characters). paradox::ParamUty for everything else. predict_types: Set of predict types the learner is capable of. These differ depending on the type of the learner. LearnerClassif response: Only predicts a class label for each observation in the test set. prob: Also predicts the posterior probability for each class for each observation in the test set. LearnerRegr response: Only predicts a numeric response for each observation in the test set. se: Also predicts the standard error for each value of response for each observation in the test set. feature_types: Set of feature types the learner can handle. See mlr_reflections$task_feature_types for feature types supported by mlr3. properties: Set of properties of the learner. Possible properties include: &quot;twoclass&quot;: The learner works on binary classification problems. &quot;multiclass&quot;: The learner works on multi-class classification problems. &quot;missings&quot;: The learner can natively handle missing values. &quot;weights&quot;: The learner can work on tasks which have observation weights / case weights. &quot;parallel&quot;: The learner can be parallelized, e.g. via threading. &quot;importance&quot;: The learner supports extracting importance values for features. If this property is set, you must also implement a public method importance() to retrieve the importance values from the model. &quot;selected_features&quot;: The learner supports extracting the features which where used. If this property is set, you must also implement a public method selected_features() to retrieve the set of used features from the model. For a simplified rpart::rpart(), the initialization could look like this: initialize = function(id = &quot;classif.rpart&quot;) { ps = ParamSet$new(list( ParamDbl$new(id = &quot;cp&quot;, default = 0.01, lower = 0, upper = 1, tags = &quot;train&quot;), ParamInt$new(id = &quot;xval&quot;, default = 10L, lower = 0L, tags = &quot;train&quot;) )) ps$values = list(xval = 0L) super$initialize( id = id, packages = &quot;rpart&quot;, feature_types = c(&quot;logical&quot;, &quot;integer&quot;, &quot;numeric&quot;, &quot;factor&quot;), predict_types = c(&quot;response&quot;, &quot;prob&quot;), param_set = ps, properties = c(&quot;twoclass&quot;, &quot;multiclass&quot;, &quot;weights&quot;, &quot;missings&quot;) ) } We only have specified a small subset of the available hyperparameters: The complexity &quot;cp&quot; is numeric, its feasible range is [0,1], it defaults to 0.01 and the parameter is used during &quot;train&quot;. The complexity &quot;xval&quot; is integer, its lower bound 0, its default is 0 and the parameter is also used during &quot;train&quot;. Note that we have changed the default here from 10 to 0 to save some computation time. This is not done by setting a different default in ParamInt$new(), but instead by setting the value explicitly. 6.1.2 Train function We continue the to adept the template for a rpart::rpart() learner, and now tackle the train_internal() function. The train function takes a Task as input and must return an arbitrary model. First, we write something down that works completely without mlr3: data = iris model = rpart::rpart(Species ~ ., data = iris, xval = 0) In the next step, we replace the data frame data with a Task: task = tsk(&quot;iris&quot;) model = rpart::rpart(Species ~ ., data = task$data(), xval = 0) The target variable &quot;Species&quot; is still hard-coded and specific to the task. This is unnecessary, as the information about the target variable is stored in the task: task$target_names ## [1] &quot;Species&quot; task$formula() ## Species ~ . ## NULL We can adapt our code accordingly: rpart::rpart(task$formula(), data = task$data(), xval = 0) ## n= 150 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 150 100 setosa (0.33333 0.33333 0.33333) ## 2) Petal.Length&lt; 2.45 50 0 setosa (1.00000 0.00000 0.00000) * ## 3) Petal.Length&gt;=2.45 100 50 versicolor (0.00000 0.50000 0.50000) ## 6) Petal.Width&lt; 1.75 54 5 versicolor (0.00000 0.90741 0.09259) * ## 7) Petal.Width&gt;=1.75 46 1 virginica (0.00000 0.02174 0.97826) * The last thing missing is the handling of hyperparameters. Instead of the hard-coded xval, we query the hyperparameter settings from the Learner itself. To illustrate this, we quickly construct the tree learner from the mlr3 package, and use the method get_value() from the ParamSet to retrieve all set hyperparameters with tag &quot;train&quot;. self = lrn(&quot;classif.rpart&quot;) self$param_set$get_values(tags = &quot;train&quot;) ## $xval ## [1] 0 To pass all hyperparameters down to the model fitting function, we recommend to use either do.call or the function mlr3misc::invoke(). pars = self$param_set$get_values(tags = &quot;train&quot;) mlr3misc::invoke(rpart::rpart, task$formula(), data = task$data(), .args = pars) ## n= 150 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 150 100 setosa (0.33333 0.33333 0.33333) ## 2) Petal.Length&lt; 2.45 50 0 setosa (1.00000 0.00000 0.00000) * ## 3) Petal.Length&gt;=2.45 100 50 versicolor (0.00000 0.50000 0.50000) ## 6) Petal.Width&lt; 1.75 54 5 versicolor (0.00000 0.90741 0.09259) * ## 7) Petal.Width&gt;=1.75 46 1 virginica (0.00000 0.02174 0.97826) * In the final learner, self will of course reference the learner itself. In the last step, we wrap everything in a function. train_internal = function(task) { pars = self$param_set$get_values(tags = &quot;train&quot;) mlr3misc::invoke(rpart::rpart, task$formula(), data = task$data(), .args = pars) } 6.1.3 Predict function The internal predict function predict_internal also operates on a Task as well as on the model stored during train() in self$model. The return value is a Prediction object. We proceed analogously to the section on the train function. We start with a version without any mlr3 objects and continue to replace objects until we have reached the desired interface: # inputs: task = tsk(&quot;iris&quot;) self = list(model = rpart::rpart(task$formula(), data = task$data())) data = iris response = predict(self$model, newdata = data, type = &quot;class&quot;) prob = predict(self$model, newdata = data, type = &quot;prob&quot;) The rpart::predict.rpart() function predicts class labels if argument type is set to to &quot;class&quot;, and class probabilities if set to &quot;prob&quot;. Next, we transition from data to a task again and construct a proper PredictionClassif object to return. Additionally, as we do not want to run the prediction twice, we differentiate what type of prediction is requested by querying the set predict type of the learner. The complete predict_internal function looks like this: predict_internal = function(task) { self$predict_type = &quot;response&quot; response = prob = NULL if (self$predict_type == &quot;response&quot;) { response = predict(self$model, newdata = task$data(), type = &quot;class&quot;) } else { prob = predict(self$model, newdata = task$data(), type = &quot;prob&quot;) } PredictionClassif$new(task, response = response, prob = prob) } Note that if the learner would need to handle hyperparameters during the predict step, we would proceed analogously to the train() step and use self$params(&quot;predict&quot;) in combination with mlr3misc::invoke(). Also note that you cannot rely on the column order of the data returned by task$data(), i.e. the order of columns may be different from the order of the columns during $train(). You have to make sure that your learner accesses columns by name, not by position (like some algorithms with a matrix interface do). You may have to restore the order manually here, see “classif.svm” for an example. 6.1.4 Final learner LearnerClassifYourRpart = R6::R6Class(&quot;LearnerClassifYourRpart&quot;, inherit = LearnerClassif, public = list( initialize = function(id = &quot;classif.rpart&quot;) { ps = ParamSet$new(list( ParamDbl$new(id = &quot;cp&quot;, default = 0.01, lower = 0, upper = 1, tags = &quot;train&quot;), ParamInt$new(id = &quot;xval&quot;, default = 0L, lower = 0L, tags = &quot;train&quot;) )) ps$values = list(xval = 0L) super$initialize( id = id, packages = &quot;rpart&quot;, feature_types = c(&quot;logical&quot;, &quot;integer&quot;, &quot;numeric&quot;, &quot;factor&quot;), predict_types = c(&quot;response&quot;, &quot;prob&quot;), param_set = ps, properties = c(&quot;twoclass&quot;, &quot;multiclass&quot;, &quot;weights&quot;, &quot;missings&quot;) ) }, train_internal = function(task) { pars = self$param_set$get_values(tag = &quot;train&quot;) mlr3misc::invoke(rpart::rpart, task$formula(), data = task$data(), .args = pars) }, predict_internal = function(task) { self$predict_type = &quot;response&quot; response = prob = NULL if (self$predict_type == &quot;response&quot;) { response = predict(self$model, newdata = task$data(), type = &quot;class&quot;) } else { prob = predict(self$model, newdata = task$data(), type = &quot;prob&quot;) } PredictionClassif$new(task, response = response, prob = prob) } ) ) lrn = LearnerClassifYourRpart$new() print(lrn) ## &lt;LearnerClassifYourRpart:classif.rpart&gt; ## * Model: - ## * Parameters: xval=0 ## * Packages: rpart ## * Predict Type: response ## * Feature types: logical, integer, numeric, factor ## * Properties: missings, multiclass, twoclass, weights To run some basic tests: task = tsk(&quot;iris&quot;) lrn$train(task) p = lrn$predict(task) p$confusion ## truth ## response setosa versicolor virginica ## setosa 50 0 0 ## versicolor 0 49 5 ## virginica 0 1 45 To run a bunch of automatic tests, you may source some auxiliary scripts from the unit tests of mlr3: helper = list.files(system.file(&quot;testthat&quot;, package = &quot;mlr3&quot;), pattern = &quot;^helper.*\\\\.[rR]&quot;, full.names = TRUE) ok = lapply(helper, source) stopifnot(run_autotest(lrn)) "],
["extending-pipeops.html", "6.2 Adding new PipeOps", " 6.2 Adding new PipeOps This section showcases how the mlr3pipelines package can be extended to include custom PipeOps. To run the following examples, we will need a Task; we are using the well-known “Iris” task: library(&quot;mlr3&quot;) task = mlr_tasks$get(&quot;iris&quot;) task$data() ## # A tibble: 150 x 5 ## Species Petal.Length Petal.Width Sepal.Length Sepal.Width ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 1.4 0.2 5.1 3.5 ## 2 setosa 1.4 0.2 4.9 3 ## 3 setosa 1.3 0.2 4.7 3.2 ## 4 setosa 1.5 0.2 4.6 3.1 ## 5 setosa 1.4 0.2 5 3.6 ## 6 setosa 1.7 0.4 5.4 3.9 ## 7 setosa 1.4 0.3 4.6 3.4 ## 8 setosa 1.5 0.2 5 3.4 ## 9 setosa 1.4 0.2 4.4 2.9 ## 10 setosa 1.5 0.1 4.9 3.1 ## # … with 140 more rows mlr3pipelines is fundamentally built around R6. When planning to create custom PipeOp objects, it can only help to familiarize yourself with it. In principle, all a PipeOp must do is inherit from the PipeOp R6 class and implement the train() and predict() functions. There are, however, several auxiliary subclasses that can make the creation of certain operations much easier. 6.2.1 General Case Example: PipeOpCopy A very simple yet useful PipeOp is PipeOpCopy, which takes a single input and creates a variable number of output channels, all of which receive a copy of the input data. It is a simple example that showcases the important steps in defining a custom PipeOp. We will show a simplified version here, PipeOpCopyTwo, that creates exactly two copies of its input data. The following figure visualizes how our PipeOp is situated in the Pipeline and the significant in- and outputs. 6.2.1.1 First Steps: Inheriting from PipeOp The first part of creating a custom PipeOp is inheriting from PipeOp. We make a mental note that we need to implement a train() and a predict() function, and that we probably want to have an initialize() as well: PipeOpCopyTwo = R6::R6Class(&quot;PipeOpCopyTwo&quot;, inherit = mlr3pipelines::PipeOp, public = list( initialize = function(id = &quot;copy.two&quot;) { .... }, train = function(inputs) { .... }, predict = function(inputs) { .... } ) ) 6.2.1.2 Channel Definitions We need to tell the PipeOp the layout of its channels: How many there are, what their names are going to be, and what types are acceptable. This is done on initialization of the PipeOp (using a super$initialize call) by giving the input and output data.table objects. These must have three columns: a &quot;name&quot; column giving the names of input and output channels, and a &quot;train&quot; and &quot;predict&quot; column naming the class of objects we expect during training and prediction as input / output. A special value for these classes is &quot;*&quot;, which indicates that any class will be accepted; our simple copy operator accepts any kind of input, so this will be useful. We have only one input, but two output channels. By convention, we name a single channel &quot;input&quot; or &quot;output&quot;, and a group of channels [&quot;input1&quot;, &quot;input2&quot;, …], unless there is a reason to give specific different names. Therefore, our input data.table will have a single row &lt;&quot;input&quot;, &quot;*&quot;, &quot;*&quot;&gt;, and our output table will have two rows, &lt;&quot;output1&quot;, &quot;*&quot;, &quot;*&quot;&gt; and &lt;&quot;output2&quot;, &quot;*&quot;, &quot;*&quot;&gt;. All of this is given to the PipeOp creator. Our initialize() will thus look as follows: initialize = function(id = &quot;copy.two&quot;) { input = data.table::data.table(name = &quot;input&quot;, train = &quot;*&quot;, predict = &quot;*&quot;) # the following will create two rows and automatically fill the `train` # and `predict` cols with &quot;*&quot; output = data.table::data.table( name = c(&quot;output1&quot;, &quot;output2&quot;), train = &quot;*&quot;, predict = &quot;*&quot; ) super$initialize(id, input = input, output = output ) } 6.2.1.3 Train and Predict Both train() and predict() will receive a list as input and must give a list in return. According to our input and output definitions, we will always get a list with a single element as input, and will need to return a list with two elements. Because all we want to do is create two copies, we will just create the copies using c(inputs, inputs). Two things to consider: The train() function must always modify the self$state variable to something that is not NULL or NO_OP. This is because the $state slot is used as a signal that PipeOp has been trained on data, even if the state itself is not important to the PipeOp (as in our case). Therefore, our train() will set self$state = list(). It is not necessary to “clone” our input or make deep copies, because we don’t modify the data. However, if we were changing a reference-passed object, for example by changing data in a Task, we would have to make a deep copy first. This is because a PipeOp may never modify its input object by reference. Our train() and predict() functions are now: train = function(inputs) { self$state = list() c(inputs, inputs) } predict = function(inputs) { c(inputs, inputs) } 6.2.1.4 Putting it Together The whole definition thus becomes PipeOpCopyTwo = R6::R6Class(&quot;PipeOpCopyTwo&quot;, inherit = mlr3pipelines::PipeOp, public = list( initialize = function(id = &quot;copy.two&quot;) { super$initialize(id, input = data.table::data.table(name = &quot;input&quot;, train = &quot;*&quot;, predict = &quot;*&quot;), output = data.table::data.table(name = c(&quot;output1&quot;, &quot;output2&quot;), train = &quot;*&quot;, predict = &quot;*&quot;) ) }, train = function(inputs) { self$state = list() c(inputs, inputs) }, predict = function(inputs) { c(inputs, inputs) } ) ) We can create an instance of our PipeOp, put it in a graph, and see what happens when we train it on something: library(&quot;mlr3pipelines&quot;) poct = PipeOpCopyTwo$new() gr = Graph$new() gr$add_pipeop(poct) print(gr) ## Graph with 1 PipeOps: ## ID State sccssors prdcssors ## copy.two &lt;&lt;UNTRAINED&gt;&gt; result = gr$train(task) str(result) ## List of 2 ## $ copy.two.output1:Classes &#39;TaskClassif&#39;, &#39;TaskSupervised&#39;, &#39;Task&#39;, &#39;R6&#39; &lt;TaskClassif:iris&gt; ## $ copy.two.output2:Classes &#39;TaskClassif&#39;, &#39;TaskSupervised&#39;, &#39;Task&#39;, &#39;R6&#39; &lt;TaskClassif:iris&gt; 6.2.2 Special Case: Preprocessing Many PipeOps perform an operation on exactly one Task, and return exactly one Task. They may even not care about the “Target” / “Outcome” variable of that task, and only do some modification of some input data. However, it is usually important to them that the Task on which they perform prediction has the same data columns as the Task on which they train. For these cases, the auxiliary base class PipeOpTaskPreproc exists. It inherits from PipeOp itself, and other PipeOps should use it if they fall in the kind of use-case named above. When inheriting from PipeOpTaskPreproc, one must either implement the train_task and predict_task functions, or the train_dt, predict_dt functions, depending on whether wants to operate on a Task object or on data.tables. In the second case, one can optionally also overload the select_cols function, which chooses which of the incoming Task’s features are given to the train_dt / predict_dt functions. The following will show two examples: PipeOpDropNA, which removes a Task’s rows with missing values during training (and implements train_task and predict_task), and PipeOpScale, which scales a Task’s numeric columns (and implements train_dt, predict_dt, and select_cols). 6.2.2.1 Example: PipeOpDropNA Dropping rows with missing values may be important when training a model that can not handle them. Because mlr3 Tasks only contain a view to the underlying data, it is not necessary to modify data to remove rows with missing values. Instead, the rows can be removed using the Task’s $filter method, which modifies the Task in-place. This is done in the train_task function. We take care that we also set the $state slot to signal that the PipeOp was trained. The predict_task function does not need to do anything; removing missing values during prediction is not as useful, since learners that cannot handle them will just ignore the respective rows. Furthermore, mlr3 expects a Learner to always return just as many predictions as it was given input rows, so a PipeOp that removes Task rows during training can not be used inside a GraphLearner. When we inherit from PipeOpTaskPreproc, it sets the input and output data.tables for us to only accept a single Task. The only thing we do during initialize() is therefore to set an id (which can optionally be changed by the user). The complete PipeOpDropNA can therefore be written as follows. Note that it inherits from PipeOpTaskPreproc, unlike the PipeOpCopyTwo example from above: PipeOpDropNA = R6::R6Class(&quot;PipeOpDropNA&quot;, inherit = mlr3pipelines::PipeOpTaskPreproc, public = list( initialize = function(id = &quot;drop.na&quot;) { super$initialize(id) }, train_task = function(task) { self$state = list() featuredata = task$data(cols = task$feature_names) exclude = apply(is.na(featuredata), 1, any) task$filter(task$row_ids[!exclude]) }, predict_task = function(task) { # nothing to be done task } ) ) To test this PipeOp, we create a small task with missing values: smalliris = iris[(1:5) * 30, ] smalliris[1, 1] = NA smalliris[2, 2] = NA sitask = TaskClassif$new(&quot;smalliris&quot;, as_data_backend(smalliris), &quot;Species&quot;) print(sitask$data()) ## Species Petal.Length Petal.Width Sepal.Length Sepal.Width ## 1: setosa 1.6 0.2 NA 3.2 ## 2: versicolor 3.9 1.4 5.2 NA ## 3: versicolor 4.0 1.3 5.5 2.5 ## 4: virginica 5.0 1.5 6.0 2.2 ## 5: virginica 5.1 1.8 5.9 3.0 We test this by feeding it to a new Graph that uses PipeOpDropNA. gr = Graph$new() gr$add_pipeop(PipeOpDropNA$new()) filtered_task = gr$train(sitask)[[1]] print(filtered_task$data()) ## Species Petal.Length Petal.Width Sepal.Length Sepal.Width ## 1: versicolor 4.0 1.3 5.5 2.5 ## 2: virginica 5.0 1.5 6.0 2.2 ## 3: virginica 5.1 1.8 5.9 3.0 6.2.2.2 Example: PipeOpScaleAlways An often-applied preprocessing step is to simply center and/or scale the data to mean \\(0\\) and standard deviation \\(1\\). This fits the PipeOpTaskPreproc pattern quite well. Because it always replaces all columns that it operates on, and does not require any information about the task’s target, it only needs to overload the train_dt and predict_dt functions. This saves some boilerplate-code from getting the correct feature columns out of the task, and replacing them after modification. Because scaling only makes sense on numeric features, we want to instruct PipeOpTaskPreproc to give us only these numeric columns. We do this by overloading the select_cols function: It is called by the class to determine which columns to give to train_dt and predict_dt. Its input is the Task that is being transformed, and it should return a character vector of all features to work with. When it is not overloaded, it uses all columns; instead, we will set it to only give us numeric columns. Because the levels() of the data table given to train_dt and predict_dt may be different from the levels task’s levels, these functions must also take a levels argument that is a named list of column names indicating their levels. When working with numeric data, this argument can be ignored, but it should be used instead of levels(dt[[column]]) for factorial or character columns. This is the first PipeOp where we will be using the $state slot for something useful: We save the centering offset and scaling coefficient and use it in $predict()! For simplicity, we are not using hyperparameters and will always scale and center all data. Compare this PipeOpScaleAlways operator to the one defined inside the mlr3pipelines package, PipeOpScale, defined in PipeOpScale.R. PipeOpScaleAlways = R6::R6Class(&quot;PipeOpScaleAlways&quot;, inherit = mlr3pipelines::PipeOpTaskPreproc, public = list( initialize = function(id = &quot;scale.always&quot;) { super$initialize(id = id) }, select_cols = function(task) { task$feature_types[type == &quot;numeric&quot;, id] }, train_dt = function(dt, levels, target) { sc = scale(as.matrix(dt)) self$state = list( center = attr(sc, &quot;scaled:center&quot;), scale = attr(sc, &quot;scaled:scale&quot;) ) sc }, predict_dt = function(dt, levels) { t((t(dt) - self$state$center) / self$state$scale) } ) ) (Note for the observant: If you check PipeOpScale.R from the mlr3pipelines package, you will notice that is uses “get(&quot;type&quot;)” and “get(&quot;id&quot;)” instead of “type” and “id”, because the static code checker on CRAN would otherwise complain about references to undefined variables. This is a “problem” with data.table and not exclusive to mlr3pipelines.) We can, again, create a new Graph that uses this PipeOp to test it. Compare the resulting data to the original “iris” Task data printed at the beginning: gr = Graph$new() gr$add_pipeop(PipeOpScaleAlways$new()) result = gr$train(task) result[[1]]$data() ## # A tibble: 150 x 5 ## Species Petal.Length Petal.Width Sepal.Length Sepal.Width ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa -1.34 -1.31 -0.898 1.02 ## 2 setosa -1.34 -1.31 -1.14 -0.132 ## 3 setosa -1.39 -1.31 -1.38 0.327 ## 4 setosa -1.28 -1.31 -1.50 0.0979 ## 5 setosa -1.34 -1.31 -1.02 1.25 ## 6 setosa -1.17 -1.05 -0.535 1.93 ## 7 setosa -1.34 -1.18 -1.50 0.786 ## 8 setosa -1.28 -1.31 -1.02 0.786 ## 9 setosa -1.34 -1.31 -1.74 -0.361 ## 10 setosa -1.28 -1.44 -1.14 0.0979 ## # … with 140 more rows 6.2.3 Special Case: Preprocessing with Simple Train It is possible to make even further simplifications for many PipeOps that perform mostly the same operation during training and prediction. The point of Task preprocessing is often to modify the training data in mostly the same way as prediction data (but in a way that may depend on training data). Consider constant feature removal, for example: The goal is to remove features that have no variance, or only a single factor level. However, what features get removed must be decided during training, and may only depend on training data. Furthermore, the actual process of removing features is the same during training and prediction. A simplification to make is therefore to have a function get_state(task) which sets the $state slot during training, and a transform(task) function, which gets called both during training and prediction. This is done in the PipeOpTaskPreprocSimple class. Just like PipeOpTaskPreproc, one can inherit from this and overload these functions to get a PipeOp that performs preprocessing with very little boilerplate code. Just like PipeOpTaskPreproc, PipeOpTaskPreprocSimple offers the possibility to instead overload the get_state_dt(dt, levels) and transform_dt(dt, levels) functions (and optionally, again, the select_cols(task) function) to operate on data.table feature data instead of the whole Task. Even some methods that do not use PipeOpTaskPreprocSimple could work in a similar way: The PipeOpScaleAlways example from above will be shown to also work with this paradigm. 6.2.3.1 Example: PipeOpDropConst A typical example of a preprocessing operation that does almost the same operation during training and prediction is an operation that drops features depending on a criterion that is evaluated during training. One simple example of this is dropping constant features. Because the mlr3 Task class offers a flexible view on underlying data, it is most efficient to drop columns from the task directly using its $select() function, so the get_state_dt(dt, levels) / transform_dt(dt, levels) functions will not get used; instead we overload the get_state(task) and transform(task) functions. The get_state() function’s result is saved to the $state slot, so we want to return something that is useful for dropping features. We choose to save the names of all the columns that have nonzero variance. For brevity, we use length(unique(column)) &gt; 1 to check whether more than one distinct value is present; a more sophisticated version could have a tolerance parameter for numeric values that are very close to each other. The transform() function is evaluated both during training and prediction, and can rely on the $state slot being present. All it does here is call the Task$select function with the columns we chose to keep. The full PipeOp could be written as follows: PipeOpDropConst = R6::R6Class(&quot;PipeOpDropConst&quot;, inherit = mlr3pipelines::PipeOpTaskPreprocSimple, public = list( initialize = function(id = &quot;drop.const&quot;) { super$initialize(id = id) }, get_state = function(task) { data = task$data(cols = task$feature_names) nonconst = sapply(data, function(column) length(unique(column)) &gt; 1) list(cnames = colnames(data)[nonconst]) }, transform = function(task) { task$select(self$state$cnames) } ) ) This can be tested using the first five rows of the “Iris” Task, for which one feature (&quot;Petal.Width&quot;) is constant: irishead = task$clone()$filter(1:5) irishead$data() ## # A tibble: 5 x 5 ## Species Petal.Length Petal.Width Sepal.Length Sepal.Width ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 1.4 0.2 5.1 3.5 ## 2 setosa 1.4 0.2 4.9 3 ## 3 setosa 1.3 0.2 4.7 3.2 ## 4 setosa 1.5 0.2 4.6 3.1 ## 5 setosa 1.4 0.2 5 3.6 gr = Graph$new()$add_pipeop(PipeOpDropConst$new()) dropped_task = gr$train(irishead)[[1]] dropped_task$data() ## # A tibble: 5 x 4 ## Species Petal.Length Sepal.Length Sepal.Width ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 1.4 5.1 3.5 ## 2 setosa 1.4 4.9 3 ## 3 setosa 1.3 4.7 3.2 ## 4 setosa 1.5 4.6 3.1 ## 5 setosa 1.4 5 3.6 We can also see that the $state was correctly set. Calling $predict() with this graph, even with different data (the whole Iris Task!) will still drop the &quot;Petal.Width&quot; column, as it should. gr$pipeops$drop.const$state ## $cnames ## [1] &quot;Petal.Length&quot; &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; ## ## $affected_cols ## [1] &quot;Petal.Length&quot; &quot;Petal.Width&quot; &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; ## ## $intasklayout ## id type ## 1: Petal.Length numeric ## 2: Petal.Width numeric ## 3: Sepal.Length numeric ## 4: Sepal.Width numeric ## ## $outtasklayout ## id type ## 1: Petal.Length numeric ## 2: Sepal.Length numeric ## 3: Sepal.Width numeric dropped_predict = gr$predict(task)[[1]] dropped_predict$data() ## # A tibble: 150 x 4 ## Species Petal.Length Sepal.Length Sepal.Width ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 1.4 5.1 3.5 ## 2 setosa 1.4 4.9 3 ## 3 setosa 1.3 4.7 3.2 ## 4 setosa 1.5 4.6 3.1 ## 5 setosa 1.4 5 3.6 ## 6 setosa 1.7 5.4 3.9 ## 7 setosa 1.4 4.6 3.4 ## 8 setosa 1.5 5 3.4 ## 9 setosa 1.4 4.4 2.9 ## 10 setosa 1.5 4.9 3.1 ## # … with 140 more rows 6.2.3.2 Example: PipeOpScaleAlwaysSimple This example will show how a PipeOpTaskPreprocSimple can be used when only working on feature data in form of a data.table. Instead of calling the scale() function, the center and scale values are calculated directly and saved to the $state slot. The transform_dt function will then perform the same operation during both training and prediction: subtract the center and divide by the scale value. As in the PipeOpScaleAlways example above, we use select_cols() so that we only work on numeric columns. PipeOpScaleAlwaysSimple = R6::R6Class(&quot;PipeOpScaleAlwaysSimple&quot;, inherit = mlr3pipelines::PipeOpTaskPreprocSimple, public = list( initialize = function(id = &quot;scale.always.simple&quot;) { super$initialize(id = id) }, select_cols = function(task) { task$feature_types[type == &quot;numeric&quot;, id] }, get_state_dt = function(dt, levels, target) { list( center = sapply(dt, mean), scale = sapply(dt, sd) ) }, transform_dt = function(dt, levels) { t((t(dt) - self$state$center) / self$state$scale) } ) ) We can compare this PipeOp to the one above to show that it behaves the same. gr = Graph$new()$add_pipeop(PipeOpScaleAlways$new()) result_posa = gr$train(task)[[1]] gr = Graph$new()$add_pipeop(PipeOpScaleAlwaysSimple$new()) result_posa_simple = gr$train(task)[[1]] result_posa$data() ## # A tibble: 150 x 5 ## Species Petal.Length Petal.Width Sepal.Length Sepal.Width ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa -1.34 -1.31 -0.898 1.02 ## 2 setosa -1.34 -1.31 -1.14 -0.132 ## 3 setosa -1.39 -1.31 -1.38 0.327 ## 4 setosa -1.28 -1.31 -1.50 0.0979 ## 5 setosa -1.34 -1.31 -1.02 1.25 ## 6 setosa -1.17 -1.05 -0.535 1.93 ## 7 setosa -1.34 -1.18 -1.50 0.786 ## 8 setosa -1.28 -1.31 -1.02 0.786 ## 9 setosa -1.34 -1.31 -1.74 -0.361 ## 10 setosa -1.28 -1.44 -1.14 0.0979 ## # … with 140 more rows result_posa_simple$data() ## # A tibble: 150 x 5 ## Species Petal.Length Petal.Width Sepal.Length Sepal.Width ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa -1.34 -1.31 -0.898 1.02 ## 2 setosa -1.34 -1.31 -1.14 -0.132 ## 3 setosa -1.39 -1.31 -1.38 0.327 ## 4 setosa -1.28 -1.31 -1.50 0.0979 ## 5 setosa -1.34 -1.31 -1.02 1.25 ## 6 setosa -1.17 -1.05 -0.535 1.93 ## 7 setosa -1.34 -1.18 -1.50 0.786 ## 8 setosa -1.28 -1.31 -1.02 0.786 ## 9 setosa -1.34 -1.31 -1.74 -0.361 ## 10 setosa -1.28 -1.44 -1.14 0.0979 ## # … with 140 more rows 6.2.4 Hyperparameters mlr3pipelines uses the paradox package to define parameter spaces for PipeOps. Parameters for PipeOps can modify their behavior in certain ways, e.g. switch centering or scaling off in the PipeOpScale operator. The unified interface makes it possible to have parameters for whole Graphs that modify the individual PipeOp’s behavior. The Graphs, when encapsulated in GraphLearners, can even be tuned using the tuning functionality in mlr3tuning. Hyperparameters are declared during initialization, when calling the PipeOp’s $initialize() function, by giving a param_set argument. The param_set must be a ParamSet from the paradox package; see the mlr3book for more information on how to define parameter spaces. After construction, the ParamSet can be accessed through the $param_set slot. While it is possible to modify this ParamSet, using e.g. the $add() and $add_dep() functions, after adding it to the PipeOp, it is strongly advised against. Hyperparameters can be set and queried through the $values slot. When setting hyperparameters, they are automatically checked to satisfy all conditions set by the $param_set, so it is not necessary to type check them. Be aware that it is always possible to remove hyperparameter values. When a PipeOp is initialized, it usually does not have any parameter values—$values takes the value list(). It is possible to set initial parameter values in the $initialize() constructor; this must be done after the super$initialize() call where the corresponding ParamSet must be supplied. This is because setting $values checks against the current $param_set, which would fail if the $param_set was not set yet. When using an underlying library function (the scale function in PipeOpScale, say), then there is usually a “default” behaviour of that function when a parameter is not given. It is good practice to use this default behaviour whenever a parameter is not set (or when it was removed). This can easily be done when using the mlr3misc library’s mlr3misc::invoke() function, which has functionality similar to do.call(). 6.2.4.1 Hyperparameter Example: PipeOpScale How to use hyperparameters can best be shown through the example of PipeOpScale, which is very similar to the example above, PipeOpScaleAlways. The difference is made by the presence of hyperparameters. PipeOpScale constructs a ParamSet in its $initialize function and passes this on to the super$initialize function: PipeOpScale$public_methods$initialize ## function (id = &quot;scale&quot;, param_vals = list()) ## { ## ps = ParamSet$new(params = list(ParamLgl$new(&quot;center&quot;, default = TRUE, ## tags = c(&quot;train&quot;, &quot;scale&quot;)), ParamLgl$new(&quot;scale&quot;, default = TRUE, ## tags = c(&quot;train&quot;, &quot;scale&quot;)))) ## super$initialize(id = id, param_set = ps, param_vals = param_vals) ## } ## &lt;bytecode: 0xa7510c0&gt; ## &lt;environment: namespace:mlr3pipelines&gt; The user has access to this and can set and get parameters. Types are automatically checked: pss = PipeOpScale$new() print(pss$param_set) ## ParamSet: scale ## id class lower upper levels default value ## 1: center ParamLgl NA NA TRUE,FALSE TRUE ## 2: scale ParamLgl NA NA TRUE,FALSE TRUE ## 3: affect_columns ParamUty NA NA &lt;Selector&gt; pss$param_set$values$center = FALSE print(pss$param_set$values) ## $center ## [1] FALSE pss$param_set$values$scale = &quot;TRUE&quot; # bad input is checked! ## Error in (function (xs) : Assertion on &#39;xs&#39; failed: scale: Must be of type &#39;logical flag&#39;, not &#39;character&#39;. How PipeOpScale handles its parameters can be seen in its $train method: It gets the relevant parameters from its $values slot and uses them in the mlr3misc::invoke() call. This has the advantage over calling scale() directly that if a parameter is not given, its default value from the scale() function will be used. PipeOpScale$public_methods$train ## function (dt, levels, target) ## { ## sc = invoke(scale, as.matrix(dt), .args = self$param_set$get_values(tags = &quot;scale&quot;)) ## self$state = list(center = attr(sc, &quot;scaled:center&quot;) %??% ## 0, scale = attr(sc, &quot;scaled:scale&quot;) %??% 1) ## constfeat = self$state$scale == 0 ## self$state$scale[constfeat] = 1 ## sc[, constfeat] = 0 ## sc ## } ## &lt;bytecode: 0xa74fdd8&gt; ## &lt;environment: namespace:mlr3pipelines&gt; Another change that is necessary compared to PipeOpScaleAlways is that the attributes &quot;scaled:scale&quot; and &quot;scaled:center&quot; are not always present, depending on parameters, and possibly need to be set to default values \\(1\\) or \\(0\\), respectively. It is now even possible (if a bit pointless) to call PipeOpScale with both scale and center set to FALSE, which returns the original dataset, unchanged. pss$param_set$values$scale = FALSE pss$param_set$values$center = FALSE gr = Graph$new() gr$add_pipeop(pss) result = gr$train(task) result[[1]]$data() ## # A tibble: 150 x 5 ## Species Petal.Length Petal.Width Sepal.Length Sepal.Width ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 1.4 0.2 5.1 3.5 ## 2 setosa 1.4 0.2 4.9 3 ## 3 setosa 1.3 0.2 4.7 3.2 ## 4 setosa 1.5 0.2 4.6 3.1 ## 5 setosa 1.4 0.2 5 3.6 ## 6 setosa 1.7 0.4 5.4 3.9 ## 7 setosa 1.4 0.3 4.6 3.4 ## 8 setosa 1.5 0.2 5 3.4 ## 9 setosa 1.4 0.2 4.4 2.9 ## 10 setosa 1.5 0.1 4.9 3.1 ## # … with 140 more rows "],
["special-tasks.html", "7 Special Tasks", " 7 Special Tasks This chapter explores the different functions of mlr3 when dealing with specific data sets that require further statistical modification to undertake sensible analysis. Following topics are discussed: Survival Analysis This sub-chapter explains how to conduct sound survival analysis in mlr3. Survival analysis is used to monitor the period of time until a specific event takes places. This specific event could be e.g. death, transmission of a disease, marriage or divorce. Two considerations are important when conducting survival analysis: Whether the event occurred within the frame of the given data How much time it took until the event occurred In summary, this sub-chapter explains how to account for these considerations and conduct survival analysis using the mlr3proba extension package. Spatial Analysis Spatial analysis data observations entail reference information about spatial characteristics. One of the largest shortcomings of spatial data analysis is the inevitable auto-correlation in spatial data. Auto-correlation is especially severe in data with marginal spatial variation. The sub-chapter on spatial analysis provides instructions on how to handle the problems associated with spatial data accordingly. Ordinal Analysis This is work in progress. See mlr3ordinal for the current state. Functional Analysis Functional analysis contains data that consists of curves varying over a continuum e.g. time, frequency or wavelength. This type of analysis is frequently used when examining measurements over various time points. Steps on how to accommodate functional data structures in mlr3 are explained in the functional analysis sub-chapter. Multilabel Classification Multilabel classification deals with objects that can belong to more than one category at the same time. Numerous target labels are attributed to a single observation. Working with multilabel data requires one to use modified algorithms, to accommodate data specific characteristics. Two approaches to multilabel classification are prominently used: The problem transformation method The algorithm adaption method Instructions on how to deal with multilabel classification in mlr3 can be found in this sub-chapter. Cost Sensitive Classification This sub-chapter deals with the implementation of cost-sensitive classification. Regular classification aims to minimize the misclassification rate and thus all types of misclassification errors are deemed equally severe. Cost-sensitive classification is a setting where the costs caused by different kinds of errors are not assumed to be equal. The objective is to minimize the expected costs. Analytical data for a big credit institution is used as a use case to illustrate the different features. Firstly, the sub-chapter provides guidance on how to implement a first model. Subsequently, the sub-chapter contains instructions on how to modify cost sensitivity measures, thresholding and threshold tuning. "],
["survival.html", "7.1 Survival Analysis", " 7.1 Survival Analysis Survival analysis examines data on whether a specific event of interest takes place and how long it takes till this event occurs. One cannot use ordinary regression analysis when dealing with survival analysis data sets. Firstly, survival data contains solely positive values and therefore needs to be transformed to avoid biases. Secondly, ordinary regression analysis cannot deal with censored observations accordingly. Censored observations are observations in which the event of interest has not occurred, yet. Survival analysis allows the user to handle censored data with limited time frames that sometimes do not entail the event of interest. Note that survival analysis accounts for both censored and uncensored observations while adjusting respective model parameters. The package mlr3proba extends mlr3 with the following objects for survival analysis: TaskSurv to define (right-censored) survival tasks LearnerSurv as base class for survival learners PredictionSurv as specialized class for Prediction objects MeasureSurv as specialized class for performance measures In this example we demonstrate the basic functionality of the package on the rats data from the survival package. This task ships as pre-defined TaskSurv with mlr3proba. library(&quot;mlr3proba&quot;) task = tsk(&quot;rats&quot;) print(task) ## &lt;TaskSurv:rats&gt; (300 x 5) ## * Target: time, status ## * Properties: - ## * Features (3): ## - int (2): litter, rx ## - fct (1): sex # the target column is a survival object: head(task$truth()) ## [1] 101+ 49 104+ 91+ 104+ 102+ # kaplan-meier plot library(&quot;mlr3viz&quot;) autoplot(task) Now, we conduct a small benchmark study on the rats task using some of the integrated survival learners: # some integrated learners learners = lapply(c(&quot;surv.coxph&quot;, &quot;surv.kaplan&quot;, &quot;surv.ranger&quot;), lrn) print(learners) ## [[1]] ## &lt;LearnerSurvCoxPH:surv.coxph&gt; ## * Model: - ## * Parameters: list() ## * Packages: survival, distr6 ## * Predict Type: distr ## * Feature types: logical, integer, numeric, factor ## * Properties: weights ## ## [[2]] ## &lt;LearnerSurvKaplan:surv.kaplan&gt; ## * Model: - ## * Parameters: list() ## * Packages: survival, distr6 ## * Predict Type: crank ## * Feature types: logical, integer, numeric, character, factor, ordered ## * Properties: missings ## ## [[3]] ## &lt;LearnerSurvRanger:surv.ranger&gt; ## * Model: - ## * Parameters: list() ## * Packages: ranger, distr6 ## * Predict Type: distr ## * Feature types: logical, integer, numeric, character, factor, ordered ## * Properties: importance, oob_error, weights # Uno&#39;s C-Index for survival measure = msr(&quot;surv.unoC&quot;) print(measure) ## &lt;MeasureSurvUnoC:surv.unoC&gt; ## * Packages: survAUC ## * Range: [0, 1] ## * Minimize: FALSE ## * Properties: na_score, requires_task, requires_train_set ## * Predict type: crank set.seed(1) bmr = benchmark(benchmark_grid(task, learners, rsmp(&quot;cv&quot;, folds = 3))) bmr$aggregate(measure) ## # A tibble: 3 x 7 ## nr resample_result task_id learner_id resampling_id iters surv.unoC ## &lt;int&gt; &lt;list&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 &lt;RsmplRsl&gt; rats surv.coxph cv 3 0.904 ## 2 2 &lt;RsmplRsl&gt; rats surv.kaplan cv 3 0 ## 3 3 &lt;RsmplRsl&gt; rats surv.ranger cv 3 0.864 autoplot(bmr, measure = measure) "],
["spatial.html", "7.2 Spatial Analysis", " 7.2 Spatial Analysis Spatial data observations entail reference information about spatial characteristics. This information is frequently stored as coordinates named ‘x’ and ‘y’. Treating spatial data using non-spatial data methods could lead to over-optimistic treatment. This is due to the underlying auto-correlation in spatial data. See mlr3spatiotemporal for the current state of the implementation. "],
["ordinal.html", "7.3 Ordinal Analysis", " 7.3 Ordinal Analysis This is work in progress. See mlr3ordinal for the current state of the implementation. "],
["functional.html", "7.4 Functional Analysis", " 7.4 Functional Analysis Functional data is data containing an ordering on the dimensions. This implies that functional data consists of curves varying over a continuum, such as time, frequency, or wavelength. 7.4.1 How to model functional data? There are two ways to model functional data: Modification of the learner, so that the learner is suitable for the functional data Modification of the task, so that the task matches the standard- or classification-learner More following soon! "],
["multilabel.html", "7.5 Multilabel Classification", " 7.5 Multilabel Classification Multilabel deals with objects that can belong to more than one category at the same time. More following soon! "],
["cost-sens.html", "7.6 Cost-Sensitive Classification", " 7.6 Cost-Sensitive Classification In regular classification the aim is to minimize the misclassification rate and thus all types of misclassification errors are deemed equally severe. A more general setting is cost-sensitive classification. Cost sensitive classification does not assume that the costs caused by different kinds of errors are equal. The objective of cost sensitive classification is to minimize the expected costs. Imagine you are an analyst for a big credit institution. Let’s also assume that a correct decision of the bank would result in 35% of the profit at the end of a specific period. A correct decision means that the bank predicts that a customer will pay their bills (hence would obtain a loan), and the customer indeed has good credit. On the other hand, a wrong decision means that the bank predicts that the customer’s credit is in good standing, but the opposite is true. This would result in a loss of 100% of the given loan. Good Customer (truth) Bad Customer (truth) Good Customer (predicted) + 0.35 - 1.0 Bad Customer (predicted) 0 0 Expressed as costs (instead of profit), we can write down the cost-matrix as follows: costs = matrix(c(-0.35, 0, 1, 0), nrow = 2) dimnames(costs) = list(response = c(&quot;good&quot;, &quot;bad&quot;), truth = c(&quot;good&quot;, &quot;bad&quot;)) print(costs) ## truth ## response good bad ## good -0.35 1 ## bad 0.00 0 An exemplary data set for such a problem is the German Credit task: library(&quot;mlr3&quot;) task = tsk(&quot;german_credit&quot;) table(task$truth()) ## ## good bad ## 700 300 The data has 70% customers who are able to pay back their credit, and 30% bad customers who default on the debt. A manager, who doesn’t have any model, could decide to give either everybody a credit or to give nobody a credit. The resulting costs for the German credit data are: # nobody: (700 * costs[2, 1] + 300 * costs[2, 2]) / 1000 ## [1] 0 # everybody (700 * costs[1, 1] + 300 * costs[1, 2]) / 1000 ## [1] 0.055 If the average loan is $20,000, the credit institute would lose more than one million dollar if it would grant everybody a credit: # average profit * average loan * number of customers 0.055 * 20000 * 1000 ## [1] 1100000 Our goal is to find a model which minimizes the costs (and thereby maximizes the expected profit). 7.6.1 A First Model For our first model, we choose an ordinary logistic regression (implemented in the add-on package mlr3learners). We first create a classification task, then resample the model using a 10-fold cross validation and extract the resulting confusion matrix: library(&quot;mlr3learners&quot;) learner = lrn(&quot;classif.log_reg&quot;) rr = resample(task, learner, rsmp(&quot;cv&quot;)) confusion = rr$prediction()$confusion print(confusion) ## truth ## response good bad ## good 602 154 ## bad 98 146 To calculate the average costs like above, we can simply multiply the elements of the confusion matrix with the elements of the previously introduced cost matrix, and sum the values of the resulting matrix: avg_costs = sum(confusion * costs) / 1000 print(avg_costs) ## [1] -0.0567 With an average loan of $20,000, the logistic regression yields the following costs: avg_costs * 20000 * 1000 ## [1] -1134000 Instead of losing over $1,000,000, the credit institute now can expect a profit of more than $1,000,000. 7.6.2 Cost-sensitive Measure Our natural next step would be to further improve the modeling step in order to maximize the profit. For this purpose we first create a cost-sensitive classification measure which calculates the costs based on our cost matrix. This allows us to conveniently quantify and compare modeling decisions. Fortunately, there already is a predefined measure Measure for this purpose: MeasureClassifCosts: cost_measure = msr(&quot;classif.costs&quot;, costs = costs) print(cost_measure) ## &lt;MeasureClassifCosts:classif.costs&gt; ## * Packages: - ## * Range: [-Inf, Inf] ## * Minimize: TRUE ## * Properties: requires_task ## * Predict type: response If we now call resample() or benchmark(), the cost-sensitive measures will be evaluated. We compare the logistic regression to a simple featureless learner and to a random forest from package ranger : learners = list( lrn(&quot;classif.log_reg&quot;), lrn(&quot;classif.featureless&quot;), lrn(&quot;classif.ranger&quot;) ) cv3 = rsmp(&quot;cv&quot;, folds = 3) bmr = benchmark(benchmark_grid(task, learners, cv3)) bmr$aggregate(cost_measure) ## # A tibble: 3 x 7 ## nr resample_result task_id learner_id resampling_id iters classif.costs ## &lt;int&gt; &lt;list&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 &lt;RsmplRsl&gt; german_c… classif.log… cv 3 -0.0544 ## 2 2 &lt;RsmplRsl&gt; german_c… classif.fea… cv 3 0.0550 ## 3 3 &lt;RsmplRsl&gt; german_c… classif.ran… cv 3 -0.0400 As expected, the featureless learner is performing comparably bad. The logistic regression and the random forest work equally well. 7.6.3 Thresholding Although we now correctly evaluate the models in a cost-sensitive fashion, the models themselves are unaware of the classification costs. They assume the same costs for both wrong classification decisions (false positives and false negatives). Some learners natively support cost-sensitive classification (e.g., XXX). However, we will concentrate on a more generic approach which works for all models which can predict probabilities for class labels: thresholding. Most learners can calculate the probability \\(p\\) for the positive class. If \\(p\\) exceeds the threshold \\(0.5\\), they predict the positive class, and the negative class otherwise. For our binary classification case of the credit data, the we primarily want to minimize the errors where the model predicts “good”, but truth is “bad” (i.e., the number of false positives) as this is the more expensive error. If we now increase the threshold to values \\(&gt; 0.5\\), we reduce the number of false negatives. Note that we increase the number of false positives simultaneously, or, in other words, we are trading false positives for false negatives. # fit models with probability prediction learner = lrn(&quot;classif.log_reg&quot;, predict_type = &quot;prob&quot;) rr = resample(task, learner, rsmp(&quot;cv&quot;)) p = rr$prediction() print(p) ## &lt;PredictionClassif&gt; for 1000 observations: ## row_id truth response prob.good prob.bad ## 14 bad good 0.6210 0.379048 ## 20 good good 0.9046 0.095355 ## 25 good good 0.9939 0.006067 ## --- ## 980 bad bad 0.3350 0.665042 ## 983 good good 0.6665 0.333486 ## 999 bad bad 0.3552 0.644800 # helper function to try different threshold values interactively with_threshold = function(p, th) { p$set_threshold(th) list(confusion = p$confusion, costs = p$score(measures = cost_measure, task = task)) } with_threshold(p, 0.5) ## $confusion ## truth ## response good bad ## good 604 159 ## bad 96 141 ## ## $costs ## classif.costs ## -0.0524 with_threshold(p, 0.75) ## $confusion ## truth ## response good bad ## good 468 69 ## bad 232 231 ## ## $costs ## classif.costs ## -0.0948 with_threshold(p, 1.0) ## $confusion ## truth ## response good bad ## good 1 1 ## bad 699 299 ## ## $costs ## classif.costs ## 0.00065 # TODO: include plot of threshold vs performance Instead of manually trying different threshold values, one uses use optimize() to find a good threshold value w.r.t. our performance measure: # simple wrapper function which takes a threshold and returns the resulting model performance # this wrapper is passed to optimize() to find its minimum for thresholds in [0.5, 1] f = function(th) { with_threshold(p, th)$costs } best = optimize(f, c(0.5, 1)) print(best) ## $minimum ## [1] 0.7175 ## ## $objective ## classif.costs ## -0.09795 # optimized confusion matrix: with_threshold(p, best$minimum)$confusion ## truth ## response good bad ## good 497 76 ## bad 203 224 Note that the function optimize() is intended for unimodal functions and therefore may converge to a local optimum here. See below for better alternatives to find good threshold values. 7.6.4 Threshold Tuning More following soon! Upcoming sections will entail: threshold tuning as pipeline operator joint hyperparameter optimization More following soon! "],
["interpretation.html", "8 Model Interpretation ", " 8 Model Interpretation "],
["iml.html", "8.1 IML", " 8.1 IML "],
["dalex.html", "8.2 Dalex", " 8.2 Dalex "],
["appendix.html", "9 Appendix ", " 9 Appendix "],
["list-learners.html", "9.1 Integrated Learners", " 9.1 Integrated Learners Id Feature Types Required packages Properties Predict Types classif.cv_glmnet lgl, int, dbl glmnet Multiclass, Twoclass, Weights response, prob classif.debug lgl, int, dbl, chr, fct, ord Missings, Multiclass, Twoclass response, prob classif.featureless lgl, int, dbl, chr, fct, ord Importance, Missings, Multiclass, Selected Features, Twoclass response, prob classif.glmnet lgl, int, dbl glmnet Multiclass, Twoclass, Weights response, prob classif.kknn lgl, int, dbl, fct, ord kknn Multiclass, Twoclass response, prob classif.lda lgl, int, dbl, fct, ord MASS Multiclass, Twoclass, Weights response, prob classif.log_reg lgl, int, dbl, chr, fct, ord stats Twoclass, Weights response, prob classif.multinom lgl, int, dbl, fct nnet Multiclass, Twoclass, Weights response, prob classif.naive_bayes lgl, int, dbl, fct e1071 Multiclass, Twoclass response, prob classif.qda lgl, int, dbl, fct, ord MASS Multiclass, Twoclass, Weights response, prob classif.ranger lgl, int, dbl, chr, fct, ord ranger Importance, Multiclass, Oob Error, Twoclass, Weights response, prob classif.rpart lgl, int, dbl, fct, ord rpart Importance, Missings, Multiclass, Selected Features, Twoclass, Weights response, prob classif.svm lgl, int, dbl e1071 Multiclass, Twoclass response, prob classif.xgboost lgl, int, dbl xgboost Importance, Missings, Multiclass, Twoclass, Weights response, prob dens.hist lgl, int, dbl, chr, fct, ord distr6 pdf, cdf dens.kde lgl, int, dbl, chr, fct, ord distr6 Missings pdf dens.kdeKD lgl, int, dbl, chr, fct, ord distr6, kerdiest pdf dens.kdeKS lgl, int, dbl, chr, fct, ord distr6, ks pdf dens.locfit lgl, int, dbl, chr, fct, ord distr6, locfit pdf dens.logspline lgl, int, dbl, chr, fct, ord distr6, logspline pdf, cdf dens.mixed lgl, int, dbl, chr, fct, ord distr6, np pdf dens.nonpar lgl, int, dbl, chr, fct, ord distr6, sm Weights pdf dens.pen lgl, int, dbl, chr, fct, ord distr6, pendensity pdf, cdf dens.plug lgl, int, dbl, chr, fct, ord distr6, plugdensity Missings pdf dens.spline lgl, int, dbl, chr, fct, ord distr6, gss Missings pdf, cdf regr.cv_glmnet lgl, int, dbl glmnet Weights response regr.featureless lgl, int, dbl, chr, fct, ord stats Importance, Missings, Selected Features response, se regr.glmnet lgl, int, dbl glmnet Weights response regr.kknn lgl, int, dbl, fct, ord kknn response regr.km lgl, int, dbl DiceKriging response, se regr.lm lgl, int, dbl, fct stats Weights response, se regr.ranger lgl, int, dbl, chr, fct, ord ranger Importance, Oob Error, Weights response, se regr.rpart lgl, int, dbl, fct, ord rpart Importance, Missings, Selected Features, Weights response regr.svm lgl, int, dbl e1071 response regr.xgboost lgl, int, dbl xgboost Importance, Missings, Weights response surv.blackboost int, dbl, fct distr6, mboost, mvtnorm, partykit, survival distr, crank, lp surv.coxph lgl, int, dbl, fct distr6, survival Weights distr, crank, lp surv.cvglmnet int, dbl, fct glmnet, survival Weights crank, lp surv.flexible lgl, int, fct, dbl distr6, flexsurv, set6, survival Weights distr, lp, crank surv.gamboost int, dbl, fct, lgl distr6, mboost, survival distr, crank, lp surv.gbm int, dbl, fct, ord gbm Importance, Missings, Weights crank, lp surv.glmboost int, dbl, fct, lgl distr6, mboost, survival distr, crank, lp surv.glmnet int, dbl, fct glmnet, survival Weights crank, lp surv.kaplan lgl, int, dbl, chr, fct, ord distr6, survival Missings crank, distr surv.mboost int, dbl, fct, lgl distr6, mboost, survival distr, crank, lp surv.obliqueRSF int, dbl, fct, ord distr6, obliqueRSF Missings crank, distr surv.penalized int, dbl, fct, ord distr6, penalized distr, crank surv.randomForestSRC lgl, int, dbl, fct, ord distr6, randomForestSRC Importance, Missings, Weights crank, distr surv.ranger lgl, int, dbl, chr, fct, ord distr6, ranger Importance, Oob Error, Weights distr, crank surv.rpart lgl, int, dbl, chr, fct, ord distr6, rpart, survival Importance, Missings, Selected Features, Weights crank, distr surv.xgboost lgl, int, dbl xgboost Importance, Missings, Weights crank, lp "],
["list-measures.html", "9.2 Integrated Performance Measures", " 9.2 Integrated Performance Measures Also see the overview on the website of mlr3measures. Id Task Type Required packages Task Properties Predict Type classif.acc classif mlr3measures response classif.auc classif mlr3measures twoclass prob classif.bacc classif mlr3measures response classif.bbrier classif mlr3measures twoclass prob classif.ce classif mlr3measures response classif.costs classif response classif.dor classif mlr3measures twoclass response classif.fbeta classif mlr3measures twoclass response classif.fdr classif mlr3measures twoclass response classif.fn classif mlr3measures twoclass response classif.fnr classif mlr3measures twoclass response classif.fomr classif mlr3measures twoclass response classif.fp classif mlr3measures twoclass response classif.fpr classif mlr3measures twoclass response classif.logloss classif mlr3measures prob classif.mbrier classif mlr3measures prob classif.mcc classif mlr3measures twoclass response classif.npv classif mlr3measures twoclass response classif.ppv classif mlr3measures twoclass response classif.precision classif mlr3measures twoclass response classif.recall classif mlr3measures twoclass response classif.sensitivity classif mlr3measures twoclass response classif.specificity classif mlr3measures twoclass response classif.tn classif mlr3measures twoclass response classif.tnr classif mlr3measures twoclass response classif.tp classif mlr3measures twoclass response classif.tpr classif mlr3measures twoclass response debug NA response dens.logloss dens pdf oob_error NA response regr.bias regr mlr3measures response regr.ktau regr mlr3measures response regr.mae regr mlr3measures response regr.mape regr mlr3measures response regr.maxae regr mlr3measures response regr.medae regr mlr3measures response regr.medse regr mlr3measures response regr.mse regr mlr3measures response regr.msle regr mlr3measures response regr.pbias regr mlr3measures response regr.rae regr mlr3measures response regr.rmse regr mlr3measures response regr.rmsle regr mlr3measures response regr.rrse regr mlr3measures response regr.rse regr mlr3measures response regr.rsq regr mlr3measures response regr.sae regr mlr3measures response regr.smape regr mlr3measures response regr.srho regr mlr3measures response regr.sse regr mlr3measures response selected_features NA response surv.beggC surv survAUC lp surv.chamblessAUC surv survAUC lp surv.gonenC surv survAUC lp surv.graf surv distr6 distr surv.grafSE surv distr6 distr surv.harrellC surv crank surv.hungAUC surv survAUC lp surv.intlogloss surv distr6 distr surv.intloglossSE surv distr6 distr surv.logloss surv distr6 distr surv.loglossSE surv distr6 distr surv.mae surv response surv.maeSE surv response surv.mse surv response surv.mseSE surv response surv.nagelkR2 surv survAUC lp surv.oquigleyR2 surv survAUC lp surv.rmse surv response surv.rmseSE surv response surv.songAUC surv survAUC lp surv.songTNR surv survAUC lp surv.songTPR surv survAUC lp surv.unoAUC surv survAUC lp surv.unoC surv survAUC crank surv.unoTNR surv survAUC lp surv.unoTPR surv survAUC lp surv.xuR2 surv survAUC lp time_both NA response time_predict NA response time_train NA response "],
["list-filters.html", "9.3 Integrated Filter Methods", " 9.3 Integrated Filter Methods 9.3.1 Standalone filter methods Name Task task_properties param_set Features Package carscore Regr character(0) &lt;environment&gt; numeric &lt;span style=&quot; font-style: italic; &quot; &gt;care&lt;/span&gt; correlation Regr character(0) &lt;environment&gt; Integer, Numeric &lt;span style=&quot; font-style: italic; &quot; &gt;stats&lt;/span&gt; cmim Classif &amp; Regr character(0) &lt;environment&gt; Integer, Numeric, Factor, Ordered &lt;span style=&quot; font-style: italic; &quot; &gt;praznik&lt;/span&gt; find_correlation Classif &amp; Regr character(0) &lt;environment&gt; Integer, Numeric &lt;span style=&quot; font-style: italic; &quot; &gt;stats&lt;/span&gt; information_gain Classif &amp; Regr character(0) &lt;environment&gt; Integer, Numeric, Factor, Ordered &lt;span style=&quot; font-style: italic; &quot; &gt;FSelectorRcpp&lt;/span&gt; variance Classif &amp; Regr character(0) &lt;environment&gt; Integer, Numeric &lt;span style=&quot; font-style: italic; &quot; &gt;stats&lt;/span&gt; anova Classif character(0) &lt;environment&gt; Integer, Numeric &lt;span style=&quot; font-style: italic; &quot; &gt;stats&lt;/span&gt; auc Classif twoclass &lt;environment&gt; Integer, Numeric &lt;span style=&quot; font-style: italic; &quot; &gt;mlr3measures&lt;/span&gt; disr Classif character(0) &lt;environment&gt; Integer, Numeric, Factor, Ordered &lt;span style=&quot; font-style: italic; &quot; &gt;praznik&lt;/span&gt; importance Classif character(0) &lt;environment&gt; c(“logical”, “integer”, “numeric”, “factor”, “ordered”) &lt;span style=&quot; font-style: italic; &quot; &gt;rpart&lt;/span&gt; jmi Classif character(0) &lt;environment&gt; Integer, Numeric, Factor, Ordered &lt;span style=&quot; font-style: italic; &quot; &gt;praznik&lt;/span&gt; jmim Classif character(0) &lt;environment&gt; Integer, Numeric, Factor, Ordered &lt;span style=&quot; font-style: italic; &quot; &gt;praznik&lt;/span&gt; kruskal_test Classif character(0) &lt;environment&gt; Integer, Numeric &lt;span style=&quot; font-style: italic; &quot; &gt;stats&lt;/span&gt; mim Classif character(0) &lt;environment&gt; Integer, Numeric, Factor, Ordered &lt;span style=&quot; font-style: italic; &quot; &gt;praznik&lt;/span&gt; mrmr Classif character(0) &lt;environment&gt; Integer, Numeric, Factor, Ordered &lt;span style=&quot; font-style: italic; &quot; &gt;praznik&lt;/span&gt; njmim Classif character(0) &lt;environment&gt; Integer, Numeric, Factor, Ordered &lt;span style=&quot; font-style: italic; &quot; &gt;praznik&lt;/span&gt; performance Classif character(0) &lt;environment&gt; c(“logical”, “integer”, “numeric”, “factor”, “ordered”) &lt;span style=&quot; font-style: italic; &quot; &gt;character(0)&lt;/span&gt; 9.3.2 Algorithms With Embedded Filter Methods ## [1] &quot;classif.featureless&quot; &quot;classif.ranger&quot; &quot;classif.rpart&quot; ## [4] &quot;classif.xgboost&quot; &quot;regr.featureless&quot; &quot;regr.ranger&quot; ## [7] &quot;regr.rpart&quot; &quot;regr.xgboost&quot; &quot;surv.gbm&quot; ## [10] &quot;surv.randomForestSRC&quot; &quot;surv.ranger&quot; &quot;surv.rpart&quot; ## [13] &quot;surv.xgboost&quot; "],
["list-pipeops.html", "9.4 Integrated PipeOp’s", " 9.4 Integrated PipeOp’s key packages input.num output.num input.train input.predict output.train output.predict boxcox bestNormalize 1 1 Task Task Task Task branch 1 – Any Any Any Any chunk 1 – Task Task Task Task classbalancing 1 1 TaskClassif TaskClassif TaskClassif TaskClassif classifavg stats – 1 – PredictionClassif – PredictionClassif classweights 1 1 TaskClassif TaskClassif TaskClassif TaskClassif colapply 1 1 Task Task Task Task collapsefactors 1 1 Task Task Task Task copy 1 – Any Any Any Any crankcompose distr6 1 1 – PredictionSurv – PredictionSurv distrcompose distr6 2 1 – PredictionSurv – PredictionSurv encode stats 1 1 Task Task Task Task encodeimpact 1 1 Task Task Task Task encodelmer lme4 | nloptr 1 1 Task Task Task Task featureunion – 1 Task Task Task Task filter 1 1 Task Task Task Task fixfactors 1 1 Task Task Task Task histbin graphics 1 1 Task Task Task Task ica fastICA 1 1 Task Task Task Task imputehist graphics 1 1 Task Task Task Task imputemean 1 1 Task Task Task Task imputemedian stats 1 1 Task Task Task Task imputenewlvl 1 1 Task Task Task Task imputesample 1 1 Task Task Task Task kernelpca kernlab 1 1 Task Task Task Task learner 1 1 TaskClassif TaskClassif – PredictionClassif learner_cv 1 1 TaskClassif TaskClassif TaskClassif TaskClassif missind 1 1 Task Task Task Task modelmatrix stats 1 1 Task Task Task Task mutate 1 1 Task Task Task Task nop 1 1 Any Any Any Any pca 1 1 Task Task Task Task quantilebin stats 1 1 Task Task Task Task regravg – 1 – PredictionRegr – PredictionRegr removeconstants 1 1 Task Task Task Task scale 1 1 Task Task Task Task scalemaxabs 1 1 Task Task Task Task scalerange 1 1 Task Task Task Task select 1 1 Task Task Task Task smote smotefamily 1 1 Task Task Task Task spatialsign 1 1 Task Task Task Task subsample 1 1 Task Task Task Task unbranch – 1 Any Any Any Any yeojohnson bestNormalize 1 1 Task Task Task Task "],
["compare-frameworks.html", "9.5 Framework Comparison", " 9.5 Framework Comparison Below, we collected some examples, where mlr3pipelines is compared to different other software packages, such as mlr, recipes and sklearn. Before diving deeper, we give a short introduction to PipeOps. 9.5.1 An introduction to “PipeOp”s In this example, we create a linear Pipeline. After scaling all input features, we rotate our data using principal component analysis. After this transformation, we use a simple Decision Tree learner for classification. As exemplary data, we will use the “iris” classification task. This object contains the famous iris dataset and some meta-information, such as the target variable. library(&quot;mlr3&quot;) task = mlr_tasks$get(&quot;iris&quot;) We quickly split our data into a train and a test set: test.idx = sample(seq_len(task$nrow), 30) train.idx = setdiff(seq_len(task$nrow), test.idx) # Set task to only use train indexes task$row_roles$use = train.idx A Pipeline (or Graph) contains multiple pipeline operators (“PipeOp”s), where each PipeOp transforms the data when it flows through it. For this use case, we require 3 transformations: A PipeOp that scales the data A PipeOp that performs PCA A PipeOp that contains the Decision Tree learner A list of available PipeOps can be obtained from library(&quot;mlr3pipelines&quot;) mlr_pipeops$keys() ## [1] &quot;boxcox&quot; &quot;branch&quot; &quot;chunk&quot; &quot;classbalancing&quot; ## [5] &quot;classifavg&quot; &quot;classweights&quot; &quot;colapply&quot; &quot;collapsefactors&quot; ## [9] &quot;copy&quot; &quot;crankcompose&quot; &quot;distrcompose&quot; &quot;encode&quot; ## [13] &quot;encodeimpact&quot; &quot;encodelmer&quot; &quot;featureunion&quot; &quot;filter&quot; ## [17] &quot;fixfactors&quot; &quot;histbin&quot; &quot;ica&quot; &quot;imputehist&quot; ## [21] &quot;imputemean&quot; &quot;imputemedian&quot; &quot;imputenewlvl&quot; &quot;imputesample&quot; ## [25] &quot;kernelpca&quot; &quot;learner&quot; &quot;learner_cv&quot; &quot;missind&quot; ## [29] &quot;modelmatrix&quot; &quot;mutate&quot; &quot;nop&quot; &quot;pca&quot; ## [33] &quot;quantilebin&quot; &quot;regravg&quot; &quot;removeconstants&quot; &quot;scale&quot; ## [37] &quot;scalemaxabs&quot; &quot;scalerange&quot; &quot;select&quot; &quot;smote&quot; ## [41] &quot;spatialsign&quot; &quot;subsample&quot; &quot;unbranch&quot; &quot;yeojohnson&quot; First we define the required PipeOps: op1 = PipeOpScale$new() op2 = PipeOpPCA$new() op3 = PipeOpLearner$new(learner = mlr_learners$get(&quot;classif.rpart&quot;)) 9.5.1.1 A quick glance into a PipeOp In order to get a better understanding of what the respective PipeOps do, we quickly look at one of them in detail: The most important slots in a PipeOp are: $train(): A function used to train the PipeOp. $predict(): A function used to predict with the PipeOp. The $train() and $predict() functions define the core functionality of our PipeOp. In many cases, in order to not leak information from the training set into the test set it is imperative to treat train and test data separately. For this we require a $train() function that learns the appropriate transformations from the training set and a $predict() function that applies the transformation on future data. In the case of PipeOpPCA this means the following: $train() learns a rotation matrix from its input and saves this matrix to an additional slot, $state. It returns the rotated input data stored in a new Task. $predict() uses the rotation matrix stored in $state in order to rotate future, unseen data. It returns those in a new Task. 9.5.1.2 Constructing the Pipeline We can now connect the PipeOps constructed earlier to a Pipeline. We can do this using the %&gt;&gt;% operator. linear_pipeline = op1 %&gt;&gt;% op2 %&gt;&gt;% op3 The result of this operation is a “Graph”. A Graph connects the input and output of each PipeOp to the following PipeOp. This allows us to specify linear processing pipelines. In this case, we connect the output of the scaling PipeOp to the input of the PCA PipeOp and the output of the PCA PipeOp to the input of PipeOpLearner. We can now train the Graph using the iris Task. linear_pipeline$train(task) ## $classif.rpart.output ## NULL When we now train the graph, the data flows through the graph as follows: The Task flows into the PipeOpScale. The PipeOp scales each column in the data contained in the Task and returns a new Task that contains the scaled data to its output. The scaled Task flows into the PipeOpPCA. PCA transforms the data and returns a (possibly smaller) Task, that contains the transformed data. This transformed data then flows into the learner, in our case classif.rpart. It is then used to train the learner, and as a result saves a model that can be used to predict new data. In order to predict on new data, we need to save the relevant transformations our data went through while training. As a result, each PipeOp saves a state, where information required to appropriately transform future data is stored. In our case, this is mean and standard deviation of each column for PipeOpScale, the PCA rotation matrix for PipeOpPCA and the learned model for PipeOpLearner. # predict on test.idx task$row_roles$use = test.idx linear_pipeline$predict(task) ## $classif.rpart.output ## &lt;PredictionClassif&gt; for 30 observations: ## row_id truth response ## 100 versicolor versicolor ## 135 virginica versicolor ## 12 setosa setosa ## --- ## 30 setosa setosa ## 149 virginica virginica ## 120 virginica versicolor 9.5.2 mlr3pipelines vs. mlr In order to showcase the benefits of mlr3pipelines over mlr’s Wrapper mechanism, we compare the case of imputing missing values before filtering the top 2 features and then applying a learner. While mlr wrappers are generally less verbose and require a little less code, this heavily inhibits flexibility. As an example, wrappers can generally not process data in parallel. 9.5.2.1 mlr library(&quot;mlr&quot;) # We first create a learner lrn = makeLearner(&quot;classif.rpart&quot;) # Wrap this learner in a FilterWrapper lrn.wrp = makeFilterWrapper(lrn, fw.abs = 2L) # And wrap the resulting wrapped learner into an ImputeWrapper. lrn.wrp = makeImputeWrapper(lrn.wrp) # Afterwards, we can train the resulting learner on a task train(lrn, iris.task) 9.5.2.2 mlr3pipelines library(&quot;mlr3&quot;) library(&quot;mlr3pipelines&quot;) library(&quot;mlr3filters&quot;) impute = PipeOpImpute$new() filter = PipeOpFilter$new(filter = FilterVariance$new(), param_vals = list(filter.nfeat = 2L)) rpart = PipeOpLearner$new(mlr_learners$get(&quot;classif.rpart&quot;)) # Assemble the Pipeline pipeline = impute %&gt;&gt;% filter %&gt;&gt;% rpart # And convert to a &#39;GraphLearner&#39; learner = GraphLearner$new(id = &quot;Pipeline&quot;, pipeline) The fact that mlr’s wrappers have to be applied inside-out, i.e. in the reverse order is often confusing. This is way more straight-forward in mlr3pipelines, where we simply chain the different methods using %&gt;&gt;%. Additionally, mlr3pipelines offers way greater possibilities with respect to the kinds of Pipelines that can be constructed. In mlr3pipelines, we allow for the construction of parallel and conditional pipelines. This was previously not possible. 9.5.3 mlr3pipelines vs. sklearn.pipeline.Pipeline In order to broaden the horizon, we compare to Python sklearn’s Pipeline methods. sklearn.pipeline.Pipeline sequentially applies a list of transforms before fitting a final estimator. Intermediate steps of the pipeline are transforms, i.e. steps that can learn from the data, but also transform the data while it flows through it. The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters. For this, it enables setting parameters of the various steps. It is thus conceptually very similar to mlr3pipelines. Similarly to mlr3pipelines, we can tune over a full Pipeline using various tuning methods. Pipeline mainly supports linear pipelines. This means, that it can execute parallel steps, such as for example Bagging, but it does not support conditional execution, i.e. PipeOpBranch. At the same time, the different transforms in the pipeline can be cached, which makes tuning over the configuration space of a Pipeline more efficient, as executing some steps multiple times can be avoided. We compare functionality available in both mlr3pipelines and sklearn.pipeline.Pipeline to give a comparison. The following example obtained from the sklearn documentation showcases a Pipeline that first Selects a feature and performs PCA on the original data, concatenates the resulting datasets and applies a Support Vector Machine. 9.5.3.1 sklearn from sklearn.pipeline import Pipeline, FeatureUnion from sklearn.model_selection import GridSearchCV from sklearn.svm import SVC from sklearn.datasets import load_iris from sklearn.decomposition import PCA from sklearn.feature_selection import SelectKBest iris = load_iris() X, y = iris.data, iris.target # This dataset is way too high-dimensional. Better do PCA: pca = PCA(n_components=2) # Maybe some original features where good, too? selection = SelectKBest(k=1) # Build estimator from PCA and Univariate selection: combined_features = FeatureUnion([(&quot;pca&quot;, pca), (&quot;univ_select&quot;, selection)]) # Use combined features to transform dataset: X_features = combined_features.fit(X, y).transform(X) svm = SVC(kernel=&quot;linear&quot;) # Do grid search over k, n_components and C: pipeline = Pipeline([(&quot;features&quot;, combined_features), (&quot;svm&quot;, svm)]) param_grid = dict(features__pca__n_components=[1, 2, 3], features__univ_select__k=[1, 2], svm__C=[0.1, 1, 10]) grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=5, verbose=10) grid_search.fit(X, y) 9.5.3.2 mlr3pipelines library(&quot;mlr3verse&quot;) iris = mlr_tasks$get(&quot;iris&quot;) # Build the steps copy = PipeOpCopy$new(2) pca = PipeOpPCA$new() selection = PipeOpFilter$new(filter = FilterVariance$new()) union = PipeOpFeatureUnion$new(2) svm = PipeOpLearner$new(mlr_learners$get(&quot;classif.svm&quot;, param_vals = list(kernel = &quot;linear&quot;))) # Assemble the Pipeline pipeline = copy %&gt;&gt;% gunion(list(pca, selection)) %&gt;&gt;% union %&gt;&gt;% svm learner = GraphLearner$new(id = &quot;Pipeline&quot;, pipeline) # For tuning, we define the resampling and the Parameter Space resampling = mlr3::mlr_resamplings$get(&quot;cv&quot;, param_vals = list(folds = 5L)) param_set = paradox::ParamSet$new(params = list( paradox::ParamDbl$new(&quot;classif.svm.cost&quot;, lower = 0.1, upper = 1), paradox::ParamInt$new(&quot;pca.rank.&quot;, lower = 1, upper = 3), paradox::ParamInt$new(&quot;variance.filter.nfeat&quot;, lower = 1, upper = 2) )) pe = PerformanceEvaluator$new(iris, learner, resampling, param_set) terminator = TerminatorEvaluations$new(60) tuner = TunerGridSearch$new(pe, terminator, resolution = 10)$tune() # Set the learner to the optimal values and train learner$param_set$values = tuner$tune_result()$values In summary, we can achieve similar results with a comparable number of lines, while at the same time offering greater flexibility with respect to which kinds of pipelines we want to optimize over. At the same time, experiments using mlr3 can now be arbitrarily parallelized using futures. 9.5.4 mlr3pipelines vs recipes recipes is a new package, that covers some of the same applications steps as mlr3pipelines. Both packages feature the possibility to connect different pre- and post-processing methods using a pipe-operator. As the recipes package tightly integrates with the tidymodels ecosystem, much of the functionality integrated there can be used in recipes. We compare recipes to mlr3pipelines using an example from the recipes vignette. The aim of the analysis is to predict whether customers pay back their loans given some information on the customers. In order to do this, we build a model that does the following: It first imputes missing values using k-nearest neighbors All factor variables are converted to numerics using dummy encoding The data is first centered then scaled. In order to validate the algorithm, data is first split into a train and test set using initial_split, training, testing. The recipe trained on the train data (see steps above) is then applied to the test data. 9.5.4.1 recipes library(&quot;tidymodels&quot;) library(&quot;rsample&quot;) data(&quot;credit_data&quot;, package = &quot;modeldata&quot;) set.seed(55) train_test_split = initial_split(credit_data) credit_train = training(train_test_split) credit_test = testing(train_test_split) rec = recipe(Status ~ ., data = credit_train) %&gt;% step_knnimpute(all_predictors()) %&gt;% step_dummy(all_predictors(), -all_numeric()) %&gt;% step_center(all_numeric()) %&gt;% step_scale(all_numeric()) trained_rec = prep(rec, training = credit_train) # Apply to train and test set train_data &lt;- bake(trained_rec, new_data = credit_train) test_data &lt;- bake(trained_rec, new_data = credit_test) Afterwards, the transformed data can be used during train and predict: # Train rf = rand_forest(mtry = 12, trees = 200, mode = &quot;classification&quot;) %&gt;% set_engine(&quot;ranger&quot;, importance = &#39;impurity&#39;) %&gt;% fit(Status ~ ., data = train_data) # Predict prds = predict(rf, test_data) 9.5.4.2 mlr3pipelines The same analysis can be performed in mlr3pipelines. Note, that for now we do not impute via knn but instead via sampling. library(&quot;data.table&quot;) library(&quot;mlr3&quot;) library(&quot;mlr3learners&quot;) library(&quot;mlr3pipelines&quot;) data(&quot;credit_data&quot;, package = &quot;modeldata&quot;) set.seed(55) # Create the task tsk = TaskClassif$new(id = &quot;credit_task&quot;, target = &quot;Status&quot;, backend = as_data_backend(data.table(credit_data))) # Build up the Pipeline: g = PipeOpImputeSample$new(id = &quot;impute&quot;) %&gt;&gt;% PipeOpEncode$new(param_vals = list(method = &quot;one-hot&quot;)) %&gt;&gt;% PipeOpScale$new() %&gt;&gt;% PipeOpLearner$new(mlr_learners$get(&quot;classif.ranger&quot;, param_vals = list(num.trees = 200, mtry = 12)) # We can visualize what happens to the data using the `plot` function: g$plot() # And we can use `mlr3&#39;s` full functionality be wrapping the Graph into a GraphLearner. glrn = GraphLearner$new(g) resample(tsk, glrn, mlr_resamplings$get(&quot;holdout&quot;)) "],
["references.html", "References", " References "]
]
